{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4221e1e3",
   "metadata": {},
   "source": [
    "# Training a basic setting with a Deep Q Network (DQN) #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d628c23d",
   "metadata": {},
   "source": [
    "Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b9fe8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "rc('text', usetex=True)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "761fd1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f09c161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory  # For experience replay!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c9f747a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.1 (SDL 2.0.14, Python 3.7.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from gym_environment_ncml import *\n",
    "from learning import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a0674c",
   "metadata": {},
   "source": [
    "Useful numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cf11480",
   "metadata": {},
   "outputs": [],
   "source": [
    "MILLION = 1000000\n",
    "HTHOUSAND = 100000\n",
    "THOUSAND = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfb0b18",
   "metadata": {},
   "source": [
    "## 1. Create environment ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b9a3f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/copernico/opt/anaconda3/envs/RL/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = GridworldMultiAgentv15()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "708f7943",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eca1527b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 25)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states, actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0343c79a",
   "metadata": {},
   "source": [
    "## 2. Create a Deep Learning Model with Keras ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "941ff796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/copernico/opt/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "model = build_model(states, actions, [32, 16], ['relu', 'relu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f136bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 25)                425       \n",
      "=================================================================\n",
      "Total params: 1,241\n",
      "Trainable params: 1,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0b28d2",
   "metadata": {},
   "source": [
    "## 3. Build Agent with Keras-RL ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a418e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = build_agent(model, actions, 0.01, EpsGreedyQPolicy(), 50000)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "# dqn.compile(Adam(lr=1e-2), metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92c1507e",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'dqn15_5b5_3236_adam_lr0.001_tmu0.01_ml50K_ns7M_eps0.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "073ef900",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 7000000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: -0.0080\n",
      "200 episodes - episode_reward: -0.400 [-50.000, 120.000] - loss: 5.089 - mae: 9.467 - mean_q: 11.273\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 0.1990\n",
      "200 episodes - episode_reward: 9.950 [-50.000, 150.000] - loss: 14.845 - mae: 27.670 - mean_q: 30.798\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 0.2740\n",
      "200 episodes - episode_reward: 13.700 [-50.000, 140.000] - loss: 21.550 - mae: 37.070 - mean_q: 40.664\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 0.3040\n",
      "200 episodes - episode_reward: 15.200 [-50.000, 160.000] - loss: 25.897 - mae: 40.404 - mean_q: 44.201\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 0.6470\n",
      "200 episodes - episode_reward: 32.350 [-50.000, 180.000] - loss: 28.149 - mae: 42.560 - mean_q: 46.517\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 0.7220\n",
      "200 episodes - episode_reward: 36.100 [-50.000, 170.000] - loss: 31.780 - mae: 46.317 - mean_q: 50.542\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.0130\n",
      "200 episodes - episode_reward: 50.650 [-50.000, 200.000] - loss: 37.967 - mae: 50.476 - mean_q: 55.064\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 1.3480\n",
      "200 episodes - episode_reward: 67.400 [-50.000, 210.000] - loss: 49.971 - mae: 60.213 - mean_q: 65.624\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.3940\n",
      "200 episodes - episode_reward: 69.700 [-50.000, 190.000] - loss: 65.755 - mae: 70.477 - mean_q: 76.716\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.6060\n",
      "200 episodes - episode_reward: 80.300 [-50.000, 210.000] - loss: 83.874 - mae: 81.625 - mean_q: 88.591\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.1600\n",
      "200 episodes - episode_reward: 108.000 [-50.000, 230.000] - loss: 103.227 - mae: 91.371 - mean_q: 99.137\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.2990\n",
      "200 episodes - episode_reward: 114.950 [-40.000, 210.000] - loss: 130.036 - mae: 102.702 - mean_q: 111.422\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.3380\n",
      "200 episodes - episode_reward: 116.900 [-40.000, 250.000] - loss: 159.043 - mae: 114.845 - mean_q: 124.592\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.4020\n",
      "200 episodes - episode_reward: 120.100 [-50.000, 260.000] - loss: 189.421 - mae: 125.945 - mean_q: 136.772\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.7320\n",
      "200 episodes - episode_reward: 136.600 [-40.000, 260.000] - loss: 221.077 - mae: 138.418 - mean_q: 150.199\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.7020\n",
      "200 episodes - episode_reward: 135.100 [-50.000, 280.000] - loss: 244.778 - mae: 144.283 - mean_q: 156.709\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 2.8040\n",
      "200 episodes - episode_reward: 140.200 [-10.000, 240.000] - loss: 259.915 - mae: 148.740 - mean_q: 161.506\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.9510\n",
      "200 episodes - episode_reward: 147.550 [-10.000, 260.000] - loss: 272.907 - mae: 152.377 - mean_q: 165.476\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.0100\n",
      "200 episodes - episode_reward: 150.500 [-30.000, 280.000] - loss: 277.551 - mae: 155.861 - mean_q: 169.283\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.9820\n",
      "200 episodes - episode_reward: 149.100 [-50.000, 260.000] - loss: 294.934 - mae: 159.382 - mean_q: 173.036\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.9380\n",
      "200 episodes - episode_reward: 146.900 [-10.000, 300.000] - loss: 301.998 - mae: 160.553 - mean_q: 174.225\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.0620\n",
      "200 episodes - episode_reward: 153.100 [-30.000, 280.000] - loss: 300.568 - mae: 159.929 - mean_q: 173.606\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.9600\n",
      "200 episodes - episode_reward: 148.000 [-20.000, 280.000] - loss: 299.284 - mae: 159.628 - mean_q: 173.207\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.6440\n",
      "200 episodes - episode_reward: 132.200 [-50.000, 260.000] - loss: 292.259 - mae: 159.396 - mean_q: 172.560\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.8890\n",
      "200 episodes - episode_reward: 144.450 [-40.000, 300.000] - loss: 292.570 - mae: 157.116 - mean_q: 170.328\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.8650\n",
      "200 episodes - episode_reward: 143.250 [-40.000, 260.000] - loss: 277.061 - mae: 153.274 - mean_q: 166.272\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 3.1350\n",
      "200 episodes - episode_reward: 156.750 [-10.000, 300.000] - loss: 277.798 - mae: 154.721 - mean_q: 167.567\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 2.8360\n",
      "200 episodes - episode_reward: 141.800 [-40.000, 250.000] - loss: 287.808 - mae: 158.015 - mean_q: 171.217\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 2.7590\n",
      "200 episodes - episode_reward: 137.950 [-10.000, 260.000] - loss: 291.360 - mae: 157.944 - mean_q: 171.029\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 3.1490\n",
      "200 episodes - episode_reward: 157.450 [-30.000, 260.000] - loss: 293.118 - mae: 158.663 - mean_q: 171.915\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 2.8060\n",
      "200 episodes - episode_reward: 140.300 [-30.000, 240.000] - loss: 301.508 - mae: 160.471 - mean_q: 173.683\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.8190\n",
      "200 episodes - episode_reward: 140.950 [-50.000, 270.000] - loss: 301.817 - mae: 160.948 - mean_q: 174.482\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.1340\n",
      "200 episodes - episode_reward: 156.700 [20.000, 260.000] - loss: 302.944 - mae: 161.740 - mean_q: 175.397\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.9270\n",
      "200 episodes - episode_reward: 146.350 [-50.000, 260.000] - loss: 303.183 - mae: 158.956 - mean_q: 172.543\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 3.2340\n",
      "200 episodes - episode_reward: 161.700 [-50.000, 290.000] - loss: 299.508 - mae: 161.093 - mean_q: 175.026\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 3.1430\n",
      "200 episodes - episode_reward: 157.150 [-50.000, 250.000] - loss: 310.472 - mae: 161.802 - mean_q: 175.795\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.4200\n",
      "200 episodes - episode_reward: 171.000 [-50.000, 280.000] - loss: 323.464 - mae: 167.707 - mean_q: 182.201\n",
      "\n",
      "Interval 38 (370000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.1800\n",
      "200 episodes - episode_reward: 159.000 [-50.000, 260.000] - loss: 319.628 - mae: 165.070 - mean_q: 178.934\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.1180\n",
      "200 episodes - episode_reward: 155.900 [-30.000, 300.000] - loss: 319.300 - mae: 166.167 - mean_q: 180.127\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.3340\n",
      "200 episodes - episode_reward: 166.700 [0.000, 280.000] - loss: 320.034 - mae: 167.128 - mean_q: 181.249\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.1940\n",
      "200 episodes - episode_reward: 159.700 [-50.000, 270.000] - loss: 322.223 - mae: 166.117 - mean_q: 179.967\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.2830\n",
      "200 episodes - episode_reward: 164.150 [-50.000, 300.000] - loss: 320.980 - mae: 167.174 - mean_q: 181.229\n",
      "\n",
      "Interval 43 (420000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.2450\n",
      "200 episodes - episode_reward: 162.250 [-40.000, 280.000] - loss: 326.796 - mae: 168.622 - mean_q: 182.933\n",
      "\n",
      "Interval 44 (430000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.1670\n",
      "200 episodes - episode_reward: 158.350 [-50.000, 270.000] - loss: 335.964 - mae: 168.840 - mean_q: 183.463\n",
      "\n",
      "Interval 45 (440000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.5620\n",
      "200 episodes - episode_reward: 178.100 [-10.000, 290.000] - loss: 332.298 - mae: 169.983 - mean_q: 184.607\n",
      "\n",
      "Interval 46 (450000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.4600\n",
      "200 episodes - episode_reward: 173.000 [10.000, 290.000] - loss: 349.115 - mae: 172.624 - mean_q: 187.283\n",
      "\n",
      "Interval 47 (460000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 3.4110\n",
      "200 episodes - episode_reward: 170.550 [-30.000, 300.000] - loss: 332.158 - mae: 168.946 - mean_q: 183.009\n",
      "\n",
      "Interval 48 (470000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.0440\n",
      "200 episodes - episode_reward: 152.200 [-50.000, 320.000] - loss: 327.690 - mae: 168.366 - mean_q: 182.016\n",
      "\n",
      "Interval 49 (480000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.3310\n",
      "200 episodes - episode_reward: 166.550 [-40.000, 270.000] - loss: 315.616 - mae: 164.211 - mean_q: 177.428\n",
      "\n",
      "Interval 50 (490000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.3010\n",
      "200 episodes - episode_reward: 165.050 [-40.000, 300.000] - loss: 312.000 - mae: 163.911 - mean_q: 177.176\n",
      "\n",
      "Interval 51 (500000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.2300\n",
      "200 episodes - episode_reward: 161.500 [-50.000, 270.000] - loss: 328.982 - mae: 167.920 - mean_q: 181.882\n",
      "\n",
      "Interval 52 (510000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.4580\n",
      "200 episodes - episode_reward: 172.900 [-20.000, 270.000] - loss: 332.369 - mae: 171.323 - mean_q: 185.543\n",
      "\n",
      "Interval 53 (520000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.6360\n",
      "200 episodes - episode_reward: 181.800 [-50.000, 290.000] - loss: 338.230 - mae: 171.326 - mean_q: 185.723\n",
      "\n",
      "Interval 54 (530000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 3.5630\n",
      "200 episodes - episode_reward: 178.150 [-40.000, 310.000] - loss: 357.385 - mae: 175.859 - mean_q: 190.633\n",
      "\n",
      "Interval 55 (540000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.4620\n",
      "200 episodes - episode_reward: 173.100 [-40.000, 330.000] - loss: 359.980 - mae: 177.086 - mean_q: 191.752\n",
      "\n",
      "Interval 56 (550000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.5300\n",
      "200 episodes - episode_reward: 176.500 [-20.000, 310.000] - loss: 354.757 - mae: 176.051 - mean_q: 190.737\n",
      "\n",
      "Interval 57 (560000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.6280\n",
      "200 episodes - episode_reward: 181.400 [10.000, 300.000] - loss: 354.899 - mae: 174.199 - mean_q: 188.650\n",
      "\n",
      "Interval 58 (570000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.5640\n",
      "200 episodes - episode_reward: 178.200 [-30.000, 300.000] - loss: 357.467 - mae: 177.375 - mean_q: 192.217\n",
      "\n",
      "Interval 59 (580000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.4090\n",
      "200 episodes - episode_reward: 170.450 [-20.000, 300.000] - loss: 363.762 - mae: 176.884 - mean_q: 191.752\n",
      "\n",
      "Interval 60 (590000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 3.4170\n",
      "200 episodes - episode_reward: 170.850 [-50.000, 300.000] - loss: 367.148 - mae: 177.915 - mean_q: 193.027\n",
      "\n",
      "Interval 61 (600000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.4890\n",
      "200 episodes - episode_reward: 174.450 [-20.000, 290.000] - loss: 359.881 - mae: 177.145 - mean_q: 192.052\n",
      "\n",
      "Interval 62 (610000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.6500\n",
      "200 episodes - episode_reward: 182.500 [0.000, 300.000] - loss: 364.349 - mae: 176.850 - mean_q: 191.955\n",
      "\n",
      "Interval 63 (620000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.5440\n",
      "200 episodes - episode_reward: 177.200 [0.000, 300.000] - loss: 370.285 - mae: 178.075 - mean_q: 193.409\n",
      "\n",
      "Interval 64 (630000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.7810\n",
      "200 episodes - episode_reward: 189.050 [30.000, 300.000] - loss: 365.758 - mae: 177.769 - mean_q: 193.094\n",
      "\n",
      "Interval 65 (640000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 3.7630\n",
      "200 episodes - episode_reward: 188.150 [0.000, 300.000] - loss: 364.028 - mae: 175.838 - mean_q: 190.990\n",
      "\n",
      "Interval 66 (650000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.6260\n",
      "200 episodes - episode_reward: 181.300 [20.000, 290.000] - loss: 358.853 - mae: 176.984 - mean_q: 192.039\n",
      "\n",
      "Interval 67 (660000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 3.4960\n",
      "200 episodes - episode_reward: 174.800 [-50.000, 290.000] - loss: 365.278 - mae: 178.292 - mean_q: 193.373\n",
      "\n",
      "Interval 68 (670000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.5840\n",
      "200 episodes - episode_reward: 179.200 [-40.000, 290.000] - loss: 361.223 - mae: 175.778 - mean_q: 190.663\n",
      "\n",
      "Interval 69 (680000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.5030\n",
      "200 episodes - episode_reward: 175.150 [-40.000, 300.000] - loss: 362.612 - mae: 176.126 - mean_q: 191.040\n",
      "\n",
      "Interval 70 (690000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.6720\n",
      "200 episodes - episode_reward: 183.600 [-50.000, 300.000] - loss: 362.824 - mae: 175.320 - mean_q: 190.302\n",
      "\n",
      "Interval 71 (700000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.7850\n",
      "200 episodes - episode_reward: 189.250 [20.000, 310.000] - loss: 369.536 - mae: 177.753 - mean_q: 193.248\n",
      "\n",
      "Interval 72 (710000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.8500\n",
      "200 episodes - episode_reward: 192.500 [-40.000, 290.000] - loss: 366.592 - mae: 178.168 - mean_q: 193.479\n",
      "\n",
      "Interval 73 (720000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.6450\n",
      "200 episodes - episode_reward: 182.250 [-20.000, 300.000] - loss: 371.392 - mae: 177.496 - mean_q: 192.856\n",
      "\n",
      "Interval 74 (730000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.8120\n",
      "200 episodes - episode_reward: 190.600 [-50.000, 300.000] - loss: 383.131 - mae: 180.008 - mean_q: 195.736\n",
      "\n",
      "Interval 75 (740000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.4250\n",
      "200 episodes - episode_reward: 171.250 [-40.000, 300.000] - loss: 378.328 - mae: 181.475 - mean_q: 196.995\n",
      "\n",
      "Interval 76 (750000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 3.6410\n",
      "200 episodes - episode_reward: 182.050 [0.000, 300.000] - loss: 378.091 - mae: 183.778 - mean_q: 199.360\n",
      "\n",
      "Interval 77 (760000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.3580\n",
      "200 episodes - episode_reward: 167.900 [-50.000, 280.000] - loss: 371.074 - mae: 179.186 - mean_q: 194.114\n",
      "\n",
      "Interval 78 (770000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.5000\n",
      "200 episodes - episode_reward: 175.000 [-30.000, 290.000] - loss: 361.923 - mae: 177.361 - mean_q: 192.239\n",
      "\n",
      "Interval 79 (780000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.6620\n",
      "200 episodes - episode_reward: 183.100 [0.000, 280.000] - loss: 350.744 - mae: 172.969 - mean_q: 187.370\n",
      "\n",
      "Interval 80 (790000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.8350\n",
      "200 episodes - episode_reward: 191.750 [20.000, 320.000] - loss: 354.972 - mae: 177.146 - mean_q: 192.267\n",
      "\n",
      "Interval 81 (800000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.8250\n",
      "200 episodes - episode_reward: 191.250 [0.000, 320.000] - loss: 370.049 - mae: 179.130 - mean_q: 194.408\n",
      "\n",
      "Interval 82 (810000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 3.6800\n",
      "200 episodes - episode_reward: 184.000 [-30.000, 290.000] - loss: 384.663 - mae: 182.221 - mean_q: 197.866\n",
      "\n",
      "Interval 83 (820000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.5180\n",
      "200 episodes - episode_reward: 175.900 [-40.000, 310.000] - loss: 393.706 - mae: 183.126 - mean_q: 198.756\n",
      "\n",
      "Interval 84 (830000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.8390\n",
      "200 episodes - episode_reward: 191.950 [-30.000, 290.000] - loss: 373.359 - mae: 180.285 - mean_q: 195.719\n",
      "\n",
      "Interval 85 (840000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.9720\n",
      "200 episodes - episode_reward: 198.600 [0.000, 300.000] - loss: 378.407 - mae: 181.572 - mean_q: 197.077\n",
      "\n",
      "Interval 86 (850000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.6170\n",
      "200 episodes - episode_reward: 180.850 [-40.000, 290.000] - loss: 390.001 - mae: 183.626 - mean_q: 199.305\n",
      "\n",
      "Interval 87 (860000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 3.9640\n",
      "200 episodes - episode_reward: 198.200 [70.000, 330.000] - loss: 392.804 - mae: 184.698 - mean_q: 200.556\n",
      "\n",
      "Interval 88 (870000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.9150\n",
      "200 episodes - episode_reward: 195.750 [20.000, 300.000] - loss: 384.410 - mae: 183.111 - mean_q: 198.651\n",
      "\n",
      "Interval 89 (880000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.6480\n",
      "200 episodes - episode_reward: 182.400 [-50.000, 290.000] - loss: 379.003 - mae: 179.698 - mean_q: 194.753\n",
      "\n",
      "Interval 90 (890000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.7380\n",
      "200 episodes - episode_reward: 186.900 [-50.000, 270.000] - loss: 362.924 - mae: 177.891 - mean_q: 192.586\n",
      "\n",
      "Interval 91 (900000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.2500\n",
      "200 episodes - episode_reward: 162.500 [-30.000, 270.000] - loss: 373.905 - mae: 179.725 - mean_q: 194.462\n",
      "\n",
      "Interval 92 (910000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.5720\n",
      "200 episodes - episode_reward: 178.600 [-20.000, 320.000] - loss: 362.277 - mae: 176.650 - mean_q: 191.389\n",
      "\n",
      "Interval 93 (920000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.6350\n",
      "200 episodes - episode_reward: 181.750 [-40.000, 280.000] - loss: 357.128 - mae: 175.355 - mean_q: 189.866\n",
      "\n",
      "Interval 94 (930000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.6170\n",
      "200 episodes - episode_reward: 180.850 [-50.000, 310.000] - loss: 346.889 - mae: 173.502 - mean_q: 188.026\n",
      "\n",
      "Interval 95 (940000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.5760\n",
      "200 episodes - episode_reward: 178.800 [-50.000, 290.000] - loss: 361.512 - mae: 175.876 - mean_q: 190.689\n",
      "\n",
      "Interval 96 (950000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.5370\n",
      "200 episodes - episode_reward: 176.850 [-30.000, 300.000] - loss: 347.874 - mae: 174.404 - mean_q: 189.118\n",
      "\n",
      "Interval 97 (960000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.3560\n",
      "200 episodes - episode_reward: 167.800 [-40.000, 260.000] - loss: 344.773 - mae: 172.871 - mean_q: 187.403\n",
      "\n",
      "Interval 98 (970000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.6190\n",
      "200 episodes - episode_reward: 180.950 [-50.000, 290.000] - loss: 339.524 - mae: 171.229 - mean_q: 185.615\n",
      "\n",
      "Interval 99 (980000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.6440\n",
      "200 episodes - episode_reward: 182.200 [-50.000, 290.000] - loss: 332.724 - mae: 170.558 - mean_q: 184.696\n",
      "\n",
      "Interval 100 (990000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.7900\n",
      "200 episodes - episode_reward: 189.500 [20.000, 300.000] - loss: 341.123 - mae: 173.463 - mean_q: 187.828\n",
      "\n",
      "Interval 101 (1000000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.7970\n",
      "200 episodes - episode_reward: 189.850 [-50.000, 300.000] - loss: 351.473 - mae: 172.443 - mean_q: 186.974\n",
      "\n",
      "Interval 102 (1010000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.6130\n",
      "200 episodes - episode_reward: 180.650 [-50.000, 320.000] - loss: 352.468 - mae: 173.975 - mean_q: 188.772\n",
      "\n",
      "Interval 103 (1020000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.8260\n",
      "200 episodes - episode_reward: 191.300 [-40.000, 330.000] - loss: 365.280 - mae: 179.095 - mean_q: 194.150\n",
      "\n",
      "Interval 104 (1030000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.7810\n",
      "200 episodes - episode_reward: 189.050 [-30.000, 320.000] - loss: 368.412 - mae: 178.943 - mean_q: 193.976\n",
      "\n",
      "Interval 105 (1040000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 4.0120\n",
      "200 episodes - episode_reward: 200.600 [-40.000, 320.000] - loss: 365.421 - mae: 178.028 - mean_q: 193.024\n",
      "\n",
      "Interval 106 (1050000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.8610\n",
      "200 episodes - episode_reward: 193.050 [-40.000, 300.000] - loss: 371.743 - mae: 177.533 - mean_q: 192.631\n",
      "\n",
      "Interval 107 (1060000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.7350\n",
      "200 episodes - episode_reward: 186.750 [-40.000, 320.000] - loss: 366.786 - mae: 179.815 - mean_q: 195.034\n",
      "\n",
      "Interval 108 (1070000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.7060\n",
      "200 episodes - episode_reward: 185.300 [-50.000, 290.000] - loss: 376.076 - mae: 180.198 - mean_q: 195.523\n",
      "\n",
      "Interval 109 (1080000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.6850\n",
      "200 episodes - episode_reward: 184.250 [10.000, 290.000] - loss: 371.336 - mae: 181.403 - mean_q: 196.667\n",
      "\n",
      "Interval 110 (1090000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.8230\n",
      "200 episodes - episode_reward: 191.150 [-50.000, 310.000] - loss: 377.141 - mae: 180.520 - mean_q: 195.958\n",
      "\n",
      "Interval 111 (1100000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.8140\n",
      "200 episodes - episode_reward: 190.700 [-30.000, 300.000] - loss: 363.719 - mae: 176.530 - mean_q: 191.777\n",
      "\n",
      "Interval 112 (1110000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7280\n",
      "200 episodes - episode_reward: 186.400 [40.000, 310.000] - loss: 372.057 - mae: 178.624 - mean_q: 194.151\n",
      "\n",
      "Interval 113 (1120000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.9710\n",
      "200 episodes - episode_reward: 198.550 [-50.000, 350.000] - loss: 368.855 - mae: 178.706 - mean_q: 194.331\n",
      "\n",
      "Interval 114 (1130000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7320\n",
      "200 episodes - episode_reward: 186.600 [10.000, 300.000] - loss: 386.386 - mae: 182.103 - mean_q: 198.078\n",
      "\n",
      "Interval 115 (1140000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.5650\n",
      "200 episodes - episode_reward: 178.250 [-10.000, 310.000] - loss: 374.725 - mae: 179.420 - mean_q: 194.902\n",
      "\n",
      "Interval 116 (1150000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.9550\n",
      "200 episodes - episode_reward: 197.750 [-20.000, 350.000] - loss: 365.738 - mae: 177.519 - mean_q: 192.872\n",
      "\n",
      "Interval 117 (1160000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7120\n",
      "200 episodes - episode_reward: 185.600 [-50.000, 290.000] - loss: 366.406 - mae: 177.352 - mean_q: 192.217\n",
      "\n",
      "Interval 118 (1170000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.4680\n",
      "200 episodes - episode_reward: 173.400 [-50.000, 280.000] - loss: 358.514 - mae: 175.068 - mean_q: 189.741\n",
      "\n",
      "Interval 119 (1180000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.4400\n",
      "200 episodes - episode_reward: 172.000 [-20.000, 300.000] - loss: 359.995 - mae: 177.570 - mean_q: 192.166\n",
      "\n",
      "Interval 120 (1190000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.6780\n",
      "200 episodes - episode_reward: 183.900 [-20.000, 310.000] - loss: 354.318 - mae: 174.913 - mean_q: 189.377\n",
      "\n",
      "Interval 121 (1200000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 3.8320\n",
      "200 episodes - episode_reward: 191.600 [-40.000, 290.000] - loss: 362.522 - mae: 177.880 - mean_q: 192.935\n",
      "\n",
      "Interval 122 (1210000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.6560\n",
      "200 episodes - episode_reward: 182.800 [-20.000, 310.000] - loss: 363.327 - mae: 177.320 - mean_q: 192.115\n",
      "\n",
      "Interval 123 (1220000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.7740\n",
      "200 episodes - episode_reward: 188.700 [30.000, 330.000] - loss: 341.285 - mae: 171.860 - mean_q: 185.967\n",
      "\n",
      "Interval 124 (1230000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.8480\n",
      "200 episodes - episode_reward: 192.400 [-40.000, 320.000] - loss: 345.857 - mae: 173.893 - mean_q: 188.342\n",
      "\n",
      "Interval 125 (1240000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 3.9000\n",
      "200 episodes - episode_reward: 195.000 [-40.000, 320.000] - loss: 357.976 - mae: 174.904 - mean_q: 189.578\n",
      "\n",
      "Interval 126 (1250000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.6330\n",
      "200 episodes - episode_reward: 181.650 [-40.000, 330.000] - loss: 355.342 - mae: 175.966 - mean_q: 190.656\n",
      "\n",
      "Interval 127 (1260000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.9960\n",
      "200 episodes - episode_reward: 199.800 [50.000, 300.000] - loss: 357.981 - mae: 175.713 - mean_q: 190.384\n",
      "\n",
      "Interval 128 (1270000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.8700\n",
      "200 episodes - episode_reward: 193.500 [20.000, 300.000] - loss: 371.022 - mae: 178.344 - mean_q: 193.609\n",
      "\n",
      "Interval 129 (1280000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.6280\n",
      "200 episodes - episode_reward: 181.400 [20.000, 280.000] - loss: 365.747 - mae: 178.192 - mean_q: 192.795\n",
      "\n",
      "Interval 130 (1290000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.4420\n",
      "200 episodes - episode_reward: 172.100 [-50.000, 290.000] - loss: 358.980 - mae: 174.887 - mean_q: 189.346\n",
      "\n",
      "Interval 131 (1300000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 3.4700\n",
      "200 episodes - episode_reward: 173.500 [-20.000, 280.000] - loss: 372.819 - mae: 180.806 - mean_q: 195.650\n",
      "\n",
      "Interval 132 (1310000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 3.7670\n",
      "200 episodes - episode_reward: 188.350 [-20.000, 300.000] - loss: 356.745 - mae: 176.835 - mean_q: 191.111\n",
      "\n",
      "Interval 133 (1320000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 3.6620\n",
      "200 episodes - episode_reward: 183.100 [-10.000, 290.000] - loss: 345.777 - mae: 173.936 - mean_q: 188.130\n",
      "\n",
      "Interval 134 (1330000 steps performed)\n",
      "10000/10000 [==============================] - 58s 6ms/step - reward: 3.6220\n",
      "200 episodes - episode_reward: 181.100 [-20.000, 300.000] - loss: 343.118 - mae: 172.096 - mean_q: 186.115\n",
      "\n",
      "Interval 135 (1340000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 3.6270\n",
      "200 episodes - episode_reward: 181.350 [-40.000, 300.000] - loss: 346.652 - mae: 171.657 - mean_q: 185.964\n",
      "\n",
      "Interval 136 (1350000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 3.8830\n",
      "200 episodes - episode_reward: 194.150 [10.000, 350.000] - loss: 351.691 - mae: 176.055 - mean_q: 190.654\n",
      "\n",
      "Interval 137 (1360000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 3.7490\n",
      "200 episodes - episode_reward: 187.450 [-20.000, 330.000] - loss: 362.436 - mae: 179.450 - mean_q: 194.402\n",
      "\n",
      "Interval 138 (1370000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 3.9290\n",
      "200 episodes - episode_reward: 196.450 [30.000, 290.000] - loss: 360.866 - mae: 176.948 - mean_q: 191.506\n",
      "\n",
      "Interval 139 (1380000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 4.0030\n",
      "200 episodes - episode_reward: 200.150 [-40.000, 300.000] - loss: 360.468 - mae: 176.710 - mean_q: 191.344\n",
      "\n",
      "Interval 140 (1390000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 4.0630\n",
      "200 episodes - episode_reward: 203.150 [-20.000, 300.000] - loss: 365.080 - mae: 177.252 - mean_q: 192.024\n",
      "\n",
      "Interval 141 (1400000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.7850\n",
      "200 episodes - episode_reward: 189.250 [20.000, 300.000] - loss: 361.063 - mae: 177.698 - mean_q: 192.424\n",
      "\n",
      "Interval 142 (1410000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 3.7290\n",
      "200 episodes - episode_reward: 186.450 [-50.000, 320.000] - loss: 371.932 - mae: 179.603 - mean_q: 194.559\n",
      "\n",
      "Interval 143 (1420000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 3.7200\n",
      "200 episodes - episode_reward: 186.000 [-20.000, 320.000] - loss: 375.041 - mae: 180.234 - mean_q: 195.473\n",
      "\n",
      "Interval 144 (1430000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 4.1200\n",
      "200 episodes - episode_reward: 206.000 [-10.000, 300.000] - loss: 362.593 - mae: 179.223 - mean_q: 194.074\n",
      "\n",
      "Interval 145 (1440000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.7690\n",
      "200 episodes - episode_reward: 188.450 [-20.000, 310.000] - loss: 357.539 - mae: 177.429 - mean_q: 191.654\n",
      "\n",
      "Interval 146 (1450000 steps performed)\n",
      "10000/10000 [==============================] - 80s 8ms/step - reward: 3.7820\n",
      "200 episodes - episode_reward: 189.100 [-40.000, 300.000] - loss: 354.587 - mae: 175.021 - mean_q: 189.133\n",
      "\n",
      "Interval 147 (1460000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 3.5420\n",
      "200 episodes - episode_reward: 177.100 [-30.000, 290.000] - loss: 360.794 - mae: 178.587 - mean_q: 193.155\n",
      "\n",
      "Interval 148 (1470000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.8620\n",
      "200 episodes - episode_reward: 193.100 [20.000, 300.000] - loss: 359.003 - mae: 176.853 - mean_q: 191.292\n",
      "\n",
      "Interval 149 (1480000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.5000\n",
      "200 episodes - episode_reward: 175.000 [-40.000, 270.000] - loss: 341.567 - mae: 172.547 - mean_q: 186.656\n",
      "\n",
      "Interval 150 (1490000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.6900\n",
      "200 episodes - episode_reward: 184.500 [-10.000, 300.000] - loss: 344.020 - mae: 172.229 - mean_q: 186.406\n",
      "\n",
      "Interval 151 (1500000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.8580\n",
      "200 episodes - episode_reward: 192.900 [-30.000, 290.000] - loss: 332.097 - mae: 169.203 - mean_q: 183.259\n",
      "\n",
      "Interval 152 (1510000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.9760\n",
      "200 episodes - episode_reward: 198.800 [-10.000, 280.000] - loss: 350.688 - mae: 174.236 - mean_q: 188.831\n",
      "\n",
      "Interval 153 (1520000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 4.1820\n",
      "200 episodes - episode_reward: 209.100 [-20.000, 300.000] - loss: 363.877 - mae: 176.658 - mean_q: 191.500\n",
      "\n",
      "Interval 154 (1530000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 4.0790\n",
      "200 episodes - episode_reward: 203.950 [-20.000, 330.000] - loss: 377.150 - mae: 180.370 - mean_q: 195.886\n",
      "\n",
      "Interval 155 (1540000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 3.9430\n",
      "200 episodes - episode_reward: 197.150 [-50.000, 330.000] - loss: 378.401 - mae: 181.976 - mean_q: 197.027\n",
      "\n",
      "Interval 156 (1550000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.7220\n",
      "200 episodes - episode_reward: 186.100 [10.000, 300.000] - loss: 387.644 - mae: 182.108 - mean_q: 197.368\n",
      "\n",
      "Interval 157 (1560000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 4.0100\n",
      "200 episodes - episode_reward: 200.500 [-20.000, 300.000] - loss: 372.337 - mae: 180.010 - mean_q: 195.115\n",
      "\n",
      "Interval 158 (1570000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.9930\n",
      "200 episodes - episode_reward: 199.650 [-30.000, 300.000] - loss: 367.897 - mae: 179.721 - mean_q: 194.563\n",
      "\n",
      "Interval 159 (1580000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.8180\n",
      "200 episodes - episode_reward: 190.900 [30.000, 290.000] - loss: 368.802 - mae: 178.153 - mean_q: 193.223\n",
      "\n",
      "Interval 160 (1590000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 4.2920\n",
      "200 episodes - episode_reward: 214.600 [70.000, 340.000] - loss: 370.367 - mae: 177.937 - mean_q: 193.287\n",
      "\n",
      "Interval 161 (1600000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 3.8310\n",
      "200 episodes - episode_reward: 191.550 [0.000, 310.000] - loss: 379.293 - mae: 180.052 - mean_q: 195.338\n",
      "\n",
      "Interval 162 (1610000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 4.0390\n",
      "200 episodes - episode_reward: 201.950 [-50.000, 320.000] - loss: 368.960 - mae: 178.549 - mean_q: 193.642\n",
      "\n",
      "Interval 163 (1620000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.7350\n",
      "200 episodes - episode_reward: 186.750 [20.000, 300.000] - loss: 367.470 - mae: 179.683 - mean_q: 194.780\n",
      "\n",
      "Interval 164 (1630000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.9210\n",
      "200 episodes - episode_reward: 196.050 [50.000, 300.000] - loss: 361.186 - mae: 176.650 - mean_q: 191.252\n",
      "\n",
      "Interval 165 (1640000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.8300\n",
      "200 episodes - episode_reward: 191.500 [-30.000, 300.000] - loss: 360.284 - mae: 178.570 - mean_q: 193.440\n",
      "\n",
      "Interval 166 (1650000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 4.0050\n",
      "200 episodes - episode_reward: 200.250 [20.000, 300.000] - loss: 361.582 - mae: 176.868 - mean_q: 191.680\n",
      "\n",
      "Interval 167 (1660000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 4.0080\n",
      "200 episodes - episode_reward: 200.400 [-40.000, 310.000] - loss: 362.589 - mae: 177.128 - mean_q: 192.156\n",
      "\n",
      "Interval 168 (1670000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.8380\n",
      "200 episodes - episode_reward: 191.900 [-30.000, 290.000] - loss: 366.312 - mae: 179.327 - mean_q: 194.554\n",
      "\n",
      "Interval 169 (1680000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.8520\n",
      "200 episodes - episode_reward: 192.600 [10.000, 300.000] - loss: 365.630 - mae: 179.634 - mean_q: 194.568\n",
      "\n",
      "Interval 170 (1690000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.8460\n",
      "200 episodes - episode_reward: 192.300 [-10.000, 310.000] - loss: 373.063 - mae: 179.130 - mean_q: 194.068\n",
      "\n",
      "Interval 171 (1700000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 3.7170\n",
      "200 episodes - episode_reward: 185.850 [-20.000, 290.000] - loss: 372.429 - mae: 178.847 - mean_q: 194.115\n",
      "\n",
      "Interval 172 (1710000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.9470\n",
      "200 episodes - episode_reward: 197.350 [-20.000, 300.000] - loss: 366.692 - mae: 177.821 - mean_q: 193.330\n",
      "\n",
      "Interval 173 (1720000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.8610\n",
      "200 episodes - episode_reward: 193.050 [10.000, 280.000] - loss: 358.413 - mae: 174.780 - mean_q: 189.687\n",
      "\n",
      "Interval 174 (1730000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.8750\n",
      "200 episodes - episode_reward: 193.750 [-30.000, 320.000] - loss: 352.016 - mae: 172.959 - mean_q: 187.979\n",
      "\n",
      "Interval 175 (1740000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 4.1560\n",
      "200 episodes - episode_reward: 207.800 [30.000, 330.000] - loss: 359.126 - mae: 177.806 - mean_q: 193.230\n",
      "\n",
      "Interval 176 (1750000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 4.0560\n",
      "200 episodes - episode_reward: 202.800 [30.000, 310.000] - loss: 374.020 - mae: 181.070 - mean_q: 196.206\n",
      "\n",
      "Interval 177 (1760000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 3.8980\n",
      "200 episodes - episode_reward: 194.900 [-20.000, 290.000] - loss: 370.284 - mae: 178.880 - mean_q: 193.683\n",
      "\n",
      "Interval 178 (1770000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 4.1150\n",
      "200 episodes - episode_reward: 205.750 [90.000, 310.000] - loss: 367.790 - mae: 177.123 - mean_q: 191.970\n",
      "\n",
      "Interval 179 (1780000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 3.8150\n",
      "200 episodes - episode_reward: 190.750 [-50.000, 280.000] - loss: 363.205 - mae: 178.093 - mean_q: 192.769\n",
      "\n",
      "Interval 180 (1790000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.5430\n",
      "200 episodes - episode_reward: 177.150 [-50.000, 280.000] - loss: 363.290 - mae: 178.771 - mean_q: 193.267\n",
      "\n",
      "Interval 181 (1800000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.9570\n",
      "200 episodes - episode_reward: 197.850 [10.000, 300.000] - loss: 359.932 - mae: 174.987 - mean_q: 189.438\n",
      "\n",
      "Interval 182 (1810000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.8340\n",
      "200 episodes - episode_reward: 191.700 [-10.000, 300.000] - loss: 355.269 - mae: 175.385 - mean_q: 189.785\n",
      "\n",
      "Interval 183 (1820000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.9060\n",
      "200 episodes - episode_reward: 195.300 [40.000, 320.000] - loss: 355.868 - mae: 176.373 - mean_q: 190.854\n",
      "\n",
      "Interval 184 (1830000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.8690\n",
      "200 episodes - episode_reward: 193.450 [-40.000, 330.000] - loss: 352.041 - mae: 176.202 - mean_q: 190.661\n",
      "\n",
      "Interval 185 (1840000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 4.2050\n",
      "200 episodes - episode_reward: 210.250 [-30.000, 310.000] - loss: 350.229 - mae: 172.116 - mean_q: 186.384\n",
      "\n",
      "Interval 186 (1850000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.9610\n",
      "200 episodes - episode_reward: 198.050 [0.000, 330.000] - loss: 353.046 - mae: 175.302 - mean_q: 189.821\n",
      "\n",
      "Interval 187 (1860000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.8040\n",
      "200 episodes - episode_reward: 190.200 [-30.000, 310.000] - loss: 371.128 - mae: 178.432 - mean_q: 193.071\n",
      "\n",
      "Interval 188 (1870000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.7560\n",
      "200 episodes - episode_reward: 187.800 [-50.000, 330.000] - loss: 360.425 - mae: 175.001 - mean_q: 189.311\n",
      "\n",
      "Interval 189 (1880000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.6430\n",
      "200 episodes - episode_reward: 182.150 [-40.000, 300.000] - loss: 346.858 - mae: 173.210 - mean_q: 187.362\n",
      "\n",
      "Interval 190 (1890000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.7460\n",
      "200 episodes - episode_reward: 187.300 [-40.000, 290.000] - loss: 342.726 - mae: 172.519 - mean_q: 186.453\n",
      "\n",
      "Interval 191 (1900000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.8580\n",
      "200 episodes - episode_reward: 192.900 [-10.000, 280.000] - loss: 340.353 - mae: 172.916 - mean_q: 186.796\n",
      "\n",
      "Interval 192 (1910000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.8500\n",
      "200 episodes - episode_reward: 192.500 [-30.000, 300.000] - loss: 345.077 - mae: 172.224 - mean_q: 186.354\n",
      "\n",
      "Interval 193 (1920000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.9790\n",
      "200 episodes - episode_reward: 198.950 [-10.000, 300.000] - loss: 338.063 - mae: 169.312 - mean_q: 183.661\n",
      "\n",
      "Interval 194 (1930000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.8420\n",
      "200 episodes - episode_reward: 192.100 [-50.000, 320.000] - loss: 350.657 - mae: 174.453 - mean_q: 189.240\n",
      "\n",
      "Interval 195 (1940000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.8470\n",
      "200 episodes - episode_reward: 192.350 [-40.000, 310.000] - loss: 358.280 - mae: 176.976 - mean_q: 191.916\n",
      "\n",
      "Interval 196 (1950000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.9540\n",
      "200 episodes - episode_reward: 197.700 [10.000, 310.000] - loss: 369.780 - mae: 177.977 - mean_q: 193.098\n",
      "\n",
      "Interval 197 (1960000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.7740\n",
      "200 episodes - episode_reward: 188.700 [-10.000, 300.000] - loss: 358.905 - mae: 176.417 - mean_q: 191.411\n",
      "\n",
      "Interval 198 (1970000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.7800\n",
      "200 episodes - episode_reward: 189.000 [-30.000, 310.000] - loss: 348.412 - mae: 173.311 - mean_q: 187.720\n",
      "\n",
      "Interval 199 (1980000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.9260\n",
      "200 episodes - episode_reward: 196.300 [-50.000, 280.000] - loss: 352.730 - mae: 172.521 - mean_q: 187.105\n",
      "\n",
      "Interval 200 (1990000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 4.1280\n",
      "200 episodes - episode_reward: 206.400 [-40.000, 310.000] - loss: 352.723 - mae: 173.286 - mean_q: 187.979\n",
      "\n",
      "Interval 201 (2000000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 4.0390\n",
      "200 episodes - episode_reward: 201.950 [-40.000, 310.000] - loss: 360.567 - mae: 176.949 - mean_q: 192.140\n",
      "\n",
      "Interval 202 (2010000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.3190\n",
      "200 episodes - episode_reward: 215.950 [40.000, 300.000] - loss: 370.917 - mae: 178.246 - mean_q: 193.631\n",
      "\n",
      "Interval 203 (2020000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 4.1090\n",
      "200 episodes - episode_reward: 205.450 [30.000, 330.000] - loss: 376.304 - mae: 179.961 - mean_q: 195.508\n",
      "\n",
      "Interval 204 (2030000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.8730\n",
      "200 episodes - episode_reward: 193.650 [30.000, 300.000] - loss: 390.505 - mae: 182.904 - mean_q: 198.503\n",
      "\n",
      "Interval 205 (2040000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.6420\n",
      "200 episodes - episode_reward: 182.100 [-20.000, 300.000] - loss: 372.176 - mae: 180.781 - mean_q: 195.914\n",
      "\n",
      "Interval 206 (2050000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.8580\n",
      "200 episodes - episode_reward: 192.900 [20.000, 290.000] - loss: 366.214 - mae: 177.280 - mean_q: 192.141\n",
      "\n",
      "Interval 207 (2060000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.9330\n",
      "200 episodes - episode_reward: 196.650 [-40.000, 310.000] - loss: 350.429 - mae: 174.108 - mean_q: 188.545\n",
      "\n",
      "Interval 208 (2070000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.9660\n",
      "200 episodes - episode_reward: 198.300 [50.000, 320.000] - loss: 342.195 - mae: 172.079 - mean_q: 186.271\n",
      "\n",
      "Interval 209 (2080000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.9960\n",
      "200 episodes - episode_reward: 199.800 [-40.000, 300.000] - loss: 355.880 - mae: 176.225 - mean_q: 190.862\n",
      "\n",
      "Interval 210 (2090000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.9350\n",
      "200 episodes - episode_reward: 196.750 [-50.000, 300.000] - loss: 352.032 - mae: 173.770 - mean_q: 187.970\n",
      "\n",
      "Interval 211 (2100000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.7890\n",
      "200 episodes - episode_reward: 189.450 [-20.000, 290.000] - loss: 349.602 - mae: 175.164 - mean_q: 189.549\n",
      "\n",
      "Interval 212 (2110000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.9790\n",
      "200 episodes - episode_reward: 198.950 [10.000, 300.000] - loss: 348.821 - mae: 171.754 - mean_q: 185.844\n",
      "\n",
      "Interval 213 (2120000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 4.0410\n",
      "200 episodes - episode_reward: 202.050 [-50.000, 300.000] - loss: 337.488 - mae: 170.131 - mean_q: 184.052\n",
      "\n",
      "Interval 214 (2130000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 4.0440\n",
      "200 episodes - episode_reward: 202.200 [-30.000, 310.000] - loss: 346.658 - mae: 172.070 - mean_q: 186.170\n",
      "\n",
      "Interval 215 (2140000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.7870\n",
      "200 episodes - episode_reward: 189.350 [-50.000, 330.000] - loss: 356.580 - mae: 173.857 - mean_q: 188.359\n",
      "\n",
      "Interval 216 (2150000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.9640\n",
      "200 episodes - episode_reward: 198.200 [20.000, 310.000] - loss: 355.033 - mae: 174.195 - mean_q: 188.708\n",
      "\n",
      "Interval 217 (2160000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.9660\n",
      "200 episodes - episode_reward: 148.300 [-40.000, 270.000] - loss: 372.396 - mae: 177.346 - mean_q: 192.049\n",
      "\n",
      "Interval 218 (2170000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.0490\n",
      "200 episodes - episode_reward: 152.450 [-10.000, 250.000] - loss: 361.447 - mae: 177.534 - mean_q: 191.949\n",
      "\n",
      "Interval 219 (2180000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.1850\n",
      "200 episodes - episode_reward: 159.250 [-20.000, 290.000] - loss: 345.034 - mae: 172.245 - mean_q: 186.528\n",
      "\n",
      "Interval 220 (2190000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.4440\n",
      "200 episodes - episode_reward: 172.200 [-50.000, 300.000] - loss: 330.653 - mae: 167.844 - mean_q: 181.676\n",
      "\n",
      "Interval 221 (2200000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.4260\n",
      "200 episodes - episode_reward: 171.300 [-40.000, 320.000] - loss: 315.579 - mae: 166.737 - mean_q: 180.546\n",
      "\n",
      "Interval 222 (2210000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.4900\n",
      "200 episodes - episode_reward: 174.500 [-50.000, 280.000] - loss: 313.938 - mae: 163.711 - mean_q: 177.462\n",
      "\n",
      "Interval 223 (2220000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.6390\n",
      "200 episodes - episode_reward: 181.950 [-50.000, 290.000] - loss: 305.177 - mae: 159.903 - mean_q: 173.629\n",
      "\n",
      "Interval 224 (2230000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.3870\n",
      "200 episodes - episode_reward: 169.350 [-30.000, 300.000] - loss: 315.444 - mae: 164.436 - mean_q: 178.611\n",
      "\n",
      "Interval 225 (2240000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.9430\n",
      "200 episodes - episode_reward: 197.150 [-10.000, 310.000] - loss: 315.134 - mae: 165.584 - mean_q: 179.673\n",
      "\n",
      "Interval 226 (2250000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.6250\n",
      "200 episodes - episode_reward: 181.250 [-40.000, 290.000] - loss: 320.361 - mae: 165.319 - mean_q: 179.193\n",
      "\n",
      "Interval 227 (2260000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.8690\n",
      "200 episodes - episode_reward: 193.450 [-10.000, 310.000] - loss: 319.325 - mae: 166.652 - mean_q: 180.658\n",
      "\n",
      "Interval 228 (2270000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7360\n",
      "200 episodes - episode_reward: 186.800 [-50.000, 320.000] - loss: 322.135 - mae: 165.723 - mean_q: 179.555\n",
      "\n",
      "Interval 229 (2280000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.5280\n",
      "200 episodes - episode_reward: 176.400 [-50.000, 280.000] - loss: 323.002 - mae: 165.804 - mean_q: 179.843\n",
      "\n",
      "Interval 230 (2290000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7000\n",
      "200 episodes - episode_reward: 185.000 [-50.000, 310.000] - loss: 315.624 - mae: 167.294 - mean_q: 181.217\n",
      "\n",
      "Interval 231 (2300000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.7220\n",
      "200 episodes - episode_reward: 186.100 [-50.000, 280.000] - loss: 323.091 - mae: 166.703 - mean_q: 180.403\n",
      "\n",
      "Interval 232 (2310000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7560\n",
      "200 episodes - episode_reward: 187.800 [-40.000, 300.000] - loss: 314.149 - mae: 164.854 - mean_q: 178.337\n",
      "\n",
      "Interval 233 (2320000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 3.7540\n",
      "200 episodes - episode_reward: 187.700 [-20.000, 340.000] - loss: 318.347 - mae: 166.988 - mean_q: 180.689\n",
      "\n",
      "Interval 234 (2330000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.6390\n",
      "200 episodes - episode_reward: 181.950 [-50.000, 300.000] - loss: 314.044 - mae: 164.931 - mean_q: 178.416\n",
      "\n",
      "Interval 235 (2340000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.7120\n",
      "200 episodes - episode_reward: 185.600 [0.000, 290.000] - loss: 314.972 - mae: 166.444 - mean_q: 179.961\n",
      "\n",
      "Interval 236 (2350000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.4030\n",
      "200 episodes - episode_reward: 170.150 [-40.000, 290.000] - loss: 318.611 - mae: 165.200 - mean_q: 178.659\n",
      "\n",
      "Interval 237 (2360000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.5650\n",
      "200 episodes - episode_reward: 178.250 [-40.000, 290.000] - loss: 315.635 - mae: 162.966 - mean_q: 176.619\n",
      "\n",
      "Interval 238 (2370000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.9310\n",
      "200 episodes - episode_reward: 196.550 [-40.000, 300.000] - loss: 309.938 - mae: 163.933 - mean_q: 177.787\n",
      "\n",
      "Interval 239 (2380000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.7640\n",
      "200 episodes - episode_reward: 188.200 [-30.000, 290.000] - loss: 319.410 - mae: 166.426 - mean_q: 180.494\n",
      "\n",
      "Interval 240 (2390000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.4630\n",
      "200 episodes - episode_reward: 173.150 [-10.000, 310.000] - loss: 322.223 - mae: 164.844 - mean_q: 179.008\n",
      "\n",
      "Interval 241 (2400000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.6140\n",
      "200 episodes - episode_reward: 180.700 [-50.000, 290.000] - loss: 320.092 - mae: 163.346 - mean_q: 177.481\n",
      "\n",
      "Interval 242 (2410000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.6500\n",
      "200 episodes - episode_reward: 182.500 [-50.000, 280.000] - loss: 320.079 - mae: 164.642 - mean_q: 178.836\n",
      "\n",
      "Interval 243 (2420000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.3650\n",
      "200 episodes - episode_reward: 168.250 [-30.000, 270.000] - loss: 324.924 - mae: 169.543 - mean_q: 183.738\n",
      "\n",
      "Interval 244 (2430000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.5210\n",
      "200 episodes - episode_reward: 176.050 [-50.000, 280.000] - loss: 314.576 - mae: 166.369 - mean_q: 180.057\n",
      "\n",
      "Interval 245 (2440000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.7130\n",
      "200 episodes - episode_reward: 185.650 [30.000, 310.000] - loss: 300.209 - mae: 162.109 - mean_q: 175.606\n",
      "\n",
      "Interval 246 (2450000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.4600\n",
      "200 episodes - episode_reward: 173.000 [-50.000, 300.000] - loss: 316.951 - mae: 166.095 - mean_q: 180.040\n",
      "\n",
      "Interval 247 (2460000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.5840\n",
      "200 episodes - episode_reward: 179.200 [0.000, 290.000] - loss: 328.695 - mae: 167.706 - mean_q: 181.662\n",
      "\n",
      "Interval 248 (2470000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.9800\n",
      "200 episodes - episode_reward: 199.000 [20.000, 330.000] - loss: 322.983 - mae: 166.635 - mean_q: 180.750\n",
      "\n",
      "Interval 249 (2480000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.9320\n",
      "200 episodes - episode_reward: 196.600 [-10.000, 340.000] - loss: 329.224 - mae: 166.503 - mean_q: 180.755\n",
      "\n",
      "Interval 250 (2490000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.8130\n",
      "200 episodes - episode_reward: 190.650 [20.000, 290.000] - loss: 338.682 - mae: 170.703 - mean_q: 185.336\n",
      "\n",
      "Interval 251 (2500000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.7280\n",
      "200 episodes - episode_reward: 186.400 [-50.000, 290.000] - loss: 335.909 - mae: 168.308 - mean_q: 182.594\n",
      "\n",
      "Interval 252 (2510000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.6840\n",
      "200 episodes - episode_reward: 184.200 [50.000, 290.000] - loss: 331.653 - mae: 170.074 - mean_q: 184.472\n",
      "\n",
      "Interval 253 (2520000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.9110\n",
      "200 episodes - episode_reward: 195.550 [-20.000, 320.000] - loss: 327.299 - mae: 167.865 - mean_q: 182.037\n",
      "\n",
      "Interval 254 (2530000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.7290\n",
      "200 episodes - episode_reward: 186.450 [-20.000, 290.000] - loss: 324.480 - mae: 167.008 - mean_q: 181.024\n",
      "\n",
      "Interval 255 (2540000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.7610\n",
      "200 episodes - episode_reward: 188.050 [-20.000, 300.000] - loss: 325.286 - mae: 167.075 - mean_q: 181.231\n",
      "\n",
      "Interval 256 (2550000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.8990\n",
      "200 episodes - episode_reward: 194.950 [-50.000, 280.000] - loss: 326.186 - mae: 166.709 - mean_q: 180.847\n",
      "\n",
      "Interval 257 (2560000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.7620\n",
      "200 episodes - episode_reward: 188.100 [0.000, 270.000] - loss: 328.638 - mae: 169.251 - mean_q: 183.379\n",
      "\n",
      "Interval 258 (2570000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.9280\n",
      "200 episodes - episode_reward: 196.400 [-30.000, 300.000] - loss: 344.234 - mae: 171.588 - mean_q: 185.981\n",
      "\n",
      "Interval 259 (2580000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.7850\n",
      "200 episodes - episode_reward: 189.250 [30.000, 300.000] - loss: 337.418 - mae: 168.930 - mean_q: 183.199\n",
      "\n",
      "Interval 260 (2590000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.9120\n",
      "200 episodes - episode_reward: 195.600 [10.000, 310.000] - loss: 336.068 - mae: 170.427 - mean_q: 184.819\n",
      "\n",
      "Interval 261 (2600000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.9270\n",
      "200 episodes - episode_reward: 196.350 [20.000, 290.000] - loss: 336.125 - mae: 170.859 - mean_q: 185.342\n",
      "\n",
      "Interval 262 (2610000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7720\n",
      "200 episodes - episode_reward: 188.600 [0.000, 300.000] - loss: 334.830 - mae: 170.253 - mean_q: 184.663\n",
      "\n",
      "Interval 263 (2620000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.8710\n",
      "200 episodes - episode_reward: 193.550 [-50.000, 310.000] - loss: 330.464 - mae: 169.254 - mean_q: 183.187\n",
      "\n",
      "Interval 264 (2630000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.1290\n",
      "200 episodes - episode_reward: 206.450 [50.000, 320.000] - loss: 344.503 - mae: 172.158 - mean_q: 186.192\n",
      "\n",
      "Interval 265 (2640000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.1420\n",
      "200 episodes - episode_reward: 207.100 [30.000, 350.000] - loss: 339.388 - mae: 173.017 - mean_q: 187.003\n",
      "\n",
      "Interval 266 (2650000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7310\n",
      "200 episodes - episode_reward: 186.550 [-30.000, 310.000] - loss: 348.108 - mae: 172.191 - mean_q: 186.013\n",
      "\n",
      "Interval 267 (2660000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7130\n",
      "200 episodes - episode_reward: 185.650 [-50.000, 310.000] - loss: 335.732 - mae: 171.319 - mean_q: 185.327\n",
      "\n",
      "Interval 268 (2670000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7280\n",
      "200 episodes - episode_reward: 186.400 [-10.000, 310.000] - loss: 334.275 - mae: 169.031 - mean_q: 183.091\n",
      "\n",
      "Interval 269 (2680000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.6450\n",
      "200 episodes - episode_reward: 182.250 [-30.000, 330.000] - loss: 335.183 - mae: 170.215 - mean_q: 184.557\n",
      "\n",
      "Interval 270 (2690000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.8040\n",
      "200 episodes - episode_reward: 190.200 [0.000, 310.000] - loss: 338.302 - mae: 170.241 - mean_q: 184.549\n",
      "\n",
      "Interval 271 (2700000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.5950\n",
      "200 episodes - episode_reward: 179.750 [-40.000, 310.000] - loss: 337.101 - mae: 171.843 - mean_q: 186.378\n",
      "\n",
      "Interval 272 (2710000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.9160\n",
      "200 episodes - episode_reward: 195.800 [-40.000, 310.000] - loss: 348.462 - mae: 174.658 - mean_q: 189.072\n",
      "\n",
      "Interval 273 (2720000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7900\n",
      "200 episodes - episode_reward: 189.500 [-40.000, 300.000] - loss: 351.584 - mae: 174.882 - mean_q: 189.196\n",
      "\n",
      "Interval 274 (2730000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.8270\n",
      "200 episodes - episode_reward: 191.350 [20.000, 330.000] - loss: 337.196 - mae: 170.689 - mean_q: 184.465\n",
      "\n",
      "Interval 275 (2740000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.9270\n",
      "200 episodes - episode_reward: 196.350 [-40.000, 310.000] - loss: 323.382 - mae: 167.885 - mean_q: 181.553\n",
      "\n",
      "Interval 276 (2750000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.9490\n",
      "200 episodes - episode_reward: 197.450 [20.000, 300.000] - loss: 331.805 - mae: 170.214 - mean_q: 184.057\n",
      "\n",
      "Interval 277 (2760000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.6980\n",
      "200 episodes - episode_reward: 184.900 [-50.000, 310.000] - loss: 330.342 - mae: 167.080 - mean_q: 180.631\n",
      "\n",
      "Interval 278 (2770000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.8340\n",
      "200 episodes - episode_reward: 191.700 [-40.000, 340.000] - loss: 331.462 - mae: 171.166 - mean_q: 185.330\n",
      "\n",
      "Interval 279 (2780000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.9200\n",
      "200 episodes - episode_reward: 196.000 [-50.000, 300.000] - loss: 342.974 - mae: 172.957 - mean_q: 187.090\n",
      "\n",
      "Interval 280 (2790000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.1420\n",
      "200 episodes - episode_reward: 207.100 [-40.000, 310.000] - loss: 343.561 - mae: 173.592 - mean_q: 188.487\n",
      "\n",
      "Interval 281 (2800000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.1370\n",
      "200 episodes - episode_reward: 206.850 [90.000, 300.000] - loss: 361.302 - mae: 177.203 - mean_q: 192.404\n",
      "\n",
      "Interval 282 (2810000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.9970\n",
      "200 episodes - episode_reward: 199.850 [-50.000, 320.000] - loss: 358.086 - mae: 174.850 - mean_q: 189.612\n",
      "\n",
      "Interval 283 (2820000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.9650\n",
      "200 episodes - episode_reward: 198.250 [-20.000, 320.000] - loss: 350.283 - mae: 174.523 - mean_q: 189.075\n",
      "\n",
      "Interval 284 (2830000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.8720\n",
      "200 episodes - episode_reward: 193.600 [-50.000, 300.000] - loss: 343.243 - mae: 171.134 - mean_q: 185.405\n",
      "\n",
      "Interval 285 (2840000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.9970\n",
      "200 episodes - episode_reward: 199.850 [-20.000, 310.000] - loss: 342.025 - mae: 171.346 - mean_q: 185.312\n",
      "\n",
      "Interval 286 (2850000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.8950\n",
      "200 episodes - episode_reward: 194.750 [-50.000, 290.000] - loss: 335.445 - mae: 170.171 - mean_q: 184.001\n",
      "\n",
      "Interval 287 (2860000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.9470\n",
      "200 episodes - episode_reward: 197.350 [0.000, 330.000] - loss: 334.095 - mae: 171.130 - mean_q: 184.790\n",
      "\n",
      "Interval 288 (2870000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.8680\n",
      "200 episodes - episode_reward: 193.400 [30.000, 290.000] - loss: 333.151 - mae: 172.478 - mean_q: 186.158\n",
      "\n",
      "Interval 289 (2880000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.6650\n",
      "200 episodes - episode_reward: 183.250 [10.000, 280.000] - loss: 346.025 - mae: 173.136 - mean_q: 186.902\n",
      "\n",
      "Interval 290 (2890000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7740\n",
      "200 episodes - episode_reward: 188.700 [-30.000, 300.000] - loss: 336.012 - mae: 170.416 - mean_q: 184.054\n",
      "\n",
      "Interval 291 (2900000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.2850\n",
      "200 episodes - episode_reward: 164.250 [-40.000, 270.000] - loss: 331.546 - mae: 169.143 - mean_q: 182.753\n",
      "\n",
      "Interval 292 (2910000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.6820\n",
      "200 episodes - episode_reward: 184.100 [-40.000, 320.000] - loss: 332.628 - mae: 168.847 - mean_q: 182.449\n",
      "\n",
      "Interval 293 (2920000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.5660\n",
      "200 episodes - episode_reward: 178.300 [-30.000, 290.000] - loss: 316.288 - mae: 164.267 - mean_q: 177.630\n",
      "\n",
      "Interval 294 (2930000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7510\n",
      "200 episodes - episode_reward: 187.550 [0.000, 270.000] - loss: 297.863 - mae: 160.747 - mean_q: 173.787\n",
      "\n",
      "Interval 295 (2940000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7170\n",
      "200 episodes - episode_reward: 185.850 [-20.000, 310.000] - loss: 293.155 - mae: 159.547 - mean_q: 172.436\n",
      "\n",
      "Interval 296 (2950000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.0370\n",
      "200 episodes - episode_reward: 201.850 [20.000, 310.000] - loss: 298.532 - mae: 160.401 - mean_q: 173.508\n",
      "\n",
      "Interval 297 (2960000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.8670\n",
      "200 episodes - episode_reward: 193.350 [-50.000, 300.000] - loss: 311.405 - mae: 162.387 - mean_q: 175.584\n",
      "\n",
      "Interval 298 (2970000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.8410\n",
      "200 episodes - episode_reward: 192.050 [20.000, 320.000] - loss: 312.561 - mae: 165.715 - mean_q: 179.266\n",
      "\n",
      "Interval 299 (2980000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7080\n",
      "200 episodes - episode_reward: 185.400 [20.000, 270.000] - loss: 319.424 - mae: 166.215 - mean_q: 179.438\n",
      "\n",
      "Interval 300 (2990000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7160\n",
      "200 episodes - episode_reward: 185.800 [-10.000, 290.000] - loss: 305.445 - mae: 161.892 - mean_q: 174.742\n",
      "\n",
      "Interval 301 (3000000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.4610\n",
      "200 episodes - episode_reward: 173.050 [10.000, 300.000] - loss: 307.233 - mae: 163.315 - mean_q: 176.283\n",
      "\n",
      "Interval 302 (3010000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.5950\n",
      "200 episodes - episode_reward: 179.750 [-10.000, 300.000] - loss: 304.150 - mae: 162.522 - mean_q: 175.460\n",
      "\n",
      "Interval 303 (3020000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.8030\n",
      "200 episodes - episode_reward: 190.150 [-20.000, 300.000] - loss: 299.715 - mae: 159.940 - mean_q: 172.733\n",
      "\n",
      "Interval 304 (3030000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.8490\n",
      "200 episodes - episode_reward: 192.450 [-10.000, 300.000] - loss: 301.347 - mae: 162.425 - mean_q: 175.550\n",
      "\n",
      "Interval 305 (3040000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7820\n",
      "200 episodes - episode_reward: 189.100 [-20.000, 270.000] - loss: 316.724 - mae: 164.896 - mean_q: 178.482\n",
      "\n",
      "Interval 306 (3050000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7590\n",
      "200 episodes - episode_reward: 187.950 [0.000, 300.000] - loss: 325.052 - mae: 168.460 - mean_q: 182.416\n",
      "\n",
      "Interval 307 (3060000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.8590\n",
      "200 episodes - episode_reward: 192.950 [50.000, 310.000] - loss: 335.709 - mae: 169.007 - mean_q: 183.147\n",
      "\n",
      "Interval 308 (3070000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7970\n",
      "200 episodes - episode_reward: 189.850 [-20.000, 290.000] - loss: 319.482 - mae: 164.776 - mean_q: 178.460\n",
      "\n",
      "Interval 309 (3080000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.8050\n",
      "200 episodes - episode_reward: 190.250 [-40.000, 320.000] - loss: 315.502 - mae: 163.861 - mean_q: 177.641\n",
      "\n",
      "Interval 310 (3090000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.0020\n",
      "200 episodes - episode_reward: 200.100 [-40.000, 290.000] - loss: 311.501 - mae: 164.842 - mean_q: 178.726\n",
      "\n",
      "Interval 311 (3100000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7510\n",
      "200 episodes - episode_reward: 187.550 [-40.000, 290.000] - loss: 318.610 - mae: 165.845 - mean_q: 179.586\n",
      "\n",
      "Interval 312 (3110000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.9220\n",
      "200 episodes - episode_reward: 196.100 [0.000, 300.000] - loss: 315.965 - mae: 164.487 - mean_q: 178.239\n",
      "\n",
      "Interval 313 (3120000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.8810\n",
      "200 episodes - episode_reward: 194.050 [-30.000, 290.000] - loss: 313.527 - mae: 163.635 - mean_q: 177.404\n",
      "\n",
      "Interval 314 (3130000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.9230\n",
      "200 episodes - episode_reward: 196.150 [0.000, 290.000] - loss: 320.922 - mae: 164.352 - mean_q: 178.264\n",
      "\n",
      "Interval 315 (3140000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.8850\n",
      "200 episodes - episode_reward: 194.250 [-10.000, 300.000] - loss: 329.256 - mae: 168.689 - mean_q: 182.890\n",
      "\n",
      "Interval 316 (3150000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7680\n",
      "200 episodes - episode_reward: 188.400 [-40.000, 310.000] - loss: 324.251 - mae: 168.270 - mean_q: 182.133\n",
      "\n",
      "Interval 317 (3160000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.8320\n",
      "200 episodes - episode_reward: 191.600 [-30.000, 300.000] - loss: 325.757 - mae: 164.920 - mean_q: 178.421\n",
      "\n",
      "Interval 318 (3170000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.0830\n",
      "200 episodes - episode_reward: 204.150 [-50.000, 310.000] - loss: 322.206 - mae: 167.324 - mean_q: 181.090\n",
      "\n",
      "Interval 319 (3180000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.9080\n",
      "200 episodes - episode_reward: 195.400 [10.000, 320.000] - loss: 328.158 - mae: 168.608 - mean_q: 182.480\n",
      "\n",
      "Interval 320 (3190000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.1050\n",
      "200 episodes - episode_reward: 205.250 [-50.000, 310.000] - loss: 333.764 - mae: 169.877 - mean_q: 183.916\n",
      "\n",
      "Interval 321 (3200000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.8860\n",
      "200 episodes - episode_reward: 194.300 [-50.000, 300.000] - loss: 335.267 - mae: 169.446 - mean_q: 183.512\n",
      "\n",
      "Interval 322 (3210000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.8010\n",
      "200 episodes - episode_reward: 190.050 [-50.000, 310.000] - loss: 330.200 - mae: 169.052 - mean_q: 182.756\n",
      "\n",
      "Interval 323 (3220000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.0140\n",
      "200 episodes - episode_reward: 200.700 [0.000, 320.000] - loss: 327.395 - mae: 168.480 - mean_q: 182.051\n",
      "\n",
      "Interval 324 (3230000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.9350\n",
      "200 episodes - episode_reward: 196.750 [0.000, 310.000] - loss: 320.891 - mae: 166.885 - mean_q: 180.135\n",
      "\n",
      "Interval 325 (3240000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.0640\n",
      "200 episodes - episode_reward: 203.200 [30.000, 300.000] - loss: 317.470 - mae: 165.208 - mean_q: 178.573\n",
      "\n",
      "Interval 326 (3250000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.8590\n",
      "200 episodes - episode_reward: 192.950 [10.000, 290.000] - loss: 322.277 - mae: 167.702 - mean_q: 181.318\n",
      "\n",
      "Interval 327 (3260000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.9810\n",
      "200 episodes - episode_reward: 199.050 [-50.000, 300.000] - loss: 325.792 - mae: 167.921 - mean_q: 181.268\n",
      "\n",
      "Interval 328 (3270000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.0680\n",
      "200 episodes - episode_reward: 203.400 [-30.000, 320.000] - loss: 321.671 - mae: 167.160 - mean_q: 180.480\n",
      "\n",
      "Interval 329 (3280000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.8900\n",
      "200 episodes - episode_reward: 194.500 [-10.000, 300.000] - loss: 326.185 - mae: 166.854 - mean_q: 179.973\n",
      "\n",
      "Interval 330 (3290000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7430\n",
      "200 episodes - episode_reward: 187.150 [-20.000, 290.000] - loss: 315.132 - mae: 163.829 - mean_q: 176.639\n",
      "\n",
      "Interval 331 (3300000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7380\n",
      "200 episodes - episode_reward: 186.900 [-10.000, 300.000] - loss: 312.776 - mae: 164.268 - mean_q: 177.527\n",
      "\n",
      "Interval 332 (3310000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.9110\n",
      "200 episodes - episode_reward: 195.550 [-20.000, 290.000] - loss: 328.374 - mae: 167.247 - mean_q: 180.844\n",
      "\n",
      "Interval 333 (3320000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7000\n",
      "200 episodes - episode_reward: 185.000 [-50.000, 310.000] - loss: 325.654 - mae: 169.663 - mean_q: 183.550\n",
      "\n",
      "Interval 334 (3330000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.9330\n",
      "200 episodes - episode_reward: 196.650 [-30.000, 290.000] - loss: 325.685 - mae: 167.250 - mean_q: 181.074\n",
      "\n",
      "Interval 335 (3340000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.9900\n",
      "200 episodes - episode_reward: 199.500 [-50.000, 310.000] - loss: 313.027 - mae: 163.023 - mean_q: 176.296\n",
      "\n",
      "Interval 336 (3350000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.5370\n",
      "200 episodes - episode_reward: 176.850 [-30.000, 290.000] - loss: 317.154 - mae: 166.866 - mean_q: 180.272\n",
      "\n",
      "Interval 337 (3360000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.8570\n",
      "200 episodes - episode_reward: 192.850 [-30.000, 330.000] - loss: 317.296 - mae: 164.477 - mean_q: 177.795\n",
      "\n",
      "Interval 338 (3370000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.8260\n",
      "200 episodes - episode_reward: 191.300 [30.000, 310.000] - loss: 318.936 - mae: 166.473 - mean_q: 179.968\n",
      "\n",
      "Interval 339 (3380000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.9740\n",
      "200 episodes - episode_reward: 198.700 [-20.000, 310.000] - loss: 319.470 - mae: 166.109 - mean_q: 179.446\n",
      "\n",
      "Interval 340 (3390000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7840\n",
      "200 episodes - episode_reward: 189.200 [-20.000, 300.000] - loss: 316.410 - mae: 165.624 - mean_q: 178.983\n",
      "\n",
      "Interval 341 (3400000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.9100\n",
      "200 episodes - episode_reward: 195.500 [10.000, 320.000] - loss: 311.880 - mae: 164.428 - mean_q: 177.764\n",
      "\n",
      "Interval 342 (3410000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.9000\n",
      "200 episodes - episode_reward: 195.000 [-50.000, 320.000] - loss: 313.445 - mae: 163.694 - mean_q: 176.980\n",
      "\n",
      "Interval 343 (3420000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.9140\n",
      "200 episodes - episode_reward: 195.700 [-50.000, 300.000] - loss: 316.442 - mae: 166.022 - mean_q: 179.621\n",
      "\n",
      "Interval 344 (3430000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7720\n",
      "200 episodes - episode_reward: 188.600 [-40.000, 290.000] - loss: 320.853 - mae: 166.459 - mean_q: 179.891\n",
      "\n",
      "Interval 345 (3440000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.8260\n",
      "200 episodes - episode_reward: 191.300 [10.000, 300.000] - loss: 317.624 - mae: 163.342 - mean_q: 176.606\n",
      "\n",
      "Interval 346 (3450000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.6390\n",
      "200 episodes - episode_reward: 181.950 [-30.000, 290.000] - loss: 302.884 - mae: 161.008 - mean_q: 174.012\n",
      "\n",
      "Interval 347 (3460000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.8240\n",
      "200 episodes - episode_reward: 191.200 [-10.000, 300.000] - loss: 305.595 - mae: 163.344 - mean_q: 176.284\n",
      "\n",
      "Interval 348 (3470000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.8200\n",
      "200 episodes - episode_reward: 191.000 [20.000, 310.000] - loss: 310.084 - mae: 165.503 - mean_q: 178.770\n",
      "\n",
      "Interval 349 (3480000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.1140\n",
      "200 episodes - episode_reward: 205.700 [50.000, 330.000] - loss: 318.584 - mae: 165.469 - mean_q: 178.670\n",
      "\n",
      "Interval 350 (3490000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7760\n",
      "200 episodes - episode_reward: 188.800 [-50.000, 290.000] - loss: 319.341 - mae: 166.773 - mean_q: 180.131\n",
      "\n",
      "Interval 351 (3500000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.0110\n",
      "200 episodes - episode_reward: 200.550 [-20.000, 320.000] - loss: 324.716 - mae: 167.316 - mean_q: 180.834\n",
      "\n",
      "Interval 352 (3510000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.0400\n",
      "200 episodes - episode_reward: 202.000 [-50.000, 330.000] - loss: 332.986 - mae: 167.331 - mean_q: 181.366\n",
      "\n",
      "Interval 353 (3520000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.0530\n",
      "200 episodes - episode_reward: 202.650 [-30.000, 300.000] - loss: 331.434 - mae: 168.969 - mean_q: 182.975\n",
      "\n",
      "Interval 354 (3530000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.1520\n",
      "200 episodes - episode_reward: 207.600 [30.000, 300.000] - loss: 344.068 - mae: 172.285 - mean_q: 186.753\n",
      "\n",
      "Interval 355 (3540000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.8970\n",
      "200 episodes - episode_reward: 194.850 [30.000, 290.000] - loss: 339.713 - mae: 172.234 - mean_q: 186.618\n",
      "\n",
      "Interval 356 (3550000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.0290\n",
      "200 episodes - episode_reward: 201.450 [-40.000, 310.000] - loss: 336.985 - mae: 170.753 - mean_q: 184.970\n",
      "\n",
      "Interval 357 (3560000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.0540\n",
      "200 episodes - episode_reward: 202.700 [-10.000, 320.000] - loss: 338.667 - mae: 170.323 - mean_q: 184.267\n",
      "\n",
      "Interval 358 (3570000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.0690\n",
      "200 episodes - episode_reward: 203.450 [10.000, 310.000] - loss: 325.870 - mae: 168.554 - mean_q: 182.357\n",
      "\n",
      "Interval 359 (3580000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7520\n",
      "200 episodes - episode_reward: 187.600 [-30.000, 320.000] - loss: 331.974 - mae: 169.073 - mean_q: 182.832\n",
      "\n",
      "Interval 360 (3590000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.1420\n",
      "200 episodes - episode_reward: 207.100 [70.000, 340.000] - loss: 320.468 - mae: 166.025 - mean_q: 179.348\n",
      "\n",
      "Interval 361 (3600000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7760\n",
      "200 episodes - episode_reward: 188.800 [-10.000, 310.000] - loss: 320.158 - mae: 166.082 - mean_q: 179.187\n",
      "\n",
      "Interval 362 (3610000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.8640\n",
      "200 episodes - episode_reward: 193.200 [-10.000, 300.000] - loss: 316.884 - mae: 167.184 - mean_q: 180.341\n",
      "\n",
      "Interval 363 (3620000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.6680\n",
      "200 episodes - episode_reward: 183.400 [-50.000, 290.000] - loss: 316.555 - mae: 164.493 - mean_q: 177.371\n",
      "\n",
      "Interval 364 (3630000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.5590\n",
      "200 episodes - episode_reward: 177.950 [-20.000, 290.000] - loss: 317.953 - mae: 166.996 - mean_q: 180.033\n",
      "\n",
      "Interval 365 (3640000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7040\n",
      "200 episodes - episode_reward: 185.200 [-40.000, 300.000] - loss: 303.466 - mae: 161.779 - mean_q: 174.484\n",
      "\n",
      "Interval 366 (3650000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.0300\n",
      "200 episodes - episode_reward: 201.500 [20.000, 300.000] - loss: 296.416 - mae: 159.861 - mean_q: 172.525\n",
      "\n",
      "Interval 367 (3660000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.8460\n",
      "200 episodes - episode_reward: 192.300 [-50.000, 310.000] - loss: 302.107 - mae: 161.842 - mean_q: 174.662\n",
      "\n",
      "Interval 368 (3670000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.8600\n",
      "200 episodes - episode_reward: 193.000 [0.000, 290.000] - loss: 313.681 - mae: 164.622 - mean_q: 177.710\n",
      "\n",
      "Interval 369 (3680000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.8990\n",
      "200 episodes - episode_reward: 194.950 [-20.000, 290.000] - loss: 312.555 - mae: 165.340 - mean_q: 178.525\n",
      "\n",
      "Interval 370 (3690000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.9200\n",
      "200 episodes - episode_reward: 196.000 [-40.000, 300.000] - loss: 321.240 - mae: 166.387 - mean_q: 179.722\n",
      "\n",
      "Interval 371 (3700000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7140\n",
      "200 episodes - episode_reward: 185.700 [-40.000, 310.000] - loss: 315.227 - mae: 165.259 - mean_q: 178.340\n",
      "\n",
      "Interval 372 (3710000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.8800\n",
      "200 episodes - episode_reward: 194.000 [10.000, 310.000] - loss: 317.717 - mae: 166.692 - mean_q: 179.972\n",
      "\n",
      "Interval 373 (3720000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.1340\n",
      "200 episodes - episode_reward: 206.700 [-50.000, 320.000] - loss: 312.524 - mae: 164.095 - mean_q: 177.144\n",
      "\n",
      "Interval 374 (3730000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.8210\n",
      "200 episodes - episode_reward: 191.050 [-30.000, 310.000] - loss: 319.197 - mae: 168.506 - mean_q: 182.211\n",
      "\n",
      "Interval 375 (3740000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.9160\n",
      "200 episodes - episode_reward: 195.800 [-10.000, 340.000] - loss: 331.263 - mae: 168.881 - mean_q: 182.590\n",
      "\n",
      "Interval 376 (3750000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.9240\n",
      "200 episodes - episode_reward: 196.200 [-10.000, 310.000] - loss: 317.471 - mae: 165.485 - mean_q: 179.063\n",
      "\n",
      "Interval 377 (3760000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7820\n",
      "200 episodes - episode_reward: 189.100 [20.000, 290.000] - loss: 318.708 - mae: 165.644 - mean_q: 179.206\n",
      "\n",
      "Interval 378 (3770000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.8590\n",
      "200 episodes - episode_reward: 192.950 [-40.000, 310.000] - loss: 322.684 - mae: 165.815 - mean_q: 179.022\n",
      "\n",
      "Interval 379 (3780000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7250\n",
      "200 episodes - episode_reward: 186.250 [-30.000, 300.000] - loss: 317.743 - mae: 166.203 - mean_q: 179.205\n",
      "\n",
      "Interval 380 (3790000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.9250\n",
      "200 episodes - episode_reward: 196.250 [-30.000, 300.000] - loss: 322.119 - mae: 166.871 - mean_q: 179.849\n",
      "\n",
      "Interval 381 (3800000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.9630\n",
      "200 episodes - episode_reward: 198.150 [10.000, 360.000] - loss: 315.025 - mae: 166.623 - mean_q: 179.651\n",
      "\n",
      "Interval 382 (3810000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.0530\n",
      "200 episodes - episode_reward: 202.650 [-30.000, 300.000] - loss: 333.642 - mae: 169.725 - mean_q: 182.989\n",
      "\n",
      "Interval 383 (3820000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.8960\n",
      "200 episodes - episode_reward: 194.800 [-40.000, 310.000] - loss: 320.132 - mae: 165.673 - mean_q: 178.780\n",
      "\n",
      "Interval 384 (3830000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.0050\n",
      "200 episodes - episode_reward: 200.250 [20.000, 300.000] - loss: 322.107 - mae: 167.874 - mean_q: 181.138\n",
      "\n",
      "Interval 385 (3840000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.0870\n",
      "200 episodes - episode_reward: 204.350 [10.000, 330.000] - loss: 337.984 - mae: 170.925 - mean_q: 184.573\n",
      "\n",
      "Interval 386 (3850000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.1650\n",
      "200 episodes - episode_reward: 208.250 [10.000, 330.000] - loss: 339.320 - mae: 170.884 - mean_q: 184.485\n",
      "\n",
      "Interval 387 (3860000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.0150\n",
      "200 episodes - episode_reward: 200.750 [-50.000, 310.000] - loss: 345.706 - mae: 171.693 - mean_q: 185.343\n",
      "\n",
      "Interval 388 (3870000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.8650\n",
      "200 episodes - episode_reward: 193.250 [-30.000, 280.000] - loss: 337.818 - mae: 172.144 - mean_q: 185.634\n",
      "\n",
      "Interval 389 (3880000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.8870\n",
      "200 episodes - episode_reward: 194.350 [-50.000, 300.000] - loss: 341.750 - mae: 173.284 - mean_q: 186.752\n",
      "\n",
      "Interval 390 (3890000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7250\n",
      "200 episodes - episode_reward: 186.250 [-40.000, 330.000] - loss: 337.999 - mae: 171.914 - mean_q: 185.387\n",
      "\n",
      "Interval 391 (3900000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.9160\n",
      "200 episodes - episode_reward: 195.800 [-40.000, 300.000] - loss: 327.308 - mae: 168.546 - mean_q: 181.771\n",
      "\n",
      "Interval 392 (3910000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.8840\n",
      "200 episodes - episode_reward: 194.200 [-50.000, 290.000] - loss: 321.378 - mae: 167.771 - mean_q: 181.230\n",
      "\n",
      "Interval 393 (3920000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.9580\n",
      "200 episodes - episode_reward: 197.900 [-30.000, 310.000] - loss: 331.890 - mae: 167.422 - mean_q: 181.266\n",
      "\n",
      "Interval 394 (3930000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.9810\n",
      "200 episodes - episode_reward: 199.050 [-10.000, 310.000] - loss: 319.505 - mae: 165.646 - mean_q: 179.126\n",
      "\n",
      "Interval 395 (3940000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.4930\n",
      "200 episodes - episode_reward: 174.650 [-50.000, 300.000] - loss: 334.026 - mae: 170.443 - mean_q: 184.216\n",
      "\n",
      "Interval 396 (3950000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7720\n",
      "200 episodes - episode_reward: 188.600 [-30.000, 300.000] - loss: 333.542 - mae: 169.520 - mean_q: 183.177\n",
      "\n",
      "Interval 397 (3960000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.6490\n",
      "200 episodes - episode_reward: 182.450 [-40.000, 320.000] - loss: 330.200 - mae: 170.555 - mean_q: 184.027\n",
      "\n",
      "Interval 398 (3970000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.9270\n",
      "200 episodes - episode_reward: 196.350 [0.000, 290.000] - loss: 333.070 - mae: 168.348 - mean_q: 181.796\n",
      "\n",
      "Interval 399 (3980000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.1690\n",
      "200 episodes - episode_reward: 208.450 [-10.000, 340.000] - loss: 317.395 - mae: 167.328 - mean_q: 181.002\n",
      "\n",
      "Interval 400 (3990000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.1130\n",
      "200 episodes - episode_reward: 205.650 [10.000, 350.000] - loss: 329.587 - mae: 169.233 - mean_q: 183.176\n",
      "\n",
      "Interval 401 (4000000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.0950\n",
      "200 episodes - episode_reward: 204.750 [-20.000, 300.000] - loss: 337.273 - mae: 169.760 - mean_q: 183.965\n",
      "\n",
      "Interval 402 (4010000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.3860\n",
      "200 episodes - episode_reward: 219.300 [80.000, 310.000] - loss: 340.342 - mae: 171.928 - mean_q: 186.059\n",
      "\n",
      "Interval 403 (4020000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.2330\n",
      "200 episodes - episode_reward: 211.650 [-20.000, 330.000] - loss: 341.489 - mae: 171.907 - mean_q: 185.984\n",
      "\n",
      "Interval 404 (4030000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.3260\n",
      "200 episodes - episode_reward: 216.300 [50.000, 310.000] - loss: 354.098 - mae: 176.253 - mean_q: 190.785\n",
      "\n",
      "Interval 405 (4040000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.9320\n",
      "200 episodes - episode_reward: 196.600 [-30.000, 280.000] - loss: 357.318 - mae: 175.465 - mean_q: 189.913\n",
      "\n",
      "Interval 406 (4050000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.9080\n",
      "200 episodes - episode_reward: 195.400 [-30.000, 290.000] - loss: 353.699 - mae: 175.029 - mean_q: 189.246\n",
      "\n",
      "Interval 407 (4060000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7350\n",
      "200 episodes - episode_reward: 186.750 [0.000, 290.000] - loss: 348.125 - mae: 174.675 - mean_q: 188.910\n",
      "\n",
      "Interval 408 (4070000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.8270\n",
      "200 episodes - episode_reward: 191.350 [0.000, 320.000] - loss: 338.992 - mae: 170.708 - mean_q: 184.531\n",
      "\n",
      "Interval 409 (4080000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7140\n",
      "200 episodes - episode_reward: 185.700 [-10.000, 320.000] - loss: 320.789 - mae: 166.486 - mean_q: 179.620\n",
      "\n",
      "Interval 410 (4090000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.1040\n",
      "200 episodes - episode_reward: 205.200 [10.000, 310.000] - loss: 318.286 - mae: 166.111 - mean_q: 179.375\n",
      "\n",
      "Interval 411 (4100000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.9260\n",
      "200 episodes - episode_reward: 196.300 [-40.000, 310.000] - loss: 316.954 - mae: 167.023 - mean_q: 180.205\n",
      "\n",
      "Interval 412 (4110000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.0590\n",
      "200 episodes - episode_reward: 202.950 [0.000, 310.000] - loss: 334.275 - mae: 169.354 - mean_q: 182.794\n",
      "\n",
      "Interval 413 (4120000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.0420\n",
      "200 episodes - episode_reward: 202.100 [20.000, 320.000] - loss: 328.678 - mae: 168.022 - mean_q: 181.280\n",
      "\n",
      "Interval 414 (4130000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.1460\n",
      "200 episodes - episode_reward: 207.300 [20.000, 310.000] - loss: 335.787 - mae: 170.863 - mean_q: 184.564\n",
      "\n",
      "Interval 415 (4140000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.0180\n",
      "200 episodes - episode_reward: 200.900 [-40.000, 340.000] - loss: 339.179 - mae: 171.031 - mean_q: 184.720\n",
      "\n",
      "Interval 416 (4150000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7620\n",
      "200 episodes - episode_reward: 188.100 [-20.000, 300.000] - loss: 343.950 - mae: 173.024 - mean_q: 186.928\n",
      "\n",
      "Interval 417 (4160000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7980\n",
      "200 episodes - episode_reward: 189.900 [-20.000, 330.000] - loss: 360.742 - mae: 176.628 - mean_q: 191.015\n",
      "\n",
      "Interval 418 (4170000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.1310\n",
      "200 episodes - episode_reward: 206.550 [-30.000, 310.000] - loss: 360.056 - mae: 176.068 - mean_q: 190.327\n",
      "\n",
      "Interval 419 (4180000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.1730\n",
      "200 episodes - episode_reward: 208.650 [-50.000, 350.000] - loss: 354.171 - mae: 177.192 - mean_q: 191.564\n",
      "\n",
      "Interval 420 (4190000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.0630\n",
      "200 episodes - episode_reward: 203.150 [20.000, 320.000] - loss: 370.098 - mae: 179.377 - mean_q: 194.102\n",
      "\n",
      "Interval 421 (4200000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.9380\n",
      "200 episodes - episode_reward: 196.900 [-50.000, 350.000] - loss: 368.229 - mae: 178.063 - mean_q: 192.551\n",
      "\n",
      "Interval 422 (4210000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.8220\n",
      "200 episodes - episode_reward: 191.100 [10.000, 310.000] - loss: 346.028 - mae: 174.433 - mean_q: 188.447\n",
      "\n",
      "Interval 423 (4220000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.0910\n",
      "200 episodes - episode_reward: 204.550 [-40.000, 350.000] - loss: 343.164 - mae: 173.451 - mean_q: 187.345\n",
      "\n",
      "Interval 424 (4230000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.9660\n",
      "200 episodes - episode_reward: 198.300 [40.000, 310.000] - loss: 329.778 - mae: 169.230 - mean_q: 182.567\n",
      "\n",
      "Interval 425 (4240000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.6690\n",
      "200 episodes - episode_reward: 183.450 [-40.000, 290.000] - loss: 326.781 - mae: 168.934 - mean_q: 182.248\n",
      "\n",
      "Interval 426 (4250000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.8270\n",
      "200 episodes - episode_reward: 191.350 [30.000, 300.000] - loss: 324.853 - mae: 168.578 - mean_q: 181.890\n",
      "\n",
      "Interval 427 (4260000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.5650\n",
      "200 episodes - episode_reward: 178.250 [0.000, 290.000] - loss: 333.264 - mae: 169.245 - mean_q: 182.611\n",
      "\n",
      "Interval 428 (4270000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.1910\n",
      "200 episodes - episode_reward: 209.550 [30.000, 290.000] - loss: 331.043 - mae: 170.443 - mean_q: 184.105\n",
      "\n",
      "Interval 429 (4280000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.1670\n",
      "200 episodes - episode_reward: 208.350 [20.000, 310.000] - loss: 340.067 - mae: 170.430 - mean_q: 184.227\n",
      "\n",
      "Interval 430 (4290000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.1390\n",
      "200 episodes - episode_reward: 206.950 [-20.000, 300.000] - loss: 327.223 - mae: 169.817 - mean_q: 183.353\n",
      "\n",
      "Interval 431 (4300000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.6980\n",
      "200 episodes - episode_reward: 184.900 [-30.000, 290.000] - loss: 333.306 - mae: 169.895 - mean_q: 183.507\n",
      "\n",
      "Interval 432 (4310000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.6760\n",
      "200 episodes - episode_reward: 183.800 [-40.000, 320.000] - loss: 331.496 - mae: 170.298 - mean_q: 183.864\n",
      "\n",
      "Interval 433 (4320000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7770\n",
      "200 episodes - episode_reward: 188.850 [-50.000, 300.000] - loss: 337.077 - mae: 170.565 - mean_q: 184.207\n",
      "\n",
      "Interval 434 (4330000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.9750\n",
      "200 episodes - episode_reward: 198.750 [-50.000, 280.000] - loss: 336.189 - mae: 170.754 - mean_q: 184.441\n",
      "\n",
      "Interval 435 (4340000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.0420\n",
      "200 episodes - episode_reward: 202.100 [10.000, 310.000] - loss: 336.289 - mae: 170.790 - mean_q: 184.869\n",
      "\n",
      "Interval 436 (4350000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.2160\n",
      "200 episodes - episode_reward: 210.800 [-10.000, 290.000] - loss: 336.705 - mae: 171.763 - mean_q: 186.000\n",
      "\n",
      "Interval 437 (4360000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.0770\n",
      "200 episodes - episode_reward: 203.850 [-30.000, 300.000] - loss: 353.392 - mae: 172.744 - mean_q: 186.910\n",
      "\n",
      "Interval 438 (4370000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.8780\n",
      "200 episodes - episode_reward: 193.900 [10.000, 290.000] - loss: 341.121 - mae: 171.375 - mean_q: 185.266\n",
      "\n",
      "Interval 439 (4380000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.0280\n",
      "200 episodes - episode_reward: 201.400 [60.000, 310.000] - loss: 338.176 - mae: 167.990 - mean_q: 181.439\n",
      "\n",
      "Interval 440 (4390000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.8650\n",
      "200 episodes - episode_reward: 193.250 [-40.000, 370.000] - loss: 330.901 - mae: 168.892 - mean_q: 182.392\n",
      "\n",
      "Interval 441 (4400000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.2430\n",
      "200 episodes - episode_reward: 212.150 [50.000, 320.000] - loss: 340.856 - mae: 173.001 - mean_q: 186.681\n",
      "\n",
      "Interval 442 (4410000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.0000\n",
      "200 episodes - episode_reward: 200.000 [10.000, 350.000] - loss: 349.802 - mae: 173.603 - mean_q: 187.211\n",
      "\n",
      "Interval 443 (4420000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.1320\n",
      "200 episodes - episode_reward: 206.600 [40.000, 310.000] - loss: 333.744 - mae: 171.020 - mean_q: 184.651\n",
      "\n",
      "Interval 444 (4430000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.2080\n",
      "200 episodes - episode_reward: 210.400 [50.000, 330.000] - loss: 346.286 - mae: 171.250 - mean_q: 185.046\n",
      "\n",
      "Interval 445 (4440000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.9770\n",
      "200 episodes - episode_reward: 198.850 [50.000, 310.000] - loss: 342.153 - mae: 173.292 - mean_q: 187.152\n",
      "\n",
      "Interval 446 (4450000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.2300\n",
      "200 episodes - episode_reward: 211.500 [0.000, 310.000] - loss: 342.225 - mae: 173.615 - mean_q: 187.554\n",
      "\n",
      "Interval 447 (4460000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.2270\n",
      "200 episodes - episode_reward: 211.350 [10.000, 340.000] - loss: 341.521 - mae: 172.180 - mean_q: 186.205\n",
      "\n",
      "Interval 448 (4470000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7420\n",
      "200 episodes - episode_reward: 187.100 [0.000, 330.000] - loss: 345.911 - mae: 172.949 - mean_q: 187.187\n",
      "\n",
      "Interval 449 (4480000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.2890\n",
      "200 episodes - episode_reward: 164.450 [-50.000, 300.000] - loss: 364.616 - mae: 178.717 - mean_q: 193.232\n",
      "\n",
      "Interval 450 (4490000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.1470\n",
      "200 episodes - episode_reward: 207.350 [70.000, 320.000] - loss: 350.992 - mae: 175.546 - mean_q: 189.734\n",
      "\n",
      "Interval 451 (4500000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.1640\n",
      "200 episodes - episode_reward: 208.200 [10.000, 310.000] - loss: 346.900 - mae: 173.210 - mean_q: 187.324\n",
      "\n",
      "Interval 452 (4510000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.2390\n",
      "200 episodes - episode_reward: 211.950 [0.000, 340.000] - loss: 347.666 - mae: 174.721 - mean_q: 188.781\n",
      "\n",
      "Interval 453 (4520000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.6440\n",
      "200 episodes - episode_reward: 182.200 [-20.000, 300.000] - loss: 350.267 - mae: 175.287 - mean_q: 188.895\n",
      "\n",
      "Interval 454 (4530000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7550\n",
      "200 episodes - episode_reward: 187.750 [-30.000, 300.000] - loss: 347.018 - mae: 172.596 - mean_q: 186.076\n",
      "\n",
      "Interval 455 (4540000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.8910\n",
      "200 episodes - episode_reward: 194.550 [-10.000, 290.000] - loss: 355.209 - mae: 174.013 - mean_q: 187.919\n",
      "\n",
      "Interval 456 (4550000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.1930\n",
      "200 episodes - episode_reward: 209.650 [90.000, 300.000] - loss: 346.390 - mae: 173.784 - mean_q: 187.758\n",
      "\n",
      "Interval 457 (4560000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.2270\n",
      "200 episodes - episode_reward: 211.350 [-10.000, 340.000] - loss: 350.699 - mae: 174.374 - mean_q: 188.592\n",
      "\n",
      "Interval 458 (4570000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.9090\n",
      "200 episodes - episode_reward: 195.450 [10.000, 320.000] - loss: 350.534 - mae: 174.781 - mean_q: 188.964\n",
      "\n",
      "Interval 459 (4580000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.5780\n",
      "200 episodes - episode_reward: 178.900 [-30.000, 280.000] - loss: 339.026 - mae: 169.425 - mean_q: 183.042\n",
      "\n",
      "Interval 460 (4590000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7810\n",
      "200 episodes - episode_reward: 189.050 [-10.000, 290.000] - loss: 323.959 - mae: 166.660 - mean_q: 180.277\n",
      "\n",
      "Interval 461 (4600000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.8000\n",
      "200 episodes - episode_reward: 190.000 [-10.000, 290.000] - loss: 327.203 - mae: 167.683 - mean_q: 181.342\n",
      "\n",
      "Interval 462 (4610000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.6330\n",
      "200 episodes - episode_reward: 181.650 [-20.000, 280.000] - loss: 319.547 - mae: 165.121 - mean_q: 178.672\n",
      "\n",
      "Interval 463 (4620000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7910\n",
      "200 episodes - episode_reward: 189.550 [-30.000, 290.000] - loss: 309.567 - mae: 162.308 - mean_q: 175.630\n",
      "\n",
      "Interval 464 (4630000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7940\n",
      "200 episodes - episode_reward: 189.700 [-30.000, 310.000] - loss: 293.129 - mae: 158.909 - mean_q: 171.799\n",
      "\n",
      "Interval 465 (4640000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.8380\n",
      "200 episodes - episode_reward: 191.900 [-30.000, 290.000] - loss: 302.403 - mae: 163.403 - mean_q: 176.580\n",
      "\n",
      "Interval 466 (4650000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.8820\n",
      "200 episodes - episode_reward: 194.100 [10.000, 290.000] - loss: 312.357 - mae: 164.047 - mean_q: 177.432\n",
      "\n",
      "Interval 467 (4660000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.8740\n",
      "200 episodes - episode_reward: 193.700 [-20.000, 280.000] - loss: 312.441 - mae: 165.113 - mean_q: 178.429\n",
      "\n",
      "Interval 468 (4670000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.9360\n",
      "200 episodes - episode_reward: 196.800 [10.000, 300.000] - loss: 317.661 - mae: 166.127 - mean_q: 179.317\n",
      "\n",
      "Interval 469 (4680000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7750\n",
      "200 episodes - episode_reward: 188.750 [-30.000, 300.000] - loss: 323.300 - mae: 166.968 - mean_q: 180.360\n",
      "\n",
      "Interval 470 (4690000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.3270\n",
      "200 episodes - episode_reward: 166.350 [-40.000, 290.000] - loss: 324.886 - mae: 169.677 - mean_q: 183.164\n",
      "\n",
      "Interval 471 (4700000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.0020\n",
      "200 episodes - episode_reward: 200.100 [20.000, 300.000] - loss: 326.857 - mae: 167.294 - mean_q: 180.784\n",
      "\n",
      "Interval 472 (4710000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.1720\n",
      "200 episodes - episode_reward: 208.600 [110.000, 310.000] - loss: 324.920 - mae: 168.922 - mean_q: 182.553\n",
      "\n",
      "Interval 473 (4720000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.2280\n",
      "200 episodes - episode_reward: 211.400 [30.000, 300.000] - loss: 338.650 - mae: 172.455 - mean_q: 186.418\n",
      "\n",
      "Interval 474 (4730000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.2780\n",
      "200 episodes - episode_reward: 213.900 [-30.000, 310.000] - loss: 346.976 - mae: 171.141 - mean_q: 185.144\n",
      "\n",
      "Interval 475 (4740000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 4.1770\n",
      "200 episodes - episode_reward: 208.850 [-50.000, 330.000] - loss: 338.111 - mae: 169.567 - mean_q: 183.415\n",
      "\n",
      "Interval 476 (4750000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.0280\n",
      "200 episodes - episode_reward: 201.400 [-50.000, 300.000] - loss: 347.031 - mae: 170.510 - mean_q: 184.420\n",
      "\n",
      "Interval 477 (4760000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.2330\n",
      "200 episodes - episode_reward: 211.650 [50.000, 300.000] - loss: 346.191 - mae: 173.833 - mean_q: 187.858\n",
      "\n",
      "Interval 478 (4770000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.9600\n",
      "200 episodes - episode_reward: 198.000 [-50.000, 310.000] - loss: 353.337 - mae: 174.668 - mean_q: 188.467\n",
      "\n",
      "Interval 479 (4780000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.0820\n",
      "200 episodes - episode_reward: 204.100 [-40.000, 320.000] - loss: 338.615 - mae: 172.967 - mean_q: 186.599\n",
      "\n",
      "Interval 480 (4790000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.8520\n",
      "200 episodes - episode_reward: 192.600 [-50.000, 290.000] - loss: 337.509 - mae: 172.036 - mean_q: 185.579\n",
      "\n",
      "Interval 481 (4800000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.9850\n",
      "200 episodes - episode_reward: 199.250 [10.000, 300.000] - loss: 333.441 - mae: 170.581 - mean_q: 184.005\n",
      "\n",
      "Interval 482 (4810000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7360\n",
      "200 episodes - episode_reward: 186.800 [0.000, 300.000] - loss: 341.157 - mae: 172.680 - mean_q: 186.319\n",
      "\n",
      "Interval 483 (4820000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.0140\n",
      "200 episodes - episode_reward: 200.700 [-10.000, 340.000] - loss: 335.261 - mae: 171.528 - mean_q: 185.160\n",
      "\n",
      "Interval 484 (4830000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.2040\n",
      "200 episodes - episode_reward: 210.200 [10.000, 300.000] - loss: 347.576 - mae: 172.395 - mean_q: 186.269\n",
      "\n",
      "Interval 485 (4840000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.2540\n",
      "200 episodes - episode_reward: 212.700 [10.000, 310.000] - loss: 332.870 - mae: 170.121 - mean_q: 183.625\n",
      "\n",
      "Interval 486 (4850000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.2700\n",
      "200 episodes - episode_reward: 213.500 [-40.000, 340.000] - loss: 329.148 - mae: 169.312 - mean_q: 182.825\n",
      "\n",
      "Interval 487 (4860000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.1990\n",
      "200 episodes - episode_reward: 209.950 [-50.000, 310.000] - loss: 333.655 - mae: 168.656 - mean_q: 182.116\n",
      "\n",
      "Interval 488 (4870000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.9260\n",
      "200 episodes - episode_reward: 196.300 [-30.000, 320.000] - loss: 338.075 - mae: 172.907 - mean_q: 186.581\n",
      "\n",
      "Interval 489 (4880000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.6180\n",
      "200 episodes - episode_reward: 180.900 [-40.000, 310.000] - loss: 345.554 - mae: 173.051 - mean_q: 186.822\n",
      "\n",
      "Interval 490 (4890000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.0120\n",
      "200 episodes - episode_reward: 200.600 [-10.000, 330.000] - loss: 343.410 - mae: 172.040 - mean_q: 185.903\n",
      "\n",
      "Interval 491 (4900000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7460\n",
      "200 episodes - episode_reward: 187.300 [-50.000, 300.000] - loss: 336.198 - mae: 171.116 - mean_q: 184.836\n",
      "\n",
      "Interval 492 (4910000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7240\n",
      "200 episodes - episode_reward: 186.200 [-10.000, 330.000] - loss: 322.270 - mae: 168.959 - mean_q: 182.655\n",
      "\n",
      "Interval 493 (4920000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.8240\n",
      "200 episodes - episode_reward: 191.200 [-20.000, 290.000] - loss: 312.898 - mae: 165.394 - mean_q: 178.699\n",
      "\n",
      "Interval 494 (4930000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7420\n",
      "200 episodes - episode_reward: 187.100 [-30.000, 280.000] - loss: 301.076 - mae: 161.836 - mean_q: 174.647\n",
      "\n",
      "Interval 495 (4940000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.8470\n",
      "200 episodes - episode_reward: 192.350 [-20.000, 320.000] - loss: 302.773 - mae: 160.723 - mean_q: 173.563\n",
      "\n",
      "Interval 496 (4950000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.1330\n",
      "200 episodes - episode_reward: 206.650 [0.000, 300.000] - loss: 309.522 - mae: 165.076 - mean_q: 178.430\n",
      "\n",
      "Interval 497 (4960000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.2590\n",
      "200 episodes - episode_reward: 212.950 [40.000, 310.000] - loss: 346.123 - mae: 172.874 - mean_q: 186.992\n",
      "\n",
      "Interval 498 (4970000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.0500\n",
      "200 episodes - episode_reward: 202.500 [10.000, 310.000] - loss: 340.470 - mae: 172.187 - mean_q: 186.033\n",
      "\n",
      "Interval 499 (4980000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.2610\n",
      "200 episodes - episode_reward: 213.050 [-10.000, 340.000] - loss: 350.446 - mae: 175.343 - mean_q: 189.245\n",
      "\n",
      "Interval 500 (4990000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.1970\n",
      "200 episodes - episode_reward: 209.850 [40.000, 320.000] - loss: 351.209 - mae: 176.545 - mean_q: 190.663\n",
      "\n",
      "Interval 501 (5000000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.9750\n",
      "200 episodes - episode_reward: 198.750 [-20.000, 290.000] - loss: 344.642 - mae: 173.756 - mean_q: 187.383\n",
      "\n",
      "Interval 502 (5010000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.0680\n",
      "200 episodes - episode_reward: 203.400 [-10.000, 300.000] - loss: 326.695 - mae: 169.864 - mean_q: 183.129\n",
      "\n",
      "Interval 503 (5020000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.0480\n",
      "200 episodes - episode_reward: 202.400 [-20.000, 310.000] - loss: 325.651 - mae: 168.269 - mean_q: 181.619\n",
      "\n",
      "Interval 504 (5030000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.3790\n",
      "200 episodes - episode_reward: 218.950 [90.000, 320.000] - loss: 324.818 - mae: 167.894 - mean_q: 181.522\n",
      "\n",
      "Interval 505 (5040000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.2410\n",
      "200 episodes - episode_reward: 212.050 [10.000, 300.000] - loss: 336.548 - mae: 170.537 - mean_q: 184.406\n",
      "\n",
      "Interval 506 (5050000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.1230\n",
      "200 episodes - episode_reward: 206.150 [-20.000, 300.000] - loss: 344.424 - mae: 173.059 - mean_q: 187.274\n",
      "\n",
      "Interval 507 (5060000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7360\n",
      "200 episodes - episode_reward: 186.800 [-50.000, 310.000] - loss: 346.797 - mae: 170.891 - mean_q: 184.785\n",
      "\n",
      "Interval 508 (5070000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.1020\n",
      "200 episodes - episode_reward: 205.100 [40.000, 320.000] - loss: 327.082 - mae: 167.546 - mean_q: 181.045\n",
      "\n",
      "Interval 509 (5080000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7390\n",
      "200 episodes - episode_reward: 186.950 [-40.000, 300.000] - loss: 330.183 - mae: 171.958 - mean_q: 185.502\n",
      "\n",
      "Interval 510 (5090000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.2430\n",
      "200 episodes - episode_reward: 212.150 [-20.000, 330.000] - loss: 333.745 - mae: 170.400 - mean_q: 183.908\n",
      "\n",
      "Interval 511 (5100000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.0770\n",
      "200 episodes - episode_reward: 203.850 [10.000, 300.000] - loss: 332.214 - mae: 170.150 - mean_q: 183.786\n",
      "\n",
      "Interval 512 (5110000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.1120\n",
      "200 episodes - episode_reward: 205.600 [0.000, 330.000] - loss: 329.540 - mae: 168.935 - mean_q: 182.163\n",
      "\n",
      "Interval 513 (5120000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.6780\n",
      "200 episodes - episode_reward: 183.900 [-20.000, 290.000] - loss: 326.450 - mae: 169.802 - mean_q: 183.055\n",
      "\n",
      "Interval 514 (5130000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.9520\n",
      "200 episodes - episode_reward: 197.600 [-20.000, 300.000] - loss: 327.618 - mae: 168.062 - mean_q: 181.125\n",
      "\n",
      "Interval 515 (5140000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.9810\n",
      "200 episodes - episode_reward: 199.050 [-20.000, 340.000] - loss: 326.356 - mae: 166.538 - mean_q: 179.671\n",
      "\n",
      "Interval 516 (5150000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.3490\n",
      "200 episodes - episode_reward: 217.450 [90.000, 340.000] - loss: 314.474 - mae: 165.480 - mean_q: 178.500\n",
      "\n",
      "Interval 517 (5160000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.5020\n",
      "200 episodes - episode_reward: 225.100 [10.000, 330.000] - loss: 327.382 - mae: 169.063 - mean_q: 182.660\n",
      "\n",
      "Interval 518 (5170000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.3020\n",
      "200 episodes - episode_reward: 215.100 [0.000, 300.000] - loss: 344.924 - mae: 174.150 - mean_q: 188.342\n",
      "\n",
      "Interval 519 (5180000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 53s 5ms/step - reward: 4.2820\n",
      "200 episodes - episode_reward: 214.100 [-10.000, 310.000] - loss: 346.758 - mae: 173.765 - mean_q: 187.822\n",
      "\n",
      "Interval 520 (5190000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.2820\n",
      "200 episodes - episode_reward: 214.100 [-20.000, 320.000] - loss: 353.667 - mae: 175.314 - mean_q: 189.433\n",
      "\n",
      "Interval 521 (5200000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.1210\n",
      "200 episodes - episode_reward: 206.050 [-10.000, 320.000] - loss: 342.299 - mae: 172.764 - mean_q: 186.634\n",
      "\n",
      "Interval 522 (5210000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.9040\n",
      "200 episodes - episode_reward: 195.200 [20.000, 310.000] - loss: 339.491 - mae: 172.389 - mean_q: 185.935\n",
      "\n",
      "Interval 523 (5220000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.6130\n",
      "200 episodes - episode_reward: 180.650 [-40.000, 310.000] - loss: 328.542 - mae: 167.568 - mean_q: 180.553\n",
      "\n",
      "Interval 524 (5230000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.0420\n",
      "200 episodes - episode_reward: 202.100 [20.000, 300.000] - loss: 319.516 - mae: 165.241 - mean_q: 178.490\n",
      "\n",
      "Interval 525 (5240000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.9090\n",
      "200 episodes - episode_reward: 195.450 [-50.000, 310.000] - loss: 322.874 - mae: 167.899 - mean_q: 181.414\n",
      "\n",
      "Interval 526 (5250000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.0060\n",
      "200 episodes - episode_reward: 200.300 [-40.000, 300.000] - loss: 325.013 - mae: 166.881 - mean_q: 180.267\n",
      "\n",
      "Interval 527 (5260000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.1760\n",
      "200 episodes - episode_reward: 208.800 [-10.000, 300.000] - loss: 333.661 - mae: 168.282 - mean_q: 182.043\n",
      "\n",
      "Interval 528 (5270000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.3600\n",
      "200 episodes - episode_reward: 218.000 [80.000, 320.000] - loss: 331.879 - mae: 169.891 - mean_q: 183.881\n",
      "\n",
      "Interval 529 (5280000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.3350\n",
      "200 episodes - episode_reward: 216.750 [-10.000, 350.000] - loss: 338.112 - mae: 171.567 - mean_q: 185.657\n",
      "\n",
      "Interval 530 (5290000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.3470\n",
      "200 episodes - episode_reward: 217.350 [50.000, 320.000] - loss: 345.838 - mae: 172.564 - mean_q: 186.576\n",
      "\n",
      "Interval 531 (5300000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.0910\n",
      "200 episodes - episode_reward: 204.550 [10.000, 330.000] - loss: 341.572 - mae: 170.855 - mean_q: 184.749\n",
      "\n",
      "Interval 532 (5310000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.1300\n",
      "200 episodes - episode_reward: 206.500 [-30.000, 300.000] - loss: 341.098 - mae: 170.430 - mean_q: 184.321\n",
      "\n",
      "Interval 533 (5320000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 2.7010\n",
      "200 episodes - episode_reward: 135.050 [-50.000, 320.000] - loss: 344.786 - mae: 172.199 - mean_q: 186.125\n",
      "\n",
      "Interval 534 (5330000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 2.5010\n",
      "200 episodes - episode_reward: 125.050 [-50.000, 280.000] - loss: 320.447 - mae: 166.398 - mean_q: 179.430\n",
      "\n",
      "Interval 535 (5340000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 2.8520\n",
      "200 episodes - episode_reward: 142.600 [-20.000, 250.000] - loss: 275.274 - mae: 154.202 - mean_q: 166.061\n",
      "\n",
      "Interval 536 (5350000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.2930\n",
      "200 episodes - episode_reward: 164.650 [-50.000, 290.000] - loss: 256.035 - mae: 147.421 - mean_q: 159.059\n",
      "\n",
      "Interval 537 (5360000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.9150\n",
      "200 episodes - episode_reward: 195.750 [30.000, 320.000] - loss: 316.249 - mae: 164.520 - mean_q: 178.215\n",
      "\n",
      "Interval 538 (5370000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7250\n",
      "200 episodes - episode_reward: 186.250 [0.000, 290.000] - loss: 340.269 - mae: 171.835 - mean_q: 185.930\n",
      "\n",
      "Interval 539 (5380000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.2330\n",
      "200 episodes - episode_reward: 211.650 [0.000, 330.000] - loss: 332.355 - mae: 170.709 - mean_q: 184.483\n",
      "\n",
      "Interval 540 (5390000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.1520\n",
      "200 episodes - episode_reward: 207.600 [-30.000, 300.000] - loss: 328.250 - mae: 168.847 - mean_q: 182.393\n",
      "\n",
      "Interval 541 (5400000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.1470\n",
      "200 episodes - episode_reward: 207.350 [80.000, 310.000] - loss: 323.301 - mae: 168.341 - mean_q: 181.851\n",
      "\n",
      "Interval 542 (5410000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.9610\n",
      "200 episodes - episode_reward: 198.050 [30.000, 320.000] - loss: 332.400 - mae: 168.633 - mean_q: 182.283\n",
      "\n",
      "Interval 543 (5420000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8070\n",
      "200 episodes - episode_reward: 140.350 [-40.000, 280.000] - loss: 336.030 - mae: 170.720 - mean_q: 184.337\n",
      "\n",
      "Interval 544 (5430000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.0810\n",
      "200 episodes - episode_reward: 154.050 [-40.000, 290.000] - loss: 307.330 - mae: 162.479 - mean_q: 175.170\n",
      "\n",
      "Interval 545 (5440000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.3200\n",
      "200 episodes - episode_reward: 166.000 [10.000, 330.000] - loss: 273.572 - mae: 152.054 - mean_q: 163.816\n",
      "\n",
      "Interval 546 (5450000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.5130: \n",
      "200 episodes - episode_reward: 175.650 [40.000, 290.000] - loss: 249.674 - mae: 147.837 - mean_q: 159.194\n",
      "\n",
      "Interval 547 (5460000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.5190\n",
      "200 episodes - episode_reward: 175.950 [40.000, 280.000] - loss: 232.159 - mae: 140.615 - mean_q: 151.440\n",
      "\n",
      "Interval 548 (5470000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.3650\n",
      "200 episodes - episode_reward: 168.250 [-40.000, 270.000] - loss: 220.943 - mae: 137.527 - mean_q: 148.226\n",
      "\n",
      "Interval 549 (5480000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.3770\n",
      "200 episodes - episode_reward: 168.850 [0.000, 270.000] - loss: 233.170 - mae: 142.553 - mean_q: 153.739\n",
      "\n",
      "Interval 550 (5490000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.6230\n",
      "200 episodes - episode_reward: 181.150 [-30.000, 280.000] - loss: 249.088 - mae: 146.810 - mean_q: 158.491\n",
      "\n",
      "Interval 551 (5500000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.5950\n",
      "200 episodes - episode_reward: 179.750 [30.000, 300.000] - loss: 253.854 - mae: 145.937 - mean_q: 157.876\n",
      "\n",
      "Interval 552 (5510000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.9570\n",
      "200 episodes - episode_reward: 147.850 [-40.000, 280.000] - loss: 248.855 - mae: 146.211 - mean_q: 158.000\n",
      "\n",
      "Interval 553 (5520000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.2200\n",
      "200 episodes - episode_reward: 161.000 [-50.000, 310.000] - loss: 238.004 - mae: 142.602 - mean_q: 154.136\n",
      "\n",
      "Interval 554 (5530000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.4990\n",
      "200 episodes - episode_reward: 174.950 [-50.000, 300.000] - loss: 237.673 - mae: 142.198 - mean_q: 153.604\n",
      "\n",
      "Interval 555 (5540000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.4180\n",
      "200 episodes - episode_reward: 170.900 [-30.000, 300.000] - loss: 246.342 - mae: 146.008 - mean_q: 157.604\n",
      "\n",
      "Interval 556 (5550000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.6290\n",
      "200 episodes - episode_reward: 181.450 [-20.000, 300.000] - loss: 257.439 - mae: 150.669 - mean_q: 162.586\n",
      "\n",
      "Interval 557 (5560000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7730\n",
      "200 episodes - episode_reward: 188.650 [-20.000, 310.000] - loss: 261.629 - mae: 150.922 - mean_q: 162.983\n",
      "\n",
      "Interval 558 (5570000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7870\n",
      "200 episodes - episode_reward: 189.350 [-30.000, 290.000] - loss: 268.189 - mae: 151.250 - mean_q: 163.254\n",
      "\n",
      "Interval 559 (5580000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.8480\n",
      "200 episodes - episode_reward: 192.400 [0.000, 280.000] - loss: 280.955 - mae: 155.011 - mean_q: 167.441\n",
      "\n",
      "Interval 560 (5590000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.0590\n",
      "200 episodes - episode_reward: 202.950 [10.000, 320.000] - loss: 299.166 - mae: 161.198 - mean_q: 174.029\n",
      "\n",
      "Interval 561 (5600000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.2770\n",
      "200 episodes - episode_reward: 213.850 [-20.000, 320.000] - loss: 324.048 - mae: 167.272 - mean_q: 180.721\n",
      "\n",
      "Interval 562 (5610000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.9160\n",
      "200 episodes - episode_reward: 195.800 [-10.000, 330.000] - loss: 332.087 - mae: 169.963 - mean_q: 183.133\n",
      "\n",
      "Interval 563 (5620000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.9550\n",
      "200 episodes - episode_reward: 197.750 [-30.000, 310.000] - loss: 311.580 - mae: 165.440 - mean_q: 177.833\n",
      "\n",
      "Interval 564 (5630000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.8550\n",
      "200 episodes - episode_reward: 192.750 [-50.000, 300.000] - loss: 303.869 - mae: 161.859 - mean_q: 174.214\n",
      "\n",
      "Interval 565 (5640000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.1660\n",
      "200 episodes - episode_reward: 208.300 [-40.000, 290.000] - loss: 306.946 - mae: 162.960 - mean_q: 175.668\n",
      "\n",
      "Interval 566 (5650000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.2840\n",
      "200 episodes - episode_reward: 214.200 [-20.000, 310.000] - loss: 304.964 - mae: 161.186 - mean_q: 173.717\n",
      "\n",
      "Interval 567 (5660000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.2360\n",
      "200 episodes - episode_reward: 211.800 [90.000, 320.000] - loss: 306.969 - mae: 163.289 - mean_q: 176.252\n",
      "\n",
      "Interval 568 (5670000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.1700\n",
      "200 episodes - episode_reward: 208.500 [30.000, 330.000] - loss: 325.138 - mae: 169.296 - mean_q: 182.820\n",
      "\n",
      "Interval 569 (5680000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.1740\n",
      "200 episodes - episode_reward: 208.700 [50.000, 300.000] - loss: 338.180 - mae: 170.717 - mean_q: 184.065\n",
      "\n",
      "Interval 570 (5690000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.1290\n",
      "200 episodes - episode_reward: 206.450 [60.000, 330.000] - loss: 324.870 - mae: 169.017 - mean_q: 182.502\n",
      "\n",
      "Interval 571 (5700000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.4630\n",
      "200 episodes - episode_reward: 223.150 [100.000, 320.000] - loss: 338.924 - mae: 170.805 - mean_q: 184.881\n",
      "\n",
      "Interval 572 (5710000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.1350\n",
      "200 episodes - episode_reward: 206.750 [-10.000, 340.000] - loss: 332.596 - mae: 169.401 - mean_q: 182.850\n",
      "\n",
      "Interval 573 (5720000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.2170\n",
      "200 episodes - episode_reward: 210.850 [0.000, 290.000] - loss: 323.363 - mae: 167.716 - mean_q: 180.971\n",
      "\n",
      "Interval 574 (5730000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.1280\n",
      "200 episodes - episode_reward: 206.400 [-20.000, 320.000] - loss: 326.271 - mae: 166.610 - mean_q: 179.602\n",
      "\n",
      "Interval 575 (5740000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.4500: 0s - rew\n",
      "200 episodes - episode_reward: 222.500 [20.000, 340.000] - loss: 317.522 - mae: 168.009 - mean_q: 181.069\n",
      "\n",
      "Interval 576 (5750000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.2620\n",
      "200 episodes - episode_reward: 213.100 [-40.000, 340.000] - loss: 329.035 - mae: 169.288 - mean_q: 182.305\n",
      "\n",
      "Interval 577 (5760000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.9330\n",
      "200 episodes - episode_reward: 196.650 [-10.000, 310.000] - loss: 337.027 - mae: 172.813 - mean_q: 186.400\n",
      "\n",
      "Interval 578 (5770000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.9520\n",
      "200 episodes - episode_reward: 197.600 [-50.000, 330.000] - loss: 332.489 - mae: 168.521 - mean_q: 181.623\n",
      "\n",
      "Interval 579 (5780000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.2400\n",
      "200 episodes - episode_reward: 212.000 [60.000, 320.000] - loss: 328.538 - mae: 169.323 - mean_q: 182.547\n",
      "\n",
      "Interval 580 (5790000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.3450\n",
      "200 episodes - episode_reward: 217.250 [40.000, 320.000] - loss: 321.907 - mae: 167.090 - mean_q: 180.170\n",
      "\n",
      "Interval 581 (5800000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.4310\n",
      "200 episodes - episode_reward: 221.550 [60.000, 310.000] - loss: 319.492 - mae: 168.110 - mean_q: 181.505\n",
      "\n",
      "Interval 582 (5810000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.2300\n",
      "200 episodes - episode_reward: 211.500 [-20.000, 330.000] - loss: 329.768 - mae: 168.436 - mean_q: 181.498\n",
      "\n",
      "Interval 583 (5820000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.3220\n",
      "200 episodes - episode_reward: 216.100 [30.000, 310.000] - loss: 327.126 - mae: 167.893 - mean_q: 180.843\n",
      "\n",
      "Interval 584 (5830000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.9940\n",
      "200 episodes - episode_reward: 199.700 [0.000, 300.000] - loss: 333.592 - mae: 171.828 - mean_q: 185.251\n",
      "\n",
      "Interval 585 (5840000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.8740\n",
      "200 episodes - episode_reward: 193.700 [-40.000, 320.000] - loss: 336.687 - mae: 173.215 - mean_q: 186.649\n",
      "\n",
      "Interval 586 (5850000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.9170\n",
      "200 episodes - episode_reward: 195.850 [-20.000, 310.000] - loss: 331.513 - mae: 170.584 - mean_q: 183.569\n",
      "\n",
      "Interval 587 (5860000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.4040\n",
      "200 episodes - episode_reward: 170.200 [-20.000, 270.000] - loss: 329.200 - mae: 172.614 - mean_q: 185.797\n",
      "\n",
      "Interval 588 (5870000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.6180\n",
      "200 episodes - episode_reward: 180.900 [10.000, 300.000] - loss: 324.659 - mae: 168.687 - mean_q: 181.525\n",
      "\n",
      "Interval 589 (5880000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.1480\n",
      "200 episodes - episode_reward: 207.400 [-50.000, 300.000] - loss: 303.970 - mae: 162.401 - mean_q: 174.917\n",
      "\n",
      "Interval 590 (5890000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.0010\n",
      "200 episodes - episode_reward: 200.050 [10.000, 340.000] - loss: 313.817 - mae: 165.214 - mean_q: 178.040\n",
      "\n",
      "Interval 591 (5900000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.2290\n",
      "200 episodes - episode_reward: 211.450 [100.000, 300.000] - loss: 316.980 - mae: 166.485 - mean_q: 179.475\n",
      "\n",
      "Interval 592 (5910000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.0930: 0s - rewa\n",
      "200 episodes - episode_reward: 204.650 [-30.000, 300.000] - loss: 321.224 - mae: 167.415 - mean_q: 180.627\n",
      "\n",
      "Interval 593 (5920000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.0390\n",
      "200 episodes - episode_reward: 201.950 [30.000, 320.000] - loss: 331.443 - mae: 169.224 - mean_q: 182.746\n",
      "\n",
      "Interval 594 (5930000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.1050\n",
      "200 episodes - episode_reward: 205.250 [10.000, 310.000] - loss: 330.095 - mae: 169.408 - mean_q: 182.796\n",
      "\n",
      "Interval 595 (5940000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.2330\n",
      "200 episodes - episode_reward: 211.650 [50.000, 340.000] - loss: 329.222 - mae: 169.820 - mean_q: 183.339\n",
      "\n",
      "Interval 596 (5950000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7050\n",
      "200 episodes - episode_reward: 185.250 [-10.000, 300.000] - loss: 329.156 - mae: 169.102 - mean_q: 182.434\n",
      "\n",
      "Interval 597 (5960000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.4600\n",
      "200 episodes - episode_reward: 173.000 [-40.000, 300.000] - loss: 320.602 - mae: 168.004 - mean_q: 180.871\n",
      "\n",
      "Interval 598 (5970000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.9300\n",
      "200 episodes - episode_reward: 146.500 [-40.000, 260.000] - loss: 301.645 - mae: 162.649 - mean_q: 175.045\n",
      "\n",
      "Interval 599 (5980000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 2.6420\n",
      "200 episodes - episode_reward: 132.100 [-50.000, 250.000] - loss: 310.632 - mae: 164.742 - mean_q: 177.070\n",
      "\n",
      "Interval 600 (5990000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.2650\n",
      "200 episodes - episode_reward: 163.250 [-50.000, 290.000] - loss: 264.546 - mae: 152.372 - mean_q: 163.573\n",
      "\n",
      "Interval 601 (6000000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 2.7690\n",
      "200 episodes - episode_reward: 138.450 [-50.000, 270.000] - loss: 237.252 - mae: 141.259 - mean_q: 151.944\n",
      "\n",
      "Interval 602 (6010000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.2310\n",
      "200 episodes - episode_reward: 161.550 [-40.000, 280.000] - loss: 220.526 - mae: 138.000 - mean_q: 148.446\n",
      "\n",
      "Interval 603 (6020000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.5760\n",
      "200 episodes - episode_reward: 178.800 [0.000, 290.000] - loss: 230.310 - mae: 140.580 - mean_q: 151.527\n",
      "\n",
      "Interval 604 (6030000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.6140\n",
      "200 episodes - episode_reward: 180.700 [20.000, 280.000] - loss: 236.681 - mae: 144.150 - mean_q: 155.416\n",
      "\n",
      "Interval 605 (6040000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.4060\n",
      "200 episodes - episode_reward: 170.300 [-20.000, 300.000] - loss: 247.962 - mae: 145.612 - mean_q: 156.990\n",
      "\n",
      "Interval 606 (6050000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7440\n",
      "200 episodes - episode_reward: 187.200 [70.000, 300.000] - loss: 253.495 - mae: 147.722 - mean_q: 159.534\n",
      "\n",
      "Interval 607 (6060000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.9350\n",
      "200 episodes - episode_reward: 196.750 [-10.000, 350.000] - loss: 269.536 - mae: 154.004 - mean_q: 166.503\n",
      "\n",
      "Interval 608 (6070000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.0500\n",
      "200 episodes - episode_reward: 202.500 [-40.000, 290.000] - loss: 324.468 - mae: 167.101 - mean_q: 180.702\n",
      "\n",
      "Interval 609 (6080000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.2570\n",
      "200 episodes - episode_reward: 212.850 [20.000, 340.000] - loss: 336.561 - mae: 170.656 - mean_q: 184.107\n",
      "\n",
      "Interval 610 (6090000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.0710\n",
      "200 episodes - episode_reward: 203.550 [-50.000, 310.000] - loss: 344.557 - mae: 175.605 - mean_q: 189.248\n",
      "\n",
      "Interval 611 (6100000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.1190\n",
      "200 episodes - episode_reward: 205.950 [-40.000, 310.000] - loss: 330.910 - mae: 169.791 - mean_q: 183.008\n",
      "\n",
      "Interval 612 (6110000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.3510\n",
      "200 episodes - episode_reward: 217.550 [60.000, 360.000] - loss: 325.610 - mae: 167.497 - mean_q: 180.690\n",
      "\n",
      "Interval 613 (6120000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.1650\n",
      "200 episodes - episode_reward: 208.250 [-40.000, 300.000] - loss: 322.316 - mae: 166.781 - mean_q: 179.792\n",
      "\n",
      "Interval 614 (6130000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.9910\n",
      "200 episodes - episode_reward: 199.550 [-10.000, 300.000] - loss: 324.041 - mae: 168.947 - mean_q: 182.011\n",
      "\n",
      "Interval 615 (6140000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.2670\n",
      "200 episodes - episode_reward: 213.350 [30.000, 320.000] - loss: 325.026 - mae: 169.034 - mean_q: 182.417\n",
      "\n",
      "Interval 616 (6150000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.1850\n",
      "200 episodes - episode_reward: 209.250 [50.000, 310.000] - loss: 324.836 - mae: 168.061 - mean_q: 181.202\n",
      "\n",
      "Interval 617 (6160000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.0920\n",
      "200 episodes - episode_reward: 204.600 [-40.000, 330.000] - loss: 317.542 - mae: 167.246 - mean_q: 180.284\n",
      "\n",
      "Interval 618 (6170000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.1790\n",
      "200 episodes - episode_reward: 208.950 [10.000, 330.000] - loss: 320.189 - mae: 168.072 - mean_q: 180.876\n",
      "\n",
      "Interval 619 (6180000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7140\n",
      "200 episodes - episode_reward: 185.700 [-20.000, 290.000] - loss: 326.583 - mae: 170.195 - mean_q: 183.156\n",
      "\n",
      "Interval 620 (6190000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 1.8790\n",
      "200 episodes - episode_reward: 93.950 [-50.000, 250.000] - loss: 340.887 - mae: 174.351 - mean_q: 187.281\n",
      "\n",
      "Interval 621 (6200000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.6310\n",
      "200 episodes - episode_reward: 181.550 [-10.000, 280.000] - loss: 301.890 - mae: 161.694 - mean_q: 173.858\n",
      "\n",
      "Interval 622 (6210000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.8210\n",
      "200 episodes - episode_reward: 191.050 [60.000, 310.000] - loss: 294.416 - mae: 160.773 - mean_q: 173.258\n",
      "\n",
      "Interval 623 (6220000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.8240\n",
      "200 episodes - episode_reward: 191.200 [-30.000, 300.000] - loss: 288.792 - mae: 159.350 - mean_q: 171.781\n",
      "\n",
      "Interval 624 (6230000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7840\n",
      "200 episodes - episode_reward: 189.200 [-40.000, 320.000] - loss: 272.092 - mae: 153.324 - mean_q: 165.477\n",
      "\n",
      "Interval 625 (6240000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7080\n",
      "200 episodes - episode_reward: 185.400 [-10.000, 280.000] - loss: 273.054 - mae: 153.620 - mean_q: 165.683\n",
      "\n",
      "Interval 626 (6250000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.9500\n",
      "200 episodes - episode_reward: 197.500 [0.000, 320.000] - loss: 292.809 - mae: 159.227 - mean_q: 171.741\n",
      "\n",
      "Interval 627 (6260000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7290\n",
      "200 episodes - episode_reward: 186.450 [30.000, 300.000] - loss: 295.267 - mae: 160.277 - mean_q: 172.762\n",
      "\n",
      "Interval 628 (6270000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.8560\n",
      "200 episodes - episode_reward: 192.800 [-10.000, 310.000] - loss: 290.495 - mae: 156.687 - mean_q: 169.230\n",
      "\n",
      "Interval 629 (6280000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.2730\n",
      "200 episodes - episode_reward: 163.650 [-40.000, 280.000] - loss: 292.493 - mae: 159.706 - mean_q: 172.063\n",
      "\n",
      "Interval 630 (6290000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.1640\n",
      "200 episodes - episode_reward: 208.200 [-10.000, 300.000] - loss: 294.491 - mae: 159.705 - mean_q: 172.111\n",
      "\n",
      "Interval 631 (6300000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.2300\n",
      "200 episodes - episode_reward: 211.500 [0.000, 320.000] - loss: 307.209 - mae: 161.874 - mean_q: 174.631\n",
      "\n",
      "Interval 632 (6310000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 4.1130\n",
      "200 episodes - episode_reward: 205.650 [-30.000, 310.000] - loss: 308.797 - mae: 163.894 - mean_q: 176.515\n",
      "\n",
      "Interval 633 (6320000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.6010\n",
      "200 episodes - episode_reward: 180.050 [-10.000, 300.000] - loss: 312.780 - mae: 165.658 - mean_q: 178.207\n",
      "\n",
      "Interval 634 (6330000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7460\n",
      "200 episodes - episode_reward: 187.300 [-20.000, 280.000] - loss: 300.550 - mae: 162.964 - mean_q: 175.344\n",
      "\n",
      "Interval 635 (6340000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.9530\n",
      "200 episodes - episode_reward: 197.650 [-50.000, 280.000] - loss: 302.094 - mae: 161.125 - mean_q: 173.426\n",
      "\n",
      "Interval 636 (6350000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.8290\n",
      "200 episodes - episode_reward: 191.450 [0.000, 300.000] - loss: 294.574 - mae: 162.048 - mean_q: 174.443\n",
      "\n",
      "Interval 637 (6360000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 2.9180\n",
      "200 episodes - episode_reward: 145.900 [-50.000, 270.000] - loss: 301.966 - mae: 162.705 - mean_q: 175.116\n",
      "\n",
      "Interval 638 (6370000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7090\n",
      "200 episodes - episode_reward: 185.450 [10.000, 300.000] - loss: 297.616 - mae: 159.508 - mean_q: 171.785\n",
      "\n",
      "Interval 639 (6380000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7440\n",
      "200 episodes - episode_reward: 187.200 [-10.000, 280.000] - loss: 305.402 - mae: 161.709 - mean_q: 174.139\n",
      "\n",
      "Interval 640 (6390000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.5820\n",
      "200 episodes - episode_reward: 179.100 [50.000, 330.000] - loss: 300.285 - mae: 161.735 - mean_q: 174.198\n",
      "\n",
      "Interval 641 (6400000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.5880\n",
      "200 episodes - episode_reward: 179.400 [-40.000, 280.000] - loss: 287.114 - mae: 158.875 - mean_q: 171.334\n",
      "\n",
      "Interval 642 (6410000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.6580\n",
      "200 episodes - episode_reward: 182.900 [10.000, 270.000] - loss: 281.222 - mae: 157.210 - mean_q: 169.288\n",
      "\n",
      "Interval 643 (6420000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.5290\n",
      "200 episodes - episode_reward: 176.450 [-10.000, 300.000] - loss: 266.898 - mae: 151.352 - mean_q: 162.963\n",
      "\n",
      "Interval 644 (6430000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.5130\n",
      "200 episodes - episode_reward: 175.650 [-40.000, 270.000] - loss: 256.708 - mae: 149.354 - mean_q: 160.731\n",
      "\n",
      "Interval 645 (6440000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.3400\n",
      "200 episodes - episode_reward: 167.000 [-40.000, 300.000] - loss: 258.075 - mae: 148.541 - mean_q: 159.880\n",
      "\n",
      "Interval 646 (6450000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.4630\n",
      "200 episodes - episode_reward: 173.150 [-30.000, 280.000] - loss: 248.587 - mae: 146.180 - mean_q: 157.412\n",
      "\n",
      "Interval 647 (6460000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7660\n",
      "200 episodes - episode_reward: 188.300 [30.000, 310.000] - loss: 254.628 - mae: 149.501 - mean_q: 161.231\n",
      "\n",
      "Interval 648 (6470000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.6910\n",
      "200 episodes - episode_reward: 184.550 [-20.000, 290.000] - loss: 270.950 - mae: 152.928 - mean_q: 164.760\n",
      "\n",
      "Interval 649 (6480000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7020\n",
      "200 episodes - episode_reward: 185.100 [-30.000, 310.000] - loss: 273.963 - mae: 155.601 - mean_q: 167.809\n",
      "\n",
      "Interval 650 (6490000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.9070\n",
      "200 episodes - episode_reward: 195.350 [20.000, 320.000] - loss: 289.590 - mae: 158.165 - mean_q: 170.610\n",
      "\n",
      "Interval 651 (6500000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.9610\n",
      "200 episodes - episode_reward: 198.050 [20.000, 360.000] - loss: 292.510 - mae: 157.012 - mean_q: 169.348\n",
      "\n",
      "Interval 652 (6510000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.8770\n",
      "200 episodes - episode_reward: 193.850 [-40.000, 280.000] - loss: 290.715 - mae: 159.668 - mean_q: 172.239\n",
      "\n",
      "Interval 653 (6520000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7910\n",
      "200 episodes - episode_reward: 189.550 [-30.000, 280.000] - loss: 291.892 - mae: 159.122 - mean_q: 171.614\n",
      "\n",
      "Interval 654 (6530000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.4870\n",
      "200 episodes - episode_reward: 174.350 [-40.000, 270.000] - loss: 291.541 - mae: 159.917 - mean_q: 172.431\n",
      "\n",
      "Interval 655 (6540000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.5580\n",
      "200 episodes - episode_reward: 177.900 [-30.000, 260.000] - loss: 284.021 - mae: 155.890 - mean_q: 168.125\n",
      "\n",
      "Interval 656 (6550000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.5710\n",
      "200 episodes - episode_reward: 178.550 [0.000, 290.000] - loss: 276.517 - mae: 153.336 - mean_q: 165.619\n",
      "\n",
      "Interval 657 (6560000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.8330\n",
      "200 episodes - episode_reward: 191.650 [-30.000, 290.000] - loss: 269.594 - mae: 152.284 - mean_q: 164.514\n",
      "\n",
      "Interval 658 (6570000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.4390\n",
      "200 episodes - episode_reward: 171.950 [-20.000, 300.000] - loss: 271.224 - mae: 151.959 - mean_q: 164.149\n",
      "\n",
      "Interval 659 (6580000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.4290\n",
      "200 episodes - episode_reward: 171.450 [0.000, 300.000] - loss: 269.829 - mae: 151.198 - mean_q: 163.286\n",
      "\n",
      "Interval 660 (6590000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.5290\n",
      "200 episodes - episode_reward: 176.450 [30.000, 270.000] - loss: 265.958 - mae: 151.661 - mean_q: 163.562\n",
      "\n",
      "Interval 661 (6600000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.4180\n",
      "200 episodes - episode_reward: 170.900 [-10.000, 280.000] - loss: 257.164 - mae: 149.114 - mean_q: 160.614\n",
      "\n",
      "Interval 662 (6610000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.5070\n",
      "200 episodes - episode_reward: 175.350 [-50.000, 280.000] - loss: 246.986 - mae: 147.532 - mean_q: 158.663\n",
      "\n",
      "Interval 663 (6620000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.5230\n",
      "200 episodes - episode_reward: 176.150 [-20.000, 290.000] - loss: 242.755 - mae: 145.230 - mean_q: 156.302\n",
      "\n",
      "Interval 664 (6630000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.5410\n",
      "200 episodes - episode_reward: 177.050 [0.000, 290.000] - loss: 244.871 - mae: 144.916 - mean_q: 155.851\n",
      "\n",
      "Interval 665 (6640000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.4730\n",
      "200 episodes - episode_reward: 173.650 [-40.000, 270.000] - loss: 240.464 - mae: 145.081 - mean_q: 155.859\n",
      "\n",
      "Interval 666 (6650000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.5700\n",
      "200 episodes - episode_reward: 178.500 [-30.000, 290.000] - loss: 237.424 - mae: 143.752 - mean_q: 154.659\n",
      "\n",
      "Interval 667 (6660000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.1430\n",
      "200 episodes - episode_reward: 157.150 [-40.000, 270.000] - loss: 242.750 - mae: 146.216 - mean_q: 157.111\n",
      "\n",
      "Interval 668 (6670000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.5000\n",
      "200 episodes - episode_reward: 175.000 [-10.000, 250.000] - loss: 227.302 - mae: 140.806 - mean_q: 151.131\n",
      "\n",
      "Interval 669 (6680000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.6930\n",
      "200 episodes - episode_reward: 184.650 [-10.000, 290.000] - loss: 225.221 - mae: 139.439 - mean_q: 149.866\n",
      "\n",
      "Interval 670 (6690000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.6140\n",
      "200 episodes - episode_reward: 180.700 [10.000, 310.000] - loss: 225.036 - mae: 139.473 - mean_q: 149.675\n",
      "\n",
      "Interval 671 (6700000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.7780\n",
      "200 episodes - episode_reward: 188.900 [90.000, 280.000] - loss: 234.107 - mae: 143.309 - mean_q: 153.840\n",
      "\n",
      "Interval 672 (6710000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.5600\n",
      "200 episodes - episode_reward: 178.000 [-40.000, 310.000] - loss: 237.807 - mae: 143.980 - mean_q: 154.675\n",
      "\n",
      "Interval 673 (6720000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.5730\n",
      "200 episodes - episode_reward: 178.650 [-20.000, 270.000] - loss: 243.702 - mae: 144.650 - mean_q: 155.824\n",
      "\n",
      "Interval 674 (6730000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.6580\n",
      "200 episodes - episode_reward: 182.900 [10.000, 290.000] - loss: 249.455 - mae: 146.049 - mean_q: 157.141\n",
      "\n",
      "Interval 675 (6740000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.3590\n",
      "200 episodes - episode_reward: 167.950 [-30.000, 280.000] - loss: 244.414 - mae: 144.946 - mean_q: 156.193\n",
      "\n",
      "Interval 676 (6750000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.6580\n",
      "200 episodes - episode_reward: 182.900 [30.000, 300.000] - loss: 249.113 - mae: 145.750 - mean_q: 157.123\n",
      "\n",
      "Interval 677 (6760000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7140\n",
      "200 episodes - episode_reward: 185.700 [10.000, 270.000] - loss: 245.921 - mae: 145.124 - mean_q: 156.477\n",
      "\n",
      "Interval 678 (6770000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7330\n",
      "200 episodes - episode_reward: 186.650 [20.000, 290.000] - loss: 253.898 - mae: 147.307 - mean_q: 158.811\n",
      "\n",
      "Interval 679 (6780000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7780\n",
      "200 episodes - episode_reward: 188.900 [10.000, 310.000] - loss: 254.131 - mae: 148.953 - mean_q: 160.599\n",
      "\n",
      "Interval 680 (6790000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.6060\n",
      "200 episodes - episode_reward: 180.300 [-10.000, 290.000] - loss: 265.309 - mae: 151.782 - mean_q: 163.506\n",
      "\n",
      "Interval 681 (6800000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.5280\n",
      "200 episodes - episode_reward: 176.400 [-20.000, 270.000] - loss: 265.262 - mae: 151.430 - mean_q: 163.165\n",
      "\n",
      "Interval 682 (6810000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.5580\n",
      "200 episodes - episode_reward: 177.900 [-40.000, 320.000] - loss: 261.815 - mae: 149.902 - mean_q: 161.389\n",
      "\n",
      "Interval 683 (6820000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.6100\n",
      "200 episodes - episode_reward: 180.500 [30.000, 290.000] - loss: 268.025 - mae: 152.546 - mean_q: 164.338\n",
      "\n",
      "Interval 684 (6830000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.5850\n",
      "200 episodes - episode_reward: 179.250 [-30.000, 320.000] - loss: 270.544 - mae: 153.726 - mean_q: 165.435\n",
      "\n",
      "Interval 685 (6840000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.3150\n",
      "200 episodes - episode_reward: 165.750 [-50.000, 280.000] - loss: 273.364 - mae: 153.163 - mean_q: 164.757\n",
      "\n",
      "Interval 686 (6850000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.5320\n",
      "200 episodes - episode_reward: 176.600 [10.000, 280.000] - loss: 256.816 - mae: 151.113 - mean_q: 162.486\n",
      "\n",
      "Interval 687 (6860000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.5180\n",
      "200 episodes - episode_reward: 175.900 [-30.000, 300.000] - loss: 263.823 - mae: 152.624 - mean_q: 164.118\n",
      "\n",
      "Interval 688 (6870000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.0930\n",
      "200 episodes - episode_reward: 154.650 [-40.000, 300.000] - loss: 249.136 - mae: 149.116 - mean_q: 160.374\n",
      "\n",
      "Interval 689 (6880000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.5540\n",
      "200 episodes - episode_reward: 177.700 [0.000, 270.000] - loss: 252.808 - mae: 146.897 - mean_q: 157.999\n",
      "\n",
      "Interval 690 (6890000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.6370\n",
      "200 episodes - episode_reward: 181.850 [-10.000, 290.000] - loss: 244.668 - mae: 146.834 - mean_q: 157.920\n",
      "\n",
      "Interval 691 (6900000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.4740\n",
      "200 episodes - episode_reward: 173.700 [20.000, 270.000] - loss: 262.936 - mae: 150.363 - mean_q: 161.835\n",
      "\n",
      "Interval 692 (6910000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7570\n",
      "200 episodes - episode_reward: 187.850 [0.000, 280.000] - loss: 262.477 - mae: 150.230 - mean_q: 161.723\n",
      "\n",
      "Interval 693 (6920000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.8590\n",
      "200 episodes - episode_reward: 192.950 [20.000, 290.000] - loss: 271.224 - mae: 153.699 - mean_q: 165.473\n",
      "\n",
      "Interval 694 (6930000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7210\n",
      "200 episodes - episode_reward: 186.050 [-40.000, 280.000] - loss: 271.988 - mae: 151.341 - mean_q: 163.005\n",
      "\n",
      "Interval 695 (6940000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7800\n",
      "200 episodes - episode_reward: 189.000 [-30.000, 290.000] - loss: 263.997 - mae: 149.367 - mean_q: 160.865\n",
      "\n",
      "Interval 696 (6950000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.5340\n",
      "200 episodes - episode_reward: 176.700 [-10.000, 290.000] - loss: 258.914 - mae: 151.533 - mean_q: 162.853\n",
      "\n",
      "Interval 697 (6960000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.5150\n",
      "200 episodes - episode_reward: 175.750 [30.000, 280.000] - loss: 266.263 - mae: 153.023 - mean_q: 164.369\n",
      "\n",
      "Interval 698 (6970000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.1830\n",
      "200 episodes - episode_reward: 159.150 [-30.000, 250.000] - loss: 257.234 - mae: 149.504 - mean_q: 160.539\n",
      "\n",
      "Interval 699 (6980000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.1170\n",
      "200 episodes - episode_reward: 155.850 [-40.000, 250.000] - loss: 251.476 - mae: 146.560 - mean_q: 157.403\n",
      "\n",
      "Interval 700 (6990000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.1100\n",
      "done, took 36742.068 seconds\n"
     ]
    }
   ],
   "source": [
    "history = dqn.fit(env, nb_steps=7*MILLION, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "429ea2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = history.history\n",
    "data['episode_reward'] = [float(v) for v in data['episode_reward']]\n",
    "data['nb_episode_steps'] = [int(v) for v in data['nb_episode_steps']]\n",
    "data['nb_steps'] = [int(v) for v in data['nb_steps']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94e1373",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "523042d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('agents/{}'.format(name))  # If the directory does not exist we cannot write the file\n",
    "with open(get_training_path(name), 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d1d24c",
   "metadata": {},
   "source": [
    "Save agent to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1759baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights(get_agent_path(name), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910b175d",
   "metadata": {},
   "source": [
    "## 4. Reloading training from Memory ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb14792",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
