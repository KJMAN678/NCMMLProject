{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4221e1e3",
   "metadata": {},
   "source": [
    "# Training a basic setting with a Deep Q Network (DQN) #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d628c23d",
   "metadata": {},
   "source": [
    "Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b9fe8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "rc('text', usetex=True)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "761fd1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f09c161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory  # For experience replay!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c9f747a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_environment_ncml import *\n",
    "from learning import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a0674c",
   "metadata": {},
   "source": [
    "Useful numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cf11480",
   "metadata": {},
   "outputs": [],
   "source": [
    "MILLION = 1000000\n",
    "HTHOUSAND = 100000\n",
    "THOUSAND = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfb0b18",
   "metadata": {},
   "source": [
    "## 1. Create environment ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b9a3f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/copernico/opt/anaconda3/envs/RL/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = GridworldMultiAgentv25()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "708f7943",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eca1527b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 25)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states, actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0343c79a",
   "metadata": {},
   "source": [
    "## 2. Create a Deep Learning Model with Keras ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "941ff796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/copernico/opt/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "model = build_model(states, actions, [32, 16], ['relu', 'relu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f136bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                352       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 25)                425       \n",
      "=================================================================\n",
      "Total params: 1,305\n",
      "Trainable params: 1,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0b28d2",
   "metadata": {},
   "source": [
    "## 3. Build Agent with Keras-RL ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a418e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = build_agent(model, actions, 0.01, EpsGreedyQPolicy(), 50000)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "# dqn.compile(Adam(lr=1e-2), metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92c1507e",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'dqn25_5b5_3236_adam_lr0.001_tmu0.01_ml50K_ns5M_eps0.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "073ef900",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: -0.6472\n",
      "200 episodes - episode_reward: -32.360 [-58.000, 26.000] - loss: 1.490 - mae: 2.423 - mean_q: -1.613\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 71s 7ms/step - reward: -0.1434\n",
      "200 episodes - episode_reward: -7.170 [-54.000, 100.000] - loss: 2.356 - mae: 3.513 - mean_q: 1.236\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 78s 8ms/step - reward: 0.1508\n",
      "200 episodes - episode_reward: 7.540 [-50.000, 112.000] - loss: 5.485 - mae: 13.538 - mean_q: 15.368\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 75s 8ms/step - reward: 0.6254\n",
      "200 episodes - episode_reward: 31.270 [-60.000, 132.000] - loss: 10.715 - mae: 24.133 - mean_q: 26.708\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 72s 7ms/step - reward: 0.7732\n",
      "200 episodes - episode_reward: 38.660 [-50.000, 128.000] - loss: 18.953 - mae: 35.181 - mean_q: 38.591\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.0150\n",
      "200 episodes - episode_reward: 50.750 [-50.000, 178.000] - loss: 27.165 - mae: 44.527 - mean_q: 48.613\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.2436\n",
      "200 episodes - episode_reward: 62.180 [-40.000, 158.000] - loss: 38.242 - mae: 54.374 - mean_q: 59.330\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 1.2048\n",
      "200 episodes - episode_reward: 60.240 [-52.000, 166.000] - loss: 49.107 - mae: 61.791 - mean_q: 67.447\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.2360\n",
      "200 episodes - episode_reward: 61.800 [-58.000, 150.000] - loss: 58.552 - mae: 68.541 - mean_q: 74.838\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.0456\n",
      "200 episodes - episode_reward: 52.280 [-46.000, 148.000] - loss: 62.878 - mae: 71.084 - mean_q: 77.454\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 1.0594\n",
      "200 episodes - episode_reward: 52.970 [-50.000, 162.000] - loss: 63.728 - mae: 70.508 - mean_q: 76.874\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.0854\n",
      "200 episodes - episode_reward: 54.270 [-50.000, 148.000] - loss: 61.866 - mae: 70.277 - mean_q: 76.700\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.1972\n",
      "200 episodes - episode_reward: 59.860 [-50.000, 140.000] - loss: 62.218 - mae: 69.801 - mean_q: 76.130\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.2034\n",
      "200 episodes - episode_reward: 60.170 [-64.000, 160.000] - loss: 62.260 - mae: 70.270 - mean_q: 76.650\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 1.1728\n",
      "200 episodes - episode_reward: 58.640 [-50.000, 162.000] - loss: 65.353 - mae: 71.961 - mean_q: 78.523\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 1.4184\n",
      "200 episodes - episode_reward: 70.920 [-50.000, 172.000] - loss: 66.621 - mae: 71.949 - mean_q: 78.589\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 1.3608\n",
      "200 episodes - episode_reward: 68.040 [-50.000, 204.000] - loss: 69.927 - mae: 75.254 - mean_q: 82.139\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.3628\n",
      "200 episodes - episode_reward: 68.140 [-50.000, 162.000] - loss: 74.496 - mae: 77.756 - mean_q: 85.046\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.4870\n",
      "200 episodes - episode_reward: 74.350 [-48.000, 168.000] - loss: 78.592 - mae: 80.232 - mean_q: 87.702\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 1.2904\n",
      "200 episodes - episode_reward: 64.520 [-46.000, 174.000] - loss: 80.023 - mae: 80.620 - mean_q: 88.062\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 1.4176\n",
      "200 episodes - episode_reward: 70.880 [-50.000, 182.000] - loss: 81.697 - mae: 81.359 - mean_q: 88.851\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 1.3984\n",
      "200 episodes - episode_reward: 69.920 [-50.000, 164.000] - loss: 79.442 - mae: 79.720 - mean_q: 87.055\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 1.4864\n",
      "200 episodes - episode_reward: 74.320 [-54.000, 170.000] - loss: 80.277 - mae: 80.794 - mean_q: 88.238\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.6042\n",
      "200 episodes - episode_reward: 80.210 [-56.000, 194.000] - loss: 84.766 - mae: 83.341 - mean_q: 91.035\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 1.5182\n",
      "200 episodes - episode_reward: 75.910 [-52.000, 174.000] - loss: 88.522 - mae: 85.212 - mean_q: 93.035\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.5744\n",
      "200 episodes - episode_reward: 78.720 [-50.000, 194.000] - loss: 92.485 - mae: 86.577 - mean_q: 94.444\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.5680\n",
      "200 episodes - episode_reward: 78.400 [-48.000, 172.000] - loss: 92.132 - mae: 87.130 - mean_q: 95.030\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.5636\n",
      "200 episodes - episode_reward: 78.180 [-40.000, 190.000] - loss: 95.928 - mae: 88.591 - mean_q: 96.570\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.5838\n",
      "200 episodes - episode_reward: 79.190 [-50.000, 210.000] - loss: 95.749 - mae: 88.485 - mean_q: 96.460\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.4110\n",
      "200 episodes - episode_reward: 70.550 [-50.000, 162.000] - loss: 96.620 - mae: 88.738 - mean_q: 96.786\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.4230\n",
      "200 episodes - episode_reward: 71.150 [-52.000, 166.000] - loss: 89.071 - mae: 85.804 - mean_q: 93.639\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.6204\n",
      "200 episodes - episode_reward: 81.020 [-46.000, 174.000] - loss: 87.208 - mae: 84.716 - mean_q: 92.573\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.5754\n",
      "200 episodes - episode_reward: 78.770 [-58.000, 240.000] - loss: 86.629 - mae: 83.562 - mean_q: 91.381\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.4782\n",
      "200 episodes - episode_reward: 73.910 [-40.000, 180.000] - loss: 86.510 - mae: 84.366 - mean_q: 92.158\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.6294\n",
      "200 episodes - episode_reward: 81.470 [-40.000, 196.000] - loss: 87.615 - mae: 85.369 - mean_q: 93.240\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.5298\n",
      "200 episodes - episode_reward: 76.490 [-50.000, 186.000] - loss: 91.182 - mae: 86.636 - mean_q: 94.674\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.5556\n",
      "200 episodes - episode_reward: 77.780 [-50.000, 186.000] - loss: 90.693 - mae: 86.340 - mean_q: 94.300\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.6420\n",
      "200 episodes - episode_reward: 82.100 [-64.000, 190.000] - loss: 89.167 - mae: 84.348 - mean_q: 92.138\n",
      "\n",
      "Interval 39 (380000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.4556\n",
      "200 episodes - episode_reward: 72.780 [-52.000, 174.000] - loss: 91.400 - mae: 86.022 - mean_q: 93.887\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.6252\n",
      "200 episodes - episode_reward: 81.260 [-36.000, 184.000] - loss: 90.340 - mae: 86.119 - mean_q: 93.914\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.5870\n",
      "200 episodes - episode_reward: 79.350 [-50.000, 188.000] - loss: 94.505 - mae: 88.172 - mean_q: 96.125\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.5538\n",
      "200 episodes - episode_reward: 77.690 [-50.000, 148.000] - loss: 92.616 - mae: 88.335 - mean_q: 96.202\n",
      "\n",
      "Interval 43 (420000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.4294\n",
      "200 episodes - episode_reward: 71.470 [-42.000, 170.000] - loss: 92.930 - mae: 87.130 - mean_q: 95.052\n",
      "\n",
      "Interval 44 (430000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.6846\n",
      "200 episodes - episode_reward: 84.230 [-16.000, 190.000] - loss: 93.140 - mae: 86.983 - mean_q: 94.900\n",
      "\n",
      "Interval 45 (440000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.6934\n",
      "200 episodes - episode_reward: 84.670 [-40.000, 192.000] - loss: 93.259 - mae: 87.360 - mean_q: 95.347\n",
      "\n",
      "Interval 46 (450000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.4966\n",
      "200 episodes - episode_reward: 74.830 [-50.000, 162.000] - loss: 94.128 - mae: 87.409 - mean_q: 95.324\n",
      "\n",
      "Interval 47 (460000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.5064\n",
      "200 episodes - episode_reward: 75.320 [-40.000, 166.000] - loss: 92.838 - mae: 86.958 - mean_q: 94.946\n",
      "\n",
      "Interval 48 (470000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.5882\n",
      "200 episodes - episode_reward: 79.410 [-44.000, 184.000] - loss: 90.409 - mae: 85.171 - mean_q: 93.003\n",
      "\n",
      "Interval 49 (480000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.6486\n",
      "200 episodes - episode_reward: 82.430 [-30.000, 222.000] - loss: 87.424 - mae: 84.163 - mean_q: 91.806\n",
      "\n",
      "Interval 50 (490000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.7516\n",
      "200 episodes - episode_reward: 87.580 [-20.000, 202.000] - loss: 85.918 - mae: 83.282 - mean_q: 90.867\n",
      "\n",
      "Interval 51 (500000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.6972\n",
      "200 episodes - episode_reward: 84.860 [-40.000, 222.000] - loss: 87.267 - mae: 83.632 - mean_q: 91.354\n",
      "\n",
      "Interval 52 (510000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.5762\n",
      "200 episodes - episode_reward: 78.810 [-50.000, 210.000] - loss: 85.901 - mae: 83.127 - mean_q: 90.758\n",
      "\n",
      "Interval 53 (520000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.6964\n",
      "200 episodes - episode_reward: 84.820 [-24.000, 202.000] - loss: 88.539 - mae: 84.356 - mean_q: 92.102\n",
      "\n",
      "Interval 54 (530000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.6928\n",
      "200 episodes - episode_reward: 84.640 [-52.000, 192.000] - loss: 88.088 - mae: 83.983 - mean_q: 91.676\n",
      "\n",
      "Interval 55 (540000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.5610\n",
      "200 episodes - episode_reward: 78.050 [-48.000, 172.000] - loss: 90.511 - mae: 86.399 - mean_q: 94.167\n",
      "\n",
      "Interval 56 (550000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.7218\n",
      "200 episodes - episode_reward: 86.090 [-20.000, 192.000] - loss: 91.944 - mae: 86.062 - mean_q: 93.953\n",
      "\n",
      "Interval 57 (560000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.8318\n",
      "200 episodes - episode_reward: 91.590 [-58.000, 208.000] - loss: 91.200 - mae: 86.961 - mean_q: 94.816\n",
      "\n",
      "Interval 58 (570000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 1.6718\n",
      "200 episodes - episode_reward: 83.590 [-30.000, 176.000] - loss: 95.293 - mae: 87.663 - mean_q: 95.509\n",
      "\n",
      "Interval 59 (580000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 1.6686\n",
      "200 episodes - episode_reward: 83.430 [-50.000, 206.000] - loss: 94.771 - mae: 88.188 - mean_q: 96.081\n",
      "\n",
      "Interval 60 (590000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.6896\n",
      "200 episodes - episode_reward: 84.480 [-50.000, 174.000] - loss: 94.329 - mae: 88.255 - mean_q: 96.113\n",
      "\n",
      "Interval 61 (600000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.5898\n",
      "200 episodes - episode_reward: 79.490 [-50.000, 192.000] - loss: 93.809 - mae: 87.210 - mean_q: 94.985\n",
      "\n",
      "Interval 62 (610000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.6284\n",
      "200 episodes - episode_reward: 81.420 [-50.000, 172.000] - loss: 90.893 - mae: 85.695 - mean_q: 93.418\n",
      "\n",
      "Interval 63 (620000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.6612\n",
      "200 episodes - episode_reward: 83.060 [-50.000, 182.000] - loss: 88.611 - mae: 84.775 - mean_q: 92.339\n",
      "\n",
      "Interval 64 (630000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 1.6220\n",
      "200 episodes - episode_reward: 81.100 [-46.000, 194.000] - loss: 87.591 - mae: 85.101 - mean_q: 92.517\n",
      "\n",
      "Interval 65 (640000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.5770\n",
      "200 episodes - episode_reward: 78.850 [-48.000, 188.000] - loss: 89.927 - mae: 84.882 - mean_q: 92.337\n",
      "\n",
      "Interval 66 (650000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.6300\n",
      "200 episodes - episode_reward: 81.500 [-52.000, 174.000] - loss: 83.907 - mae: 82.302 - mean_q: 89.615\n",
      "\n",
      "Interval 67 (660000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.6808\n",
      "200 episodes - episode_reward: 84.040 [-20.000, 184.000] - loss: 89.384 - mae: 84.690 - mean_q: 92.150\n",
      "\n",
      "Interval 68 (670000 steps performed)\n",
      "10000/10000 [==============================] - 69s 7ms/step - reward: 1.4646\n",
      "200 episodes - episode_reward: 73.230 [-42.000, 180.000] - loss: 87.550 - mae: 84.403 - mean_q: 91.980\n",
      "\n",
      "Interval 69 (680000 steps performed)\n",
      "10000/10000 [==============================] - 60s 6ms/step - reward: 1.6296\n",
      "200 episodes - episode_reward: 81.480 [-40.000, 186.000] - loss: 88.480 - mae: 84.625 - mean_q: 92.279\n",
      "\n",
      "Interval 70 (690000 steps performed)\n",
      "10000/10000 [==============================] - 75s 8ms/step - reward: 1.8226\n",
      "200 episodes - episode_reward: 91.130 [-40.000, 196.000] - loss: 89.817 - mae: 84.863 - mean_q: 92.360\n",
      "\n",
      "Interval 71 (700000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 1.7816\n",
      "200 episodes - episode_reward: 89.080 [-30.000, 170.000] - loss: 90.884 - mae: 86.230 - mean_q: 93.784\n",
      "\n",
      "Interval 72 (710000 steps performed)\n",
      "10000/10000 [==============================] - 84s 8ms/step - reward: 1.6500\n",
      "200 episodes - episode_reward: 82.500 [-36.000, 174.000] - loss: 89.465 - mae: 84.722 - mean_q: 92.062\n",
      "\n",
      "Interval 73 (720000 steps performed)\n",
      "10000/10000 [==============================] - 74s 7ms/step - reward: 1.4068\n",
      "200 episodes - episode_reward: 70.340 [-54.000, 178.000] - loss: 89.025 - mae: 84.652 - mean_q: 92.057\n",
      "\n",
      "Interval 74 (730000 steps performed)\n",
      "10000/10000 [==============================] - 67s 7ms/step - reward: 1.4834\n",
      "200 episodes - episode_reward: 74.170 [-50.000, 180.000] - loss: 90.759 - mae: 85.686 - mean_q: 93.251\n",
      "\n",
      "Interval 75 (740000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: 1.6744\n",
      "200 episodes - episode_reward: 83.720 [-64.000, 194.000] - loss: 88.635 - mae: 85.001 - mean_q: 92.484\n",
      "\n",
      "Interval 76 (750000 steps performed)\n",
      "10000/10000 [==============================] - 72s 7ms/step - reward: 1.6508\n",
      "200 episodes - episode_reward: 82.540 [-40.000, 178.000] - loss: 88.482 - mae: 85.177 - mean_q: 92.932\n",
      "\n",
      "Interval 77 (760000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 67s 7ms/step - reward: 1.6376\n",
      "200 episodes - episode_reward: 81.880 [-54.000, 176.000] - loss: 91.268 - mae: 86.514 - mean_q: 94.350\n",
      "\n",
      "Interval 78 (770000 steps performed)\n",
      "10000/10000 [==============================] - 67s 7ms/step - reward: 1.5720\n",
      "200 episodes - episode_reward: 78.600 [-50.000, 180.000] - loss: 91.824 - mae: 86.491 - mean_q: 94.227\n",
      "\n",
      "Interval 79 (780000 steps performed)\n",
      "10000/10000 [==============================] - 66s 7ms/step - reward: 1.6360\n",
      "200 episodes - episode_reward: 81.800 [-34.000, 184.000] - loss: 90.039 - mae: 85.583 - mean_q: 93.144\n",
      "\n",
      "Interval 80 (790000 steps performed)\n",
      "10000/10000 [==============================] - 65s 6ms/step - reward: 1.4014\n",
      "200 episodes - episode_reward: 70.070 [-54.000, 178.000] - loss: 90.460 - mae: 85.775 - mean_q: 93.308\n",
      "\n",
      "Interval 81 (800000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 1.5922\n",
      "200 episodes - episode_reward: 79.610 [-40.000, 178.000] - loss: 89.023 - mae: 84.609 - mean_q: 92.175\n",
      "\n",
      "Interval 82 (810000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 1.7976\n",
      "200 episodes - episode_reward: 89.880 [-24.000, 198.000] - loss: 88.076 - mae: 83.844 - mean_q: 91.327\n",
      "\n",
      "Interval 83 (820000 steps performed)\n",
      "10000/10000 [==============================] - 61s 6ms/step - reward: 1.8112\n",
      "200 episodes - episode_reward: 90.560 [-40.000, 198.000] - loss: 92.905 - mae: 86.509 - mean_q: 94.323\n",
      "\n",
      "Interval 84 (830000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 1.7604\n",
      "200 episodes - episode_reward: 88.020 [-26.000, 194.000] - loss: 99.506 - mae: 89.197 - mean_q: 97.395\n",
      "\n",
      "Interval 85 (840000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 1.8828\n",
      "200 episodes - episode_reward: 94.140 [-30.000, 200.000] - loss: 103.339 - mae: 91.683 - mean_q: 99.970\n",
      "\n",
      "Interval 86 (850000 steps performed)\n",
      "10000/10000 [==============================] - 71s 7ms/step - reward: 1.6346\n",
      "200 episodes - episode_reward: 81.730 [-58.000, 210.000] - loss: 103.137 - mae: 91.331 - mean_q: 99.584\n",
      "\n",
      "Interval 87 (860000 steps performed)\n",
      "10000/10000 [==============================] - 61s 6ms/step - reward: 1.5902\n",
      "200 episodes - episode_reward: 79.510 [-50.000, 210.000] - loss: 103.389 - mae: 90.872 - mean_q: 99.185\n",
      "\n",
      "Interval 88 (870000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.6460\n",
      "200 episodes - episode_reward: 82.300 [-50.000, 190.000] - loss: 100.311 - mae: 90.120 - mean_q: 98.274\n",
      "\n",
      "Interval 89 (880000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.6282\n",
      "200 episodes - episode_reward: 81.410 [-40.000, 202.000] - loss: 97.290 - mae: 88.616 - mean_q: 96.464\n",
      "\n",
      "Interval 90 (890000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 1.6912\n",
      "200 episodes - episode_reward: 84.560 [-2.000, 182.000] - loss: 92.797 - mae: 87.216 - mean_q: 94.929\n",
      "\n",
      "Interval 91 (900000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 1.5788\n",
      "200 episodes - episode_reward: 78.940 [-50.000, 190.000] - loss: 92.798 - mae: 87.272 - mean_q: 94.754\n",
      "\n",
      "Interval 92 (910000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.6056\n",
      "200 episodes - episode_reward: 80.280 [-50.000, 156.000] - loss: 87.905 - mae: 83.905 - mean_q: 91.278\n",
      "\n",
      "Interval 93 (920000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 1.7080\n",
      "200 episodes - episode_reward: 85.400 [-50.000, 182.000] - loss: 88.818 - mae: 85.301 - mean_q: 92.802\n",
      "\n",
      "Interval 94 (930000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 1.6962\n",
      "200 episodes - episode_reward: 84.810 [-50.000, 190.000] - loss: 91.592 - mae: 86.253 - mean_q: 93.762\n",
      "\n",
      "Interval 95 (940000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 1.7710\n",
      "200 episodes - episode_reward: 88.550 [-50.000, 170.000] - loss: 91.188 - mae: 85.418 - mean_q: 93.095\n",
      "\n",
      "Interval 96 (950000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 1.6928\n",
      "200 episodes - episode_reward: 84.640 [-50.000, 202.000] - loss: 93.450 - mae: 86.991 - mean_q: 94.851\n",
      "\n",
      "Interval 97 (960000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.8900\n",
      "200 episodes - episode_reward: 94.500 [-22.000, 196.000] - loss: 93.579 - mae: 87.589 - mean_q: 95.260\n",
      "\n",
      "Interval 98 (970000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 1.7006\n",
      "200 episodes - episode_reward: 85.030 [-14.000, 194.000] - loss: 92.694 - mae: 87.057 - mean_q: 94.463\n",
      "\n",
      "Interval 99 (980000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 1.4896\n",
      "200 episodes - episode_reward: 74.480 [-46.000, 188.000] - loss: 91.847 - mae: 86.280 - mean_q: 93.545\n",
      "\n",
      "Interval 100 (990000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.7296\n",
      "200 episodes - episode_reward: 86.480 [-40.000, 188.000] - loss: 86.347 - mae: 83.104 - mean_q: 90.202\n",
      "\n",
      "Interval 101 (1000000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.7004\n",
      "200 episodes - episode_reward: 85.020 [-50.000, 206.000] - loss: 87.336 - mae: 83.758 - mean_q: 91.055\n",
      "\n",
      "Interval 102 (1010000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 1.8632\n",
      "200 episodes - episode_reward: 93.160 [-10.000, 188.000] - loss: 88.470 - mae: 83.953 - mean_q: 91.300\n",
      "\n",
      "Interval 103 (1020000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 1.8186\n",
      "200 episodes - episode_reward: 90.930 [-40.000, 176.000] - loss: 92.200 - mae: 87.128 - mean_q: 94.765\n",
      "\n",
      "Interval 104 (1030000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.7240\n",
      "200 episodes - episode_reward: 86.200 [-30.000, 184.000] - loss: 96.344 - mae: 88.201 - mean_q: 95.911\n",
      "\n",
      "Interval 105 (1040000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 1.7634\n",
      "200 episodes - episode_reward: 88.170 [-50.000, 200.000] - loss: 96.505 - mae: 88.551 - mean_q: 96.277\n",
      "\n",
      "Interval 106 (1050000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.7864\n",
      "200 episodes - episode_reward: 89.320 [-30.000, 190.000] - loss: 100.831 - mae: 89.638 - mean_q: 97.290\n",
      "\n",
      "Interval 107 (1060000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 1.6666\n",
      "200 episodes - episode_reward: 83.330 [-50.000, 176.000] - loss: 96.238 - mae: 88.403 - mean_q: 95.912\n",
      "\n",
      "Interval 108 (1070000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 1.8514\n",
      "200 episodes - episode_reward: 92.570 [-32.000, 208.000] - loss: 96.869 - mae: 89.099 - mean_q: 96.851\n",
      "\n",
      "Interval 109 (1080000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 1.7544\n",
      "200 episodes - episode_reward: 87.720 [-40.000, 196.000] - loss: 99.775 - mae: 89.781 - mean_q: 97.611\n",
      "\n",
      "Interval 110 (1090000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 1.8710\n",
      "200 episodes - episode_reward: 93.550 [-22.000, 232.000] - loss: 95.067 - mae: 87.718 - mean_q: 95.319\n",
      "\n",
      "Interval 111 (1100000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 1.7640\n",
      "200 episodes - episode_reward: 88.200 [-40.000, 198.000] - loss: 95.327 - mae: 87.324 - mean_q: 94.742\n",
      "\n",
      "Interval 112 (1110000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.0022\n",
      "200 episodes - episode_reward: 100.110 [-50.000, 208.000] - loss: 93.563 - mae: 87.113 - mean_q: 94.696\n",
      "\n",
      "Interval 113 (1120000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.8230\n",
      "200 episodes - episode_reward: 91.150 [-18.000, 186.000] - loss: 97.128 - mae: 88.297 - mean_q: 95.645\n",
      "\n",
      "Interval 114 (1130000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.8260\n",
      "200 episodes - episode_reward: 91.300 [-28.000, 190.000] - loss: 95.136 - mae: 88.912 - mean_q: 96.250\n",
      "\n",
      "Interval 115 (1140000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 68s 7ms/step - reward: 1.7302\n",
      "200 episodes - episode_reward: 86.510 [-36.000, 202.000] - loss: 95.987 - mae: 88.125 - mean_q: 95.484\n",
      "\n",
      "Interval 116 (1150000 steps performed)\n",
      "10000/10000 [==============================] - 65s 6ms/step - reward: 1.8530\n",
      "200 episodes - episode_reward: 92.650 [-40.000, 186.000] - loss: 98.425 - mae: 89.776 - mean_q: 97.409\n",
      "\n",
      "Interval 117 (1160000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 1.8010\n",
      "200 episodes - episode_reward: 90.050 [-50.000, 178.000] - loss: 98.658 - mae: 89.772 - mean_q: 97.376\n",
      "\n",
      "Interval 118 (1170000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 1.9672\n",
      "200 episodes - episode_reward: 98.360 [-50.000, 220.000] - loss: 98.840 - mae: 89.670 - mean_q: 97.536\n",
      "\n",
      "Interval 119 (1180000 steps performed)\n",
      "10000/10000 [==============================] - 78s 8ms/step - reward: 1.9166\n",
      "200 episodes - episode_reward: 95.830 [-34.000, 186.000] - loss: 102.857 - mae: 92.126 - mean_q: 100.118\n",
      "\n",
      "Interval 120 (1190000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 1.8366\n",
      "200 episodes - episode_reward: 91.830 [-50.000, 194.000] - loss: 105.667 - mae: 92.699 - mean_q: 100.687\n",
      "\n",
      "Interval 121 (1200000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.9250\n",
      "200 episodes - episode_reward: 96.250 [-46.000, 242.000] - loss: 108.598 - mae: 94.757 - mean_q: 102.879\n",
      "\n",
      "Interval 122 (1210000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 1.7326\n",
      "200 episodes - episode_reward: 86.630 [-42.000, 202.000] - loss: 109.630 - mae: 94.162 - mean_q: 102.159\n",
      "\n",
      "Interval 123 (1220000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 1.7442\n",
      "200 episodes - episode_reward: 87.210 [-46.000, 192.000] - loss: 104.877 - mae: 93.768 - mean_q: 101.633\n",
      "\n",
      "Interval 124 (1230000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.9546\n",
      "200 episodes - episode_reward: 97.730 [-50.000, 208.000] - loss: 103.123 - mae: 91.380 - mean_q: 99.163\n",
      "\n",
      "Interval 125 (1240000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 1.9086\n",
      "200 episodes - episode_reward: 95.430 [-18.000, 214.000] - loss: 103.741 - mae: 91.562 - mean_q: 99.452\n",
      "\n",
      "Interval 126 (1250000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 1.9972\n",
      "200 episodes - episode_reward: 99.860 [-68.000, 252.000] - loss: 107.521 - mae: 94.100 - mean_q: 102.229\n",
      "\n",
      "Interval 127 (1260000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 1.9794\n",
      "200 episodes - episode_reward: 98.970 [-50.000, 220.000] - loss: 108.681 - mae: 94.511 - mean_q: 102.770\n",
      "\n",
      "Interval 128 (1270000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 1.9342\n",
      "200 episodes - episode_reward: 96.710 [-40.000, 224.000] - loss: 112.833 - mae: 95.743 - mean_q: 104.129\n",
      "\n",
      "Interval 129 (1280000 steps performed)\n",
      "10000/10000 [==============================] - 60s 6ms/step - reward: 2.0208\n",
      "200 episodes - episode_reward: 101.040 [-32.000, 186.000] - loss: 114.734 - mae: 96.411 - mean_q: 104.828\n",
      "\n",
      "Interval 130 (1290000 steps performed)\n",
      "10000/10000 [==============================] - 111s 11ms/step - reward: 2.1182\n",
      "200 episodes - episode_reward: 105.910 [-20.000, 194.000] - loss: 116.199 - mae: 98.315 - mean_q: 106.747\n",
      "\n",
      "Interval 131 (1300000 steps performed)\n",
      "10000/10000 [==============================] - 80s 8ms/step - reward: 1.9758\n",
      "200 episodes - episode_reward: 98.790 [-40.000, 202.000] - loss: 122.582 - mae: 101.507 - mean_q: 110.149\n",
      "\n",
      "Interval 132 (1310000 steps performed)\n",
      "10000/10000 [==============================] - 76s 8ms/step - reward: 1.9324\n",
      "200 episodes - episode_reward: 96.620 [-40.000, 214.000] - loss: 122.145 - mae: 100.324 - mean_q: 108.894\n",
      "\n",
      "Interval 133 (1320000 steps performed)\n",
      "10000/10000 [==============================] - 66s 7ms/step - reward: 2.0456\n",
      "200 episodes - episode_reward: 102.280 [-44.000, 234.000] - loss: 114.448 - mae: 96.774 - mean_q: 105.029\n",
      "\n",
      "Interval 134 (1330000 steps performed)\n",
      "10000/10000 [==============================] - 74s 7ms/step - reward: 2.0110\n",
      "200 episodes - episode_reward: 100.550 [-36.000, 200.000] - loss: 114.230 - mae: 96.344 - mean_q: 104.618\n",
      "\n",
      "Interval 135 (1340000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 2.2500\n",
      "200 episodes - episode_reward: 112.500 [-28.000, 198.000] - loss: 114.588 - mae: 97.008 - mean_q: 105.207\n",
      "\n",
      "Interval 136 (1350000 steps performed)\n",
      "10000/10000 [==============================] - 58s 6ms/step - reward: 2.0104\n",
      "200 episodes - episode_reward: 100.520 [-28.000, 212.000] - loss: 116.066 - mae: 98.417 - mean_q: 106.747\n",
      "\n",
      "Interval 137 (1360000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 2.1324\n",
      "200 episodes - episode_reward: 106.620 [-50.000, 248.000] - loss: 117.182 - mae: 97.812 - mean_q: 106.271\n",
      "\n",
      "Interval 138 (1370000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.0956\n",
      "200 episodes - episode_reward: 104.780 [-40.000, 202.000] - loss: 118.627 - mae: 99.161 - mean_q: 107.754\n",
      "\n",
      "Interval 139 (1380000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 2.2664\n",
      "200 episodes - episode_reward: 113.320 [-38.000, 216.000] - loss: 121.672 - mae: 99.890 - mean_q: 108.539\n",
      "\n",
      "Interval 140 (1390000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 2.3808\n",
      "200 episodes - episode_reward: 119.040 [-20.000, 210.000] - loss: 118.770 - mae: 99.291 - mean_q: 107.809\n",
      "\n",
      "Interval 141 (1400000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 2.1406\n",
      "200 episodes - episode_reward: 107.030 [-50.000, 212.000] - loss: 126.483 - mae: 100.649 - mean_q: 109.267\n",
      "\n",
      "Interval 142 (1410000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 2.1998\n",
      "200 episodes - episode_reward: 109.990 [-22.000, 246.000] - loss: 130.184 - mae: 102.452 - mean_q: 111.055\n",
      "\n",
      "Interval 143 (1420000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 2.2072\n",
      "200 episodes - episode_reward: 110.360 [-48.000, 252.000] - loss: 129.003 - mae: 103.214 - mean_q: 111.841\n",
      "\n",
      "Interval 144 (1430000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 2.3572\n",
      "200 episodes - episode_reward: 117.860 [-40.000, 230.000] - loss: 133.401 - mae: 104.559 - mean_q: 113.553\n",
      "\n",
      "Interval 145 (1440000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 2.3766\n",
      "200 episodes - episode_reward: 118.830 [-50.000, 234.000] - loss: 134.659 - mae: 104.974 - mean_q: 114.023\n",
      "\n",
      "Interval 146 (1450000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 2.3714\n",
      "200 episodes - episode_reward: 118.570 [-50.000, 222.000] - loss: 143.325 - mae: 109.209 - mean_q: 118.608\n",
      "\n",
      "Interval 147 (1460000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.3112\n",
      "200 episodes - episode_reward: 115.560 [-18.000, 264.000] - loss: 145.124 - mae: 109.613 - mean_q: 119.053\n",
      "\n",
      "Interval 148 (1470000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.4526\n",
      "200 episodes - episode_reward: 122.630 [-30.000, 228.000] - loss: 147.573 - mae: 109.536 - mean_q: 119.059\n",
      "\n",
      "Interval 149 (1480000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.2104\n",
      "200 episodes - episode_reward: 110.520 [-28.000, 222.000] - loss: 151.958 - mae: 111.119 - mean_q: 120.696\n",
      "\n",
      "Interval 150 (1490000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.3076\n",
      "200 episodes - episode_reward: 115.380 [-48.000, 222.000] - loss: 141.132 - mae: 108.647 - mean_q: 117.938\n",
      "\n",
      "Interval 151 (1500000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.2758\n",
      "200 episodes - episode_reward: 113.790 [-34.000, 232.000] - loss: 146.551 - mae: 110.466 - mean_q: 119.907\n",
      "\n",
      "Interval 152 (1510000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.1288\n",
      "200 episodes - episode_reward: 106.440 [-50.000, 214.000] - loss: 148.418 - mae: 111.413 - mean_q: 120.964\n",
      "\n",
      "Interval 153 (1520000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.2778\n",
      "200 episodes - episode_reward: 113.890 [-30.000, 212.000] - loss: 145.538 - mae: 109.174 - mean_q: 118.405\n",
      "\n",
      "Interval 154 (1530000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.3292\n",
      "200 episodes - episode_reward: 116.460 [-30.000, 256.000] - loss: 137.034 - mae: 105.642 - mean_q: 114.735\n",
      "\n",
      "Interval 155 (1540000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.4734\n",
      "200 episodes - episode_reward: 123.670 [-8.000, 224.000] - loss: 139.053 - mae: 107.181 - mean_q: 116.410\n",
      "\n",
      "Interval 156 (1550000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.3852\n",
      "200 episodes - episode_reward: 119.260 [-64.000, 244.000] - loss: 138.456 - mae: 107.153 - mean_q: 116.211\n",
      "\n",
      "Interval 157 (1560000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.2656\n",
      "200 episodes - episode_reward: 113.280 [-18.000, 214.000] - loss: 135.555 - mae: 106.082 - mean_q: 114.972\n",
      "\n",
      "Interval 158 (1570000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.3378\n",
      "200 episodes - episode_reward: 116.890 [-48.000, 230.000] - loss: 135.681 - mae: 106.056 - mean_q: 114.828\n",
      "\n",
      "Interval 159 (1580000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.3332\n",
      "200 episodes - episode_reward: 116.660 [-42.000, 262.000] - loss: 134.564 - mae: 104.796 - mean_q: 113.543\n",
      "\n",
      "Interval 160 (1590000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.3656\n",
      "200 episodes - episode_reward: 118.280 [-50.000, 220.000] - loss: 134.514 - mae: 104.458 - mean_q: 113.204\n",
      "\n",
      "Interval 161 (1600000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.3888\n",
      "200 episodes - episode_reward: 119.440 [-48.000, 248.000] - loss: 134.998 - mae: 106.281 - mean_q: 115.090\n",
      "\n",
      "Interval 162 (1610000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.4514\n",
      "200 episodes - episode_reward: 122.570 [-42.000, 276.000] - loss: 141.723 - mae: 107.808 - mean_q: 116.747\n",
      "\n",
      "Interval 163 (1620000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.2312\n",
      "200 episodes - episode_reward: 111.560 [-68.000, 228.000] - loss: 143.926 - mae: 109.775 - mean_q: 119.014\n",
      "\n",
      "Interval 164 (1630000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.4298\n",
      "200 episodes - episode_reward: 121.490 [-22.000, 240.000] - loss: 144.869 - mae: 109.962 - mean_q: 119.109\n",
      "\n",
      "Interval 165 (1640000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 2.5208\n",
      "200 episodes - episode_reward: 126.040 [-42.000, 250.000] - loss: 145.998 - mae: 109.638 - mean_q: 118.882\n",
      "\n",
      "Interval 166 (1650000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.4718\n",
      "200 episodes - episode_reward: 123.590 [-2.000, 268.000] - loss: 146.064 - mae: 110.536 - mean_q: 119.909\n",
      "\n",
      "Interval 167 (1660000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.5234\n",
      "200 episodes - episode_reward: 126.170 [-16.000, 240.000] - loss: 151.515 - mae: 112.171 - mean_q: 121.560\n",
      "\n",
      "Interval 168 (1670000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 2.3634\n",
      "200 episodes - episode_reward: 118.170 [-20.000, 236.000] - loss: 153.967 - mae: 113.363 - mean_q: 122.909\n",
      "\n",
      "Interval 169 (1680000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.4158\n",
      "200 episodes - episode_reward: 120.790 [-22.000, 226.000] - loss: 157.607 - mae: 114.279 - mean_q: 124.024\n",
      "\n",
      "Interval 170 (1690000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 2.2662\n",
      "200 episodes - episode_reward: 113.310 [-36.000, 212.000] - loss: 153.116 - mae: 112.172 - mean_q: 121.665\n",
      "\n",
      "Interval 171 (1700000 steps performed)\n",
      "10000/10000 [==============================] - 58s 6ms/step - reward: 2.5574\n",
      "200 episodes - episode_reward: 127.870 [-40.000, 254.000] - loss: 148.287 - mae: 110.332 - mean_q: 119.795\n",
      "\n",
      "Interval 172 (1710000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.5312\n",
      "200 episodes - episode_reward: 126.560 [-40.000, 234.000] - loss: 149.813 - mae: 112.167 - mean_q: 121.850\n",
      "\n",
      "Interval 173 (1720000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 2.5224\n",
      "200 episodes - episode_reward: 126.120 [-22.000, 242.000] - loss: 150.257 - mae: 111.456 - mean_q: 121.038\n",
      "\n",
      "Interval 174 (1730000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.4458\n",
      "200 episodes - episode_reward: 122.290 [-42.000, 256.000] - loss: 148.907 - mae: 110.143 - mean_q: 119.575\n",
      "\n",
      "Interval 175 (1740000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 2.4828\n",
      "200 episodes - episode_reward: 124.140 [-22.000, 238.000] - loss: 147.032 - mae: 110.077 - mean_q: 119.452\n",
      "\n",
      "Interval 176 (1750000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 2.6026\n",
      "200 episodes - episode_reward: 130.130 [-6.000, 238.000] - loss: 155.600 - mae: 114.305 - mean_q: 123.904\n",
      "\n",
      "Interval 177 (1760000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 2.5412\n",
      "200 episodes - episode_reward: 127.060 [-46.000, 236.000] - loss: 156.566 - mae: 113.223 - mean_q: 122.537\n",
      "\n",
      "Interval 178 (1770000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 2.6410\n",
      "200 episodes - episode_reward: 132.050 [28.000, 250.000] - loss: 157.336 - mae: 114.347 - mean_q: 123.911\n",
      "\n",
      "Interval 179 (1780000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 2.6136\n",
      "200 episodes - episode_reward: 130.680 [-6.000, 242.000] - loss: 160.270 - mae: 114.569 - mean_q: 124.132\n",
      "\n",
      "Interval 180 (1790000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 2.6606\n",
      "200 episodes - episode_reward: 133.030 [24.000, 230.000] - loss: 159.324 - mae: 114.947 - mean_q: 124.530\n",
      "\n",
      "Interval 181 (1800000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 2.4576\n",
      "200 episodes - episode_reward: 122.880 [-22.000, 256.000] - loss: 156.502 - mae: 112.925 - mean_q: 122.392\n",
      "\n",
      "Interval 182 (1810000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.5194\n",
      "200 episodes - episode_reward: 125.970 [-40.000, 246.000] - loss: 155.415 - mae: 112.940 - mean_q: 122.523\n",
      "\n",
      "Interval 183 (1820000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.5072\n",
      "200 episodes - episode_reward: 125.360 [-46.000, 220.000] - loss: 156.211 - mae: 114.099 - mean_q: 123.791\n",
      "\n",
      "Interval 184 (1830000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.2882\n",
      "200 episodes - episode_reward: 114.410 [-52.000, 210.000] - loss: 151.713 - mae: 112.396 - mean_q: 121.825\n",
      "\n",
      "Interval 185 (1840000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.5752\n",
      "200 episodes - episode_reward: 128.760 [-8.000, 280.000] - loss: 147.603 - mae: 110.090 - mean_q: 119.540\n",
      "\n",
      "Interval 186 (1850000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.5994\n",
      "200 episodes - episode_reward: 129.970 [0.000, 228.000] - loss: 146.517 - mae: 110.171 - mean_q: 119.741\n",
      "\n",
      "Interval 187 (1860000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.6412\n",
      "200 episodes - episode_reward: 132.060 [-74.000, 248.000] - loss: 151.259 - mae: 111.722 - mean_q: 121.480\n",
      "\n",
      "Interval 188 (1870000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.4878\n",
      "200 episodes - episode_reward: 124.390 [-52.000, 242.000] - loss: 153.357 - mae: 112.810 - mean_q: 122.650\n",
      "\n",
      "Interval 189 (1880000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 55s 5ms/step - reward: 2.6060\n",
      "200 episodes - episode_reward: 130.300 [-40.000, 240.000] - loss: 156.973 - mae: 114.489 - mean_q: 124.743\n",
      "\n",
      "Interval 190 (1890000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 2.6538\n",
      "200 episodes - episode_reward: 132.690 [-12.000, 262.000] - loss: 162.767 - mae: 116.100 - mean_q: 126.289\n",
      "\n",
      "Interval 191 (1900000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.6166\n",
      "200 episodes - episode_reward: 130.830 [-50.000, 226.000] - loss: 159.785 - mae: 114.718 - mean_q: 124.714\n",
      "\n",
      "Interval 192 (1910000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 2.4198\n",
      "200 episodes - episode_reward: 120.990 [-20.000, 216.000] - loss: 157.952 - mae: 113.683 - mean_q: 123.457\n",
      "\n",
      "Interval 193 (1920000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.4624\n",
      "200 episodes - episode_reward: 123.120 [8.000, 238.000] - loss: 149.295 - mae: 110.597 - mean_q: 119.889\n",
      "\n",
      "Interval 194 (1930000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.4414\n",
      "200 episodes - episode_reward: 122.070 [-40.000, 292.000] - loss: 145.814 - mae: 109.833 - mean_q: 119.104\n",
      "\n",
      "Interval 195 (1940000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 2.5494\n",
      "200 episodes - episode_reward: 127.470 [-40.000, 244.000] - loss: 144.168 - mae: 108.839 - mean_q: 118.003\n",
      "\n",
      "Interval 196 (1950000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.4928\n",
      "200 episodes - episode_reward: 124.640 [-40.000, 268.000] - loss: 139.627 - mae: 107.908 - mean_q: 117.006\n",
      "\n",
      "Interval 197 (1960000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.5088\n",
      "200 episodes - episode_reward: 125.440 [-56.000, 240.000] - loss: 140.467 - mae: 107.692 - mean_q: 116.800\n",
      "\n",
      "Interval 198 (1970000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.5002\n",
      "200 episodes - episode_reward: 125.010 [-40.000, 248.000] - loss: 145.362 - mae: 109.470 - mean_q: 118.913\n",
      "\n",
      "Interval 199 (1980000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 2.5282\n",
      "200 episodes - episode_reward: 126.410 [-2.000, 266.000] - loss: 148.483 - mae: 112.709 - mean_q: 122.269\n",
      "\n",
      "Interval 200 (1990000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.4754\n",
      "200 episodes - episode_reward: 123.770 [-40.000, 240.000] - loss: 149.846 - mae: 111.433 - mean_q: 120.815\n",
      "\n",
      "Interval 201 (2000000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 2.4608\n",
      "200 episodes - episode_reward: 123.040 [-10.000, 218.000] - loss: 148.741 - mae: 110.538 - mean_q: 119.961\n",
      "\n",
      "Interval 202 (2010000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.5414\n",
      "200 episodes - episode_reward: 127.070 [-30.000, 234.000] - loss: 151.479 - mae: 110.819 - mean_q: 120.395\n",
      "\n",
      "Interval 203 (2020000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 2.5296\n",
      "200 episodes - episode_reward: 126.480 [-50.000, 220.000] - loss: 148.702 - mae: 111.799 - mean_q: 121.307\n",
      "\n",
      "Interval 204 (2030000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.7812\n",
      "200 episodes - episode_reward: 139.060 [-40.000, 268.000] - loss: 153.853 - mae: 112.743 - mean_q: 122.412\n",
      "\n",
      "Interval 205 (2040000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 2.6344\n",
      "200 episodes - episode_reward: 131.720 [-50.000, 242.000] - loss: 150.129 - mae: 112.155 - mean_q: 121.629\n",
      "\n",
      "Interval 206 (2050000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 2.3728\n",
      "200 episodes - episode_reward: 118.640 [-28.000, 214.000] - loss: 149.851 - mae: 110.624 - mean_q: 120.105\n",
      "\n",
      "Interval 207 (2060000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 2.3342\n",
      "200 episodes - episode_reward: 116.710 [-28.000, 256.000] - loss: 150.016 - mae: 112.614 - mean_q: 122.110\n",
      "\n",
      "Interval 208 (2070000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 2.5156\n",
      "200 episodes - episode_reward: 125.780 [-16.000, 246.000] - loss: 148.566 - mae: 112.099 - mean_q: 121.676\n",
      "\n",
      "Interval 209 (2080000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 2.4440\n",
      "200 episodes - episode_reward: 122.200 [-42.000, 248.000] - loss: 148.462 - mae: 110.123 - mean_q: 119.324\n",
      "\n",
      "Interval 210 (2090000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 2.7184\n",
      "200 episodes - episode_reward: 135.920 [-26.000, 258.000] - loss: 141.119 - mae: 107.524 - mean_q: 116.563\n",
      "\n",
      "Interval 211 (2100000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.4868\n",
      "200 episodes - episode_reward: 124.340 [-8.000, 242.000] - loss: 148.686 - mae: 112.140 - mean_q: 121.569\n",
      "\n",
      "Interval 212 (2110000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 2.5530\n",
      "200 episodes - episode_reward: 127.650 [28.000, 228.000] - loss: 151.644 - mae: 112.620 - mean_q: 121.965\n",
      "\n",
      "Interval 213 (2120000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 2.5134\n",
      "200 episodes - episode_reward: 125.670 [-10.000, 228.000] - loss: 147.856 - mae: 110.095 - mean_q: 119.090\n",
      "\n",
      "Interval 214 (2130000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.6996\n",
      "200 episodes - episode_reward: 134.980 [-10.000, 228.000] - loss: 147.152 - mae: 112.384 - mean_q: 121.546\n",
      "\n",
      "Interval 215 (2140000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.6156\n",
      "200 episodes - episode_reward: 130.780 [-30.000, 258.000] - loss: 150.767 - mae: 112.632 - mean_q: 121.924\n",
      "\n",
      "Interval 216 (2150000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.6830\n",
      "200 episodes - episode_reward: 134.150 [-40.000, 252.000] - loss: 151.836 - mae: 112.319 - mean_q: 121.885\n",
      "\n",
      "Interval 217 (2160000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.7162\n",
      "200 episodes - episode_reward: 135.810 [-12.000, 244.000] - loss: 154.682 - mae: 113.829 - mean_q: 123.473\n",
      "\n",
      "Interval 218 (2170000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.6402\n",
      "200 episodes - episode_reward: 132.010 [-32.000, 266.000] - loss: 160.041 - mae: 114.888 - mean_q: 124.664\n",
      "\n",
      "Interval 219 (2180000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.7864\n",
      "200 episodes - episode_reward: 139.320 [-2.000, 250.000] - loss: 160.028 - mae: 116.458 - mean_q: 126.364\n",
      "\n",
      "Interval 220 (2190000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.4700\n",
      "200 episodes - episode_reward: 123.500 [-26.000, 246.000] - loss: 159.419 - mae: 114.551 - mean_q: 124.131\n",
      "\n",
      "Interval 221 (2200000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.5806\n",
      "200 episodes - episode_reward: 129.030 [2.000, 234.000] - loss: 152.570 - mae: 113.389 - mean_q: 122.802\n",
      "\n",
      "Interval 222 (2210000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.5926\n",
      "200 episodes - episode_reward: 129.630 [-36.000, 236.000] - loss: 155.006 - mae: 113.947 - mean_q: 123.386\n",
      "\n",
      "Interval 223 (2220000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.5174\n",
      "200 episodes - episode_reward: 125.870 [-60.000, 226.000] - loss: 155.276 - mae: 115.410 - mean_q: 124.937\n",
      "\n",
      "Interval 224 (2230000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.5448\n",
      "200 episodes - episode_reward: 127.240 [-8.000, 238.000] - loss: 151.873 - mae: 111.767 - mean_q: 120.813\n",
      "\n",
      "Interval 225 (2240000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.6480\n",
      "200 episodes - episode_reward: 132.400 [-46.000, 282.000] - loss: 145.787 - mae: 110.766 - mean_q: 119.851\n",
      "\n",
      "Interval 226 (2250000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.5698\n",
      "200 episodes - episode_reward: 128.490 [-10.000, 238.000] - loss: 147.781 - mae: 111.471 - mean_q: 120.704\n",
      "\n",
      "Interval 227 (2260000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.4342\n",
      "200 episodes - episode_reward: 121.710 [-50.000, 244.000] - loss: 148.974 - mae: 112.339 - mean_q: 121.507\n",
      "\n",
      "Interval 228 (2270000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 2.6104\n",
      "200 episodes - episode_reward: 130.520 [-16.000, 240.000] - loss: 148.772 - mae: 111.881 - mean_q: 121.124\n",
      "\n",
      "Interval 229 (2280000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.5968\n",
      "200 episodes - episode_reward: 129.840 [-10.000, 252.000] - loss: 153.616 - mae: 113.517 - mean_q: 122.943\n",
      "\n",
      "Interval 230 (2290000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 2.6576\n",
      "200 episodes - episode_reward: 132.880 [-26.000, 256.000] - loss: 151.748 - mae: 112.918 - mean_q: 122.141\n",
      "\n",
      "Interval 231 (2300000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.5988\n",
      "200 episodes - episode_reward: 129.940 [-36.000, 268.000] - loss: 156.362 - mae: 112.371 - mean_q: 121.532\n",
      "\n",
      "Interval 232 (2310000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 2.4438\n",
      "200 episodes - episode_reward: 122.190 [-22.000, 228.000] - loss: 153.495 - mae: 112.441 - mean_q: 121.756\n",
      "\n",
      "Interval 233 (2320000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.4594\n",
      "200 episodes - episode_reward: 122.970 [-26.000, 228.000] - loss: 155.810 - mae: 114.327 - mean_q: 123.688\n",
      "\n",
      "Interval 234 (2330000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.4922\n",
      "200 episodes - episode_reward: 124.610 [-42.000, 224.000] - loss: 154.763 - mae: 114.362 - mean_q: 123.895\n",
      "\n",
      "Interval 235 (2340000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 2.7148\n",
      "200 episodes - episode_reward: 135.740 [16.000, 232.000] - loss: 154.856 - mae: 112.525 - mean_q: 121.960\n",
      "\n",
      "Interval 236 (2350000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7356\n",
      "200 episodes - episode_reward: 136.780 [-10.000, 234.000] - loss: 150.366 - mae: 111.253 - mean_q: 120.525\n",
      "\n",
      "Interval 237 (2360000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.4842\n",
      "200 episodes - episode_reward: 124.210 [-10.000, 238.000] - loss: 152.289 - mae: 112.622 - mean_q: 121.884\n",
      "\n",
      "Interval 238 (2370000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.3040\n",
      "200 episodes - episode_reward: 115.200 [-50.000, 216.000] - loss: 152.315 - mae: 112.254 - mean_q: 121.462\n",
      "\n",
      "Interval 239 (2380000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.3120\n",
      "200 episodes - episode_reward: 115.600 [-28.000, 240.000] - loss: 144.880 - mae: 108.841 - mean_q: 117.781\n",
      "\n",
      "Interval 240 (2390000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.4762\n",
      "200 episodes - episode_reward: 123.810 [-40.000, 258.000] - loss: 144.100 - mae: 109.654 - mean_q: 118.623\n",
      "\n",
      "Interval 241 (2400000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.5266\n",
      "200 episodes - episode_reward: 126.330 [-36.000, 250.000] - loss: 140.635 - mae: 107.962 - mean_q: 116.782\n",
      "\n",
      "Interval 242 (2410000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.4522\n",
      "200 episodes - episode_reward: 122.610 [-50.000, 234.000] - loss: 137.409 - mae: 107.613 - mean_q: 116.532\n",
      "\n",
      "Interval 243 (2420000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 2.6480\n",
      "200 episodes - episode_reward: 132.400 [-50.000, 258.000] - loss: 143.338 - mae: 108.617 - mean_q: 117.756\n",
      "\n",
      "Interval 244 (2430000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6284\n",
      "200 episodes - episode_reward: 131.420 [-46.000, 264.000] - loss: 142.870 - mae: 108.230 - mean_q: 117.257\n",
      "\n",
      "Interval 245 (2440000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6650\n",
      "200 episodes - episode_reward: 133.250 [-42.000, 236.000] - loss: 146.355 - mae: 109.910 - mean_q: 119.039\n",
      "\n",
      "Interval 246 (2450000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6964\n",
      "200 episodes - episode_reward: 134.820 [14.000, 284.000] - loss: 151.654 - mae: 112.335 - mean_q: 121.675\n",
      "\n",
      "Interval 247 (2460000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6996\n",
      "200 episodes - episode_reward: 134.980 [30.000, 266.000] - loss: 153.360 - mae: 112.705 - mean_q: 122.036\n",
      "\n",
      "Interval 248 (2470000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6584\n",
      "200 episodes - episode_reward: 132.920 [0.000, 218.000] - loss: 152.233 - mae: 112.823 - mean_q: 122.154\n",
      "\n",
      "Interval 249 (2480000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6060\n",
      "200 episodes - episode_reward: 130.300 [-50.000, 240.000] - loss: 153.931 - mae: 112.092 - mean_q: 121.502\n",
      "\n",
      "Interval 250 (2490000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 2.7120\n",
      "200 episodes - episode_reward: 135.600 [-50.000, 250.000] - loss: 152.429 - mae: 113.191 - mean_q: 122.702\n",
      "\n",
      "Interval 251 (2500000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 2.6186\n",
      "200 episodes - episode_reward: 130.930 [-32.000, 234.000] - loss: 156.534 - mae: 112.729 - mean_q: 122.003\n",
      "\n",
      "Interval 252 (2510000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 2.6898\n",
      "200 episodes - episode_reward: 134.490 [-10.000, 252.000] - loss: 152.700 - mae: 113.260 - mean_q: 122.626\n",
      "\n",
      "Interval 253 (2520000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.6344\n",
      "200 episodes - episode_reward: 131.720 [-38.000, 260.000] - loss: 153.740 - mae: 113.638 - mean_q: 122.820\n",
      "\n",
      "Interval 254 (2530000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.3790\n",
      "200 episodes - episode_reward: 118.950 [-40.000, 248.000] - loss: 150.038 - mae: 111.796 - mean_q: 120.673\n",
      "\n",
      "Interval 255 (2540000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.5174\n",
      "200 episodes - episode_reward: 125.870 [-50.000, 254.000] - loss: 146.570 - mae: 110.463 - mean_q: 119.395\n",
      "\n",
      "Interval 256 (2550000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.3638\n",
      "200 episodes - episode_reward: 118.190 [-50.000, 228.000] - loss: 146.498 - mae: 110.476 - mean_q: 119.521\n",
      "\n",
      "Interval 257 (2560000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.6838\n",
      "200 episodes - episode_reward: 134.190 [-50.000, 244.000] - loss: 144.015 - mae: 109.127 - mean_q: 118.182\n",
      "\n",
      "Interval 258 (2570000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.5612\n",
      "200 episodes - episode_reward: 128.060 [-50.000, 240.000] - loss: 145.181 - mae: 108.264 - mean_q: 117.234\n",
      "\n",
      "Interval 259 (2580000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.5740\n",
      "200 episodes - episode_reward: 128.700 [-50.000, 270.000] - loss: 145.001 - mae: 109.470 - mean_q: 118.574\n",
      "\n",
      "Interval 260 (2590000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.5318\n",
      "200 episodes - episode_reward: 126.590 [-32.000, 236.000] - loss: 148.329 - mae: 110.427 - mean_q: 119.547\n",
      "\n",
      "Interval 261 (2600000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.5268\n",
      "200 episodes - episode_reward: 126.340 [-50.000, 242.000] - loss: 150.245 - mae: 112.106 - mean_q: 121.230\n",
      "\n",
      "Interval 262 (2610000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.5666\n",
      "200 episodes - episode_reward: 128.330 [-40.000, 284.000] - loss: 145.714 - mae: 110.438 - mean_q: 119.354\n",
      "\n",
      "Interval 263 (2620000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.5644\n",
      "200 episodes - episode_reward: 128.220 [-58.000, 240.000] - loss: 143.883 - mae: 108.542 - mean_q: 117.592\n",
      "\n",
      "Interval 264 (2630000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.6784\n",
      "200 episodes - episode_reward: 133.920 [-56.000, 246.000] - loss: 142.676 - mae: 108.828 - mean_q: 117.937\n",
      "\n",
      "Interval 265 (2640000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.6876\n",
      "200 episodes - episode_reward: 134.380 [-40.000, 246.000] - loss: 143.056 - mae: 109.456 - mean_q: 118.724\n",
      "\n",
      "Interval 266 (2650000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.6704\n",
      "200 episodes - episode_reward: 133.520 [-40.000, 242.000] - loss: 151.834 - mae: 112.344 - mean_q: 121.738\n",
      "\n",
      "Interval 267 (2660000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.4758\n",
      "200 episodes - episode_reward: 123.790 [-40.000, 254.000] - loss: 150.563 - mae: 112.105 - mean_q: 121.469\n",
      "\n",
      "Interval 268 (2670000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.5858\n",
      "200 episodes - episode_reward: 129.290 [-36.000, 232.000] - loss: 154.860 - mae: 113.063 - mean_q: 122.839\n",
      "\n",
      "Interval 269 (2680000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.4734\n",
      "200 episodes - episode_reward: 123.670 [-28.000, 252.000] - loss: 153.251 - mae: 112.901 - mean_q: 122.712\n",
      "\n",
      "Interval 270 (2690000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.5990\n",
      "200 episodes - episode_reward: 129.950 [8.000, 248.000] - loss: 152.694 - mae: 112.271 - mean_q: 121.893\n",
      "\n",
      "Interval 271 (2700000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.7066\n",
      "200 episodes - episode_reward: 135.330 [-34.000, 240.000] - loss: 151.498 - mae: 111.420 - mean_q: 120.930\n",
      "\n",
      "Interval 272 (2710000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.7412\n",
      "200 episodes - episode_reward: 137.060 [-50.000, 256.000] - loss: 155.080 - mae: 113.197 - mean_q: 122.898\n",
      "\n",
      "Interval 273 (2720000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.6822\n",
      "200 episodes - episode_reward: 134.110 [-30.000, 274.000] - loss: 159.591 - mae: 114.805 - mean_q: 124.712\n",
      "\n",
      "Interval 274 (2730000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 2.6926\n",
      "200 episodes - episode_reward: 134.630 [-30.000, 236.000] - loss: 161.026 - mae: 115.397 - mean_q: 125.278\n",
      "\n",
      "Interval 275 (2740000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7694\n",
      "200 episodes - episode_reward: 138.470 [4.000, 258.000] - loss: 156.743 - mae: 114.256 - mean_q: 123.889\n",
      "\n",
      "Interval 276 (2750000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7134\n",
      "200 episodes - episode_reward: 135.670 [-22.000, 290.000] - loss: 157.760 - mae: 114.242 - mean_q: 123.904\n",
      "\n",
      "Interval 277 (2760000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6904\n",
      "200 episodes - episode_reward: 134.520 [-6.000, 244.000] - loss: 156.643 - mae: 113.018 - mean_q: 122.557\n",
      "\n",
      "Interval 278 (2770000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.4728\n",
      "200 episodes - episode_reward: 123.640 [-54.000, 248.000] - loss: 153.912 - mae: 112.898 - mean_q: 122.373\n",
      "\n",
      "Interval 279 (2780000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.5780\n",
      "200 episodes - episode_reward: 128.900 [-50.000, 236.000] - loss: 155.232 - mae: 114.541 - mean_q: 124.012\n",
      "\n",
      "Interval 280 (2790000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 2.5570\n",
      "200 episodes - episode_reward: 127.850 [-40.000, 250.000] - loss: 153.121 - mae: 112.860 - mean_q: 122.080\n",
      "\n",
      "Interval 281 (2800000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.4436\n",
      "200 episodes - episode_reward: 122.180 [-26.000, 272.000] - loss: 149.793 - mae: 110.141 - mean_q: 119.099\n",
      "\n",
      "Interval 282 (2810000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.5030\n",
      "200 episodes - episode_reward: 125.150 [-14.000, 216.000] - loss: 146.097 - mae: 111.370 - mean_q: 120.482\n",
      "\n",
      "Interval 283 (2820000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6616\n",
      "200 episodes - episode_reward: 133.080 [-40.000, 232.000] - loss: 147.256 - mae: 111.797 - mean_q: 121.038\n",
      "\n",
      "Interval 284 (2830000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7192\n",
      "200 episodes - episode_reward: 135.960 [-8.000, 256.000] - loss: 151.207 - mae: 111.241 - mean_q: 120.568\n",
      "\n",
      "Interval 285 (2840000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6202\n",
      "200 episodes - episode_reward: 131.010 [-40.000, 246.000] - loss: 151.152 - mae: 110.403 - mean_q: 119.788\n",
      "\n",
      "Interval 286 (2850000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7748\n",
      "200 episodes - episode_reward: 138.740 [-30.000, 250.000] - loss: 150.955 - mae: 112.903 - mean_q: 122.620\n",
      "\n",
      "Interval 287 (2860000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8674\n",
      "200 episodes - episode_reward: 143.370 [20.000, 246.000] - loss: 156.521 - mae: 113.863 - mean_q: 123.537\n",
      "\n",
      "Interval 288 (2870000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7136\n",
      "200 episodes - episode_reward: 135.680 [-8.000, 266.000] - loss: 159.583 - mae: 114.797 - mean_q: 124.504\n",
      "\n",
      "Interval 289 (2880000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.3746\n",
      "200 episodes - episode_reward: 118.730 [-40.000, 226.000] - loss: 158.161 - mae: 114.949 - mean_q: 124.406\n",
      "\n",
      "Interval 290 (2890000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6328\n",
      "200 episodes - episode_reward: 131.640 [-6.000, 240.000] - loss: 157.554 - mae: 114.859 - mean_q: 124.371\n",
      "\n",
      "Interval 291 (2900000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.5656\n",
      "200 episodes - episode_reward: 128.280 [-40.000, 262.000] - loss: 156.547 - mae: 114.439 - mean_q: 124.226\n",
      "\n",
      "Interval 292 (2910000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6752\n",
      "200 episodes - episode_reward: 133.760 [-28.000, 236.000] - loss: 152.961 - mae: 113.804 - mean_q: 123.302\n",
      "\n",
      "Interval 293 (2920000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 2.5956\n",
      "200 episodes - episode_reward: 129.780 [-28.000, 232.000] - loss: 148.122 - mae: 112.445 - mean_q: 121.865\n",
      "\n",
      "Interval 294 (2930000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.5168\n",
      "200 episodes - episode_reward: 125.840 [-50.000, 230.000] - loss: 152.220 - mae: 113.255 - mean_q: 122.909\n",
      "\n",
      "Interval 295 (2940000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7040\n",
      "200 episodes - episode_reward: 135.200 [-40.000, 242.000] - loss: 156.000 - mae: 113.750 - mean_q: 123.542\n",
      "\n",
      "Interval 296 (2950000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.5690\n",
      "200 episodes - episode_reward: 128.450 [-60.000, 262.000] - loss: 155.136 - mae: 114.584 - mean_q: 124.213\n",
      "\n",
      "Interval 297 (2960000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7702\n",
      "200 episodes - episode_reward: 138.510 [18.000, 234.000] - loss: 158.858 - mae: 116.334 - mean_q: 126.120\n",
      "\n",
      "Interval 298 (2970000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6710\n",
      "200 episodes - episode_reward: 133.550 [6.000, 306.000] - loss: 161.305 - mae: 115.388 - mean_q: 125.257\n",
      "\n",
      "Interval 299 (2980000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7318\n",
      "200 episodes - episode_reward: 136.590 [-2.000, 234.000] - loss: 155.477 - mae: 113.391 - mean_q: 122.935\n",
      "\n",
      "Interval 300 (2990000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7876\n",
      "200 episodes - episode_reward: 139.380 [-40.000, 280.000] - loss: 158.896 - mae: 114.622 - mean_q: 124.270\n",
      "\n",
      "Interval 301 (3000000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8004\n",
      "200 episodes - episode_reward: 140.020 [-54.000, 232.000] - loss: 157.685 - mae: 113.943 - mean_q: 123.458\n",
      "\n",
      "Interval 302 (3010000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.5700\n",
      "200 episodes - episode_reward: 128.500 [-54.000, 258.000] - loss: 158.513 - mae: 115.308 - mean_q: 124.772\n",
      "\n",
      "Interval 303 (3020000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.4582\n",
      "200 episodes - episode_reward: 122.910 [-38.000, 228.000] - loss: 153.167 - mae: 111.918 - mean_q: 121.057\n",
      "\n",
      "Interval 304 (3030000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6646\n",
      "200 episodes - episode_reward: 133.230 [8.000, 256.000] - loss: 147.419 - mae: 110.163 - mean_q: 119.328\n",
      "\n",
      "Interval 305 (3040000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6894\n",
      "200 episodes - episode_reward: 134.470 [-22.000, 226.000] - loss: 149.422 - mae: 112.614 - mean_q: 121.991\n",
      "\n",
      "Interval 306 (3050000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.5074\n",
      "200 episodes - episode_reward: 125.370 [-40.000, 236.000] - loss: 155.478 - mae: 113.565 - mean_q: 122.865\n",
      "\n",
      "Interval 307 (3060000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7650\n",
      "200 episodes - episode_reward: 138.250 [-20.000, 254.000] - loss: 149.365 - mae: 111.376 - mean_q: 120.574\n",
      "\n",
      "Interval 308 (3070000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7610\n",
      "200 episodes - episode_reward: 138.050 [-30.000, 250.000] - loss: 152.832 - mae: 113.395 - mean_q: 122.755\n",
      "\n",
      "Interval 309 (3080000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6204\n",
      "200 episodes - episode_reward: 131.020 [-40.000, 310.000] - loss: 155.937 - mae: 114.025 - mean_q: 123.415\n",
      "\n",
      "Interval 310 (3090000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7194\n",
      "200 episodes - episode_reward: 135.970 [-44.000, 248.000] - loss: 156.885 - mae: 113.930 - mean_q: 123.305\n",
      "\n",
      "Interval 311 (3100000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.5954\n",
      "200 episodes - episode_reward: 129.770 [-50.000, 266.000] - loss: 153.459 - mae: 114.946 - mean_q: 124.142\n",
      "\n",
      "Interval 312 (3110000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8792\n",
      "200 episodes - episode_reward: 143.960 [-30.000, 252.000] - loss: 152.901 - mae: 112.824 - mean_q: 122.104\n",
      "\n",
      "Interval 313 (3120000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7502\n",
      "200 episodes - episode_reward: 137.510 [-20.000, 302.000] - loss: 153.254 - mae: 112.369 - mean_q: 121.645\n",
      "\n",
      "Interval 314 (3130000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7142\n",
      "200 episodes - episode_reward: 135.710 [-42.000, 262.000] - loss: 154.694 - mae: 115.119 - mean_q: 124.623\n",
      "\n",
      "Interval 315 (3140000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8046\n",
      "200 episodes - episode_reward: 140.230 [-20.000, 258.000] - loss: 156.597 - mae: 114.407 - mean_q: 123.831\n",
      "\n",
      "Interval 316 (3150000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8164\n",
      "200 episodes - episode_reward: 140.820 [-48.000, 262.000] - loss: 157.034 - mae: 115.071 - mean_q: 124.440\n",
      "\n",
      "Interval 317 (3160000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6824\n",
      "200 episodes - episode_reward: 134.120 [-36.000, 264.000] - loss: 159.843 - mae: 115.909 - mean_q: 125.555\n",
      "\n",
      "Interval 318 (3170000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6282\n",
      "200 episodes - episode_reward: 131.410 [-40.000, 264.000] - loss: 162.117 - mae: 115.668 - mean_q: 125.336\n",
      "\n",
      "Interval 319 (3180000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8066\n",
      "200 episodes - episode_reward: 140.330 [-4.000, 244.000] - loss: 156.210 - mae: 114.646 - mean_q: 124.089\n",
      "\n",
      "Interval 320 (3190000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.5732\n",
      "200 episodes - episode_reward: 128.660 [-8.000, 262.000] - loss: 158.842 - mae: 115.794 - mean_q: 125.392\n",
      "\n",
      "Interval 321 (3200000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7100\n",
      "200 episodes - episode_reward: 135.500 [-6.000, 260.000] - loss: 152.454 - mae: 112.197 - mean_q: 121.613\n",
      "\n",
      "Interval 322 (3210000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8138\n",
      "200 episodes - episode_reward: 140.690 [-50.000, 232.000] - loss: 150.631 - mae: 112.261 - mean_q: 121.493\n",
      "\n",
      "Interval 323 (3220000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8002\n",
      "200 episodes - episode_reward: 140.010 [16.000, 240.000] - loss: 150.488 - mae: 111.532 - mean_q: 120.705\n",
      "\n",
      "Interval 324 (3230000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6830\n",
      "200 episodes - episode_reward: 134.150 [4.000, 246.000] - loss: 153.125 - mae: 113.598 - mean_q: 122.860\n",
      "\n",
      "Interval 325 (3240000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6452\n",
      "200 episodes - episode_reward: 132.260 [-32.000, 248.000] - loss: 156.342 - mae: 113.903 - mean_q: 123.377\n",
      "\n",
      "Interval 326 (3250000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7188\n",
      "200 episodes - episode_reward: 135.940 [-40.000, 248.000] - loss: 155.922 - mae: 113.854 - mean_q: 123.200\n",
      "\n",
      "Interval 327 (3260000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8496\n",
      "200 episodes - episode_reward: 142.480 [-18.000, 308.000] - loss: 156.950 - mae: 113.526 - mean_q: 122.928\n",
      "\n",
      "Interval 328 (3270000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8590\n",
      "200 episodes - episode_reward: 142.950 [16.000, 252.000] - loss: 154.239 - mae: 113.722 - mean_q: 123.022\n",
      "\n",
      "Interval 329 (3280000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8678\n",
      "200 episodes - episode_reward: 143.390 [-2.000, 254.000] - loss: 155.657 - mae: 112.889 - mean_q: 121.913\n",
      "\n",
      "Interval 330 (3290000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.4566\n",
      "200 episodes - episode_reward: 122.830 [-46.000, 266.000] - loss: 152.749 - mae: 114.181 - mean_q: 123.380\n",
      "\n",
      "Interval 331 (3300000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7168\n",
      "200 episodes - episode_reward: 135.840 [-38.000, 258.000] - loss: 159.387 - mae: 116.485 - mean_q: 125.716\n",
      "\n",
      "Interval 332 (3310000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7124\n",
      "200 episodes - episode_reward: 135.620 [-40.000, 256.000] - loss: 158.591 - mae: 115.054 - mean_q: 124.457\n",
      "\n",
      "Interval 333 (3320000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7818\n",
      "200 episodes - episode_reward: 139.090 [-2.000, 296.000] - loss: 156.423 - mae: 115.108 - mean_q: 124.609\n",
      "\n",
      "Interval 334 (3330000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6944\n",
      "200 episodes - episode_reward: 134.720 [-34.000, 258.000] - loss: 156.416 - mae: 115.517 - mean_q: 125.248\n",
      "\n",
      "Interval 335 (3340000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7748\n",
      "200 episodes - episode_reward: 138.740 [-36.000, 268.000] - loss: 160.858 - mae: 115.591 - mean_q: 125.271\n",
      "\n",
      "Interval 336 (3350000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8180\n",
      "200 episodes - episode_reward: 140.900 [-2.000, 238.000] - loss: 155.875 - mae: 114.149 - mean_q: 123.772\n",
      "\n",
      "Interval 337 (3360000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6402\n",
      "200 episodes - episode_reward: 132.010 [2.000, 230.000] - loss: 156.017 - mae: 114.541 - mean_q: 124.186\n",
      "\n",
      "Interval 338 (3370000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7328\n",
      "200 episodes - episode_reward: 136.640 [12.000, 272.000] - loss: 155.796 - mae: 115.041 - mean_q: 124.669\n",
      "\n",
      "Interval 339 (3380000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7370\n",
      "200 episodes - episode_reward: 136.850 [-24.000, 286.000] - loss: 155.550 - mae: 114.939 - mean_q: 124.312\n",
      "\n",
      "Interval 340 (3390000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6940\n",
      "200 episodes - episode_reward: 134.700 [-2.000, 252.000] - loss: 154.940 - mae: 113.597 - mean_q: 123.016\n",
      "\n",
      "Interval 341 (3400000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7226\n",
      "200 episodes - episode_reward: 136.130 [-26.000, 244.000] - loss: 153.104 - mae: 114.133 - mean_q: 123.665\n",
      "\n",
      "Interval 342 (3410000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7138\n",
      "200 episodes - episode_reward: 135.690 [6.000, 232.000] - loss: 156.441 - mae: 114.915 - mean_q: 124.509\n",
      "\n",
      "Interval 343 (3420000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6914\n",
      "200 episodes - episode_reward: 134.570 [-12.000, 250.000] - loss: 155.318 - mae: 114.726 - mean_q: 124.179\n",
      "\n",
      "Interval 344 (3430000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.4290\n",
      "200 episodes - episode_reward: 121.450 [-30.000, 232.000] - loss: 156.514 - mae: 114.610 - mean_q: 123.964\n",
      "\n",
      "Interval 345 (3440000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7920\n",
      "200 episodes - episode_reward: 139.600 [-22.000, 266.000] - loss: 151.745 - mae: 112.411 - mean_q: 121.738\n",
      "\n",
      "Interval 346 (3450000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.5032\n",
      "200 episodes - episode_reward: 125.160 [-52.000, 250.000] - loss: 158.041 - mae: 114.855 - mean_q: 124.317\n",
      "\n",
      "Interval 347 (3460000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6034\n",
      "200 episodes - episode_reward: 130.170 [-36.000, 256.000] - loss: 155.846 - mae: 113.727 - mean_q: 123.184\n",
      "\n",
      "Interval 348 (3470000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7466\n",
      "200 episodes - episode_reward: 137.330 [-46.000, 244.000] - loss: 157.284 - mae: 113.279 - mean_q: 122.726\n",
      "\n",
      "Interval 349 (3480000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6898\n",
      "200 episodes - episode_reward: 134.490 [-2.000, 240.000] - loss: 154.611 - mae: 114.073 - mean_q: 123.457\n",
      "\n",
      "Interval 350 (3490000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8526\n",
      "200 episodes - episode_reward: 142.630 [30.000, 258.000] - loss: 156.316 - mae: 113.748 - mean_q: 123.088\n",
      "\n",
      "Interval 351 (3500000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6026\n",
      "200 episodes - episode_reward: 130.130 [16.000, 246.000] - loss: 157.040 - mae: 114.699 - mean_q: 124.082\n",
      "\n",
      "Interval 352 (3510000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7434\n",
      "200 episodes - episode_reward: 137.170 [-40.000, 244.000] - loss: 156.810 - mae: 112.667 - mean_q: 121.774\n",
      "\n",
      "Interval 353 (3520000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8370\n",
      "200 episodes - episode_reward: 141.850 [2.000, 288.000] - loss: 152.915 - mae: 112.654 - mean_q: 121.728\n",
      "\n",
      "Interval 354 (3530000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8742\n",
      "200 episodes - episode_reward: 143.710 [-36.000, 254.000] - loss: 154.472 - mae: 113.768 - mean_q: 123.053\n",
      "\n",
      "Interval 355 (3540000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.9238\n",
      "200 episodes - episode_reward: 146.190 [14.000, 284.000] - loss: 159.697 - mae: 115.927 - mean_q: 125.504\n",
      "\n",
      "Interval 356 (3550000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.5680\n",
      "200 episodes - episode_reward: 128.400 [-42.000, 256.000] - loss: 162.683 - mae: 114.993 - mean_q: 124.526\n",
      "\n",
      "Interval 357 (3560000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8186\n",
      "200 episodes - episode_reward: 140.930 [-8.000, 232.000] - loss: 161.834 - mae: 115.614 - mean_q: 125.348\n",
      "\n",
      "Interval 358 (3570000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.9032\n",
      "200 episodes - episode_reward: 145.160 [18.000, 268.000] - loss: 162.068 - mae: 116.417 - mean_q: 126.254\n",
      "\n",
      "Interval 359 (3580000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7622\n",
      "200 episodes - episode_reward: 138.110 [-38.000, 250.000] - loss: 162.121 - mae: 115.668 - mean_q: 125.395\n",
      "\n",
      "Interval 360 (3590000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 2.6094\n",
      "200 episodes - episode_reward: 130.470 [-42.000, 236.000] - loss: 156.300 - mae: 113.921 - mean_q: 123.446\n",
      "\n",
      "Interval 361 (3600000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7434\n",
      "200 episodes - episode_reward: 137.170 [-46.000, 228.000] - loss: 157.385 - mae: 113.474 - mean_q: 122.917\n",
      "\n",
      "Interval 362 (3610000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6342\n",
      "200 episodes - episode_reward: 131.710 [-40.000, 250.000] - loss: 154.020 - mae: 112.622 - mean_q: 121.836\n",
      "\n",
      "Interval 363 (3620000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.4664\n",
      "200 episodes - episode_reward: 123.320 [-30.000, 232.000] - loss: 149.859 - mae: 111.952 - mean_q: 121.147\n",
      "\n",
      "Interval 364 (3630000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.5326\n",
      "200 episodes - episode_reward: 126.630 [-32.000, 228.000] - loss: 151.008 - mae: 112.468 - mean_q: 121.771\n",
      "\n",
      "Interval 365 (3640000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7002\n",
      "200 episodes - episode_reward: 135.010 [-50.000, 242.000] - loss: 148.241 - mae: 110.939 - mean_q: 120.119\n",
      "\n",
      "Interval 366 (3650000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8328\n",
      "200 episodes - episode_reward: 141.640 [22.000, 228.000] - loss: 150.051 - mae: 111.388 - mean_q: 120.678\n",
      "\n",
      "Interval 367 (3660000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7332\n",
      "200 episodes - episode_reward: 136.660 [-40.000, 244.000] - loss: 149.447 - mae: 110.891 - mean_q: 120.152\n",
      "\n",
      "Interval 368 (3670000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.9480\n",
      "200 episodes - episode_reward: 147.400 [0.000, 276.000] - loss: 149.639 - mae: 111.829 - mean_q: 121.308\n",
      "\n",
      "Interval 369 (3680000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7400\n",
      "200 episodes - episode_reward: 137.000 [-2.000, 282.000] - loss: 155.698 - mae: 115.394 - mean_q: 124.929\n",
      "\n",
      "Interval 370 (3690000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.5592\n",
      "200 episodes - episode_reward: 127.960 [10.000, 220.000] - loss: 161.428 - mae: 115.317 - mean_q: 124.937\n",
      "\n",
      "Interval 371 (3700000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8128\n",
      "200 episodes - episode_reward: 140.640 [0.000, 238.000] - loss: 155.521 - mae: 113.645 - mean_q: 123.129\n",
      "\n",
      "Interval 372 (3710000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6944\n",
      "200 episodes - episode_reward: 134.720 [-58.000, 236.000] - loss: 156.855 - mae: 113.873 - mean_q: 123.471\n",
      "\n",
      "Interval 373 (3720000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7802\n",
      "200 episodes - episode_reward: 139.010 [12.000, 244.000] - loss: 158.666 - mae: 115.971 - mean_q: 125.739\n",
      "\n",
      "Interval 374 (3730000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6728\n",
      "200 episodes - episode_reward: 133.640 [-48.000, 244.000] - loss: 160.525 - mae: 116.808 - mean_q: 126.577\n",
      "\n",
      "Interval 375 (3740000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6286\n",
      "200 episodes - episode_reward: 131.430 [-40.000, 226.000] - loss: 155.887 - mae: 115.035 - mean_q: 124.660\n",
      "\n",
      "Interval 376 (3750000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6426\n",
      "200 episodes - episode_reward: 132.130 [-36.000, 250.000] - loss: 157.268 - mae: 114.586 - mean_q: 124.206\n",
      "\n",
      "Interval 377 (3760000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.5262\n",
      "200 episodes - episode_reward: 126.310 [-40.000, 230.000] - loss: 153.209 - mae: 111.499 - mean_q: 120.807\n",
      "\n",
      "Interval 378 (3770000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6096\n",
      "200 episodes - episode_reward: 130.480 [-42.000, 240.000] - loss: 148.697 - mae: 112.538 - mean_q: 121.937\n",
      "\n",
      "Interval 379 (3780000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7702\n",
      "200 episodes - episode_reward: 138.510 [-22.000, 268.000] - loss: 151.959 - mae: 112.134 - mean_q: 121.300\n",
      "\n",
      "Interval 380 (3790000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8220\n",
      "200 episodes - episode_reward: 141.100 [-22.000, 234.000] - loss: 149.708 - mae: 110.926 - mean_q: 120.188\n",
      "\n",
      "Interval 381 (3800000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7558\n",
      "200 episodes - episode_reward: 137.790 [-52.000, 254.000] - loss: 152.210 - mae: 113.599 - mean_q: 122.902\n",
      "\n",
      "Interval 382 (3810000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8734\n",
      "200 episodes - episode_reward: 143.670 [-6.000, 272.000] - loss: 154.566 - mae: 112.423 - mean_q: 121.811\n",
      "\n",
      "Interval 383 (3820000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7240\n",
      "200 episodes - episode_reward: 136.200 [-28.000, 234.000] - loss: 157.121 - mae: 114.265 - mean_q: 123.900\n",
      "\n",
      "Interval 384 (3830000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7928\n",
      "200 episodes - episode_reward: 139.640 [6.000, 252.000] - loss: 158.084 - mae: 115.326 - mean_q: 124.953\n",
      "\n",
      "Interval 385 (3840000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6812\n",
      "200 episodes - episode_reward: 134.060 [-42.000, 238.000] - loss: 154.539 - mae: 113.999 - mean_q: 123.367\n",
      "\n",
      "Interval 386 (3850000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6522\n",
      "200 episodes - episode_reward: 132.610 [-16.000, 236.000] - loss: 155.627 - mae: 113.706 - mean_q: 122.951\n",
      "\n",
      "Interval 387 (3860000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6460\n",
      "200 episodes - episode_reward: 132.300 [-40.000, 272.000] - loss: 152.766 - mae: 112.851 - mean_q: 122.107\n",
      "\n",
      "Interval 388 (3870000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7196\n",
      "200 episodes - episode_reward: 135.980 [-50.000, 250.000] - loss: 152.711 - mae: 112.488 - mean_q: 121.836\n",
      "\n",
      "Interval 389 (3880000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7376\n",
      "200 episodes - episode_reward: 136.880 [-22.000, 262.000] - loss: 151.249 - mae: 111.744 - mean_q: 121.056\n",
      "\n",
      "Interval 390 (3890000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.9778\n",
      "200 episodes - episode_reward: 148.890 [54.000, 302.000] - loss: 155.149 - mae: 115.082 - mean_q: 124.698\n",
      "\n",
      "Interval 391 (3900000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7222\n",
      "200 episodes - episode_reward: 136.110 [2.000, 256.000] - loss: 163.013 - mae: 115.748 - mean_q: 125.346\n",
      "\n",
      "Interval 392 (3910000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.9094\n",
      "200 episodes - episode_reward: 145.470 [-18.000, 236.000] - loss: 160.333 - mae: 116.336 - mean_q: 125.967\n",
      "\n",
      "Interval 393 (3920000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.5602\n",
      "200 episodes - episode_reward: 128.010 [-50.000, 222.000] - loss: 165.619 - mae: 116.041 - mean_q: 125.655\n",
      "\n",
      "Interval 394 (3930000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8508\n",
      "200 episodes - episode_reward: 142.540 [12.000, 246.000] - loss: 158.525 - mae: 115.059 - mean_q: 124.454\n",
      "\n",
      "Interval 395 (3940000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7234\n",
      "200 episodes - episode_reward: 136.170 [-42.000, 250.000] - loss: 159.504 - mae: 115.298 - mean_q: 124.694\n",
      "\n",
      "Interval 396 (3950000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6530\n",
      "200 episodes - episode_reward: 132.650 [-26.000, 254.000] - loss: 158.632 - mae: 115.781 - mean_q: 125.152\n",
      "\n",
      "Interval 397 (3960000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.5806\n",
      "200 episodes - episode_reward: 129.030 [-40.000, 260.000] - loss: 153.297 - mae: 113.397 - mean_q: 122.488\n",
      "\n",
      "Interval 398 (3970000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.4904\n",
      "200 episodes - episode_reward: 124.520 [-52.000, 276.000] - loss: 149.415 - mae: 109.746 - mean_q: 118.754\n",
      "\n",
      "Interval 399 (3980000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 1.5378\n",
      "200 episodes - episode_reward: 76.890 [-42.000, 220.000] - loss: 163.071 - mae: 117.750 - mean_q: 127.522\n",
      "\n",
      "Interval 400 (3990000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.5004\n",
      "200 episodes - episode_reward: 125.020 [0.000, 252.000] - loss: 143.881 - mae: 109.262 - mean_q: 118.171\n",
      "\n",
      "Interval 401 (4000000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.1952\n",
      "200 episodes - episode_reward: 109.760 [-24.000, 256.000] - loss: 143.125 - mae: 109.873 - mean_q: 118.835\n",
      "\n",
      "Interval 402 (4010000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.3766\n",
      "200 episodes - episode_reward: 118.830 [-30.000, 244.000] - loss: 135.131 - mae: 105.633 - mean_q: 114.307\n",
      "\n",
      "Interval 403 (4020000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.5406\n",
      "200 episodes - episode_reward: 127.030 [-38.000, 258.000] - loss: 129.643 - mae: 105.104 - mean_q: 113.780\n",
      "\n",
      "Interval 404 (4030000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.5618\n",
      "200 episodes - episode_reward: 128.090 [-4.000, 276.000] - loss: 136.304 - mae: 106.631 - mean_q: 115.410\n",
      "\n",
      "Interval 405 (4040000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6542\n",
      "200 episodes - episode_reward: 132.710 [-50.000, 258.000] - loss: 136.114 - mae: 105.551 - mean_q: 114.267\n",
      "\n",
      "Interval 406 (4050000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6194\n",
      "200 episodes - episode_reward: 130.970 [-28.000, 236.000] - loss: 136.850 - mae: 105.236 - mean_q: 113.835\n",
      "\n",
      "Interval 407 (4060000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7336\n",
      "200 episodes - episode_reward: 136.680 [-16.000, 234.000] - loss: 137.685 - mae: 106.307 - mean_q: 114.806\n",
      "\n",
      "Interval 408 (4070000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.5030\n",
      "200 episodes - episode_reward: 125.150 [-50.000, 244.000] - loss: 140.642 - mae: 107.765 - mean_q: 116.502\n",
      "\n",
      "Interval 409 (4080000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6464\n",
      "200 episodes - episode_reward: 132.320 [-40.000, 252.000] - loss: 139.985 - mae: 108.383 - mean_q: 116.983\n",
      "\n",
      "Interval 410 (4090000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.4082\n",
      "200 episodes - episode_reward: 120.410 [-16.000, 252.000] - loss: 138.154 - mae: 107.830 - mean_q: 116.434\n",
      "\n",
      "Interval 411 (4100000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6196\n",
      "200 episodes - episode_reward: 130.980 [-12.000, 256.000] - loss: 137.790 - mae: 107.126 - mean_q: 115.704\n",
      "\n",
      "Interval 412 (4110000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.5370\n",
      "200 episodes - episode_reward: 126.850 [0.000, 246.000] - loss: 136.668 - mae: 107.130 - mean_q: 115.591\n",
      "\n",
      "Interval 413 (4120000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6036\n",
      "200 episodes - episode_reward: 130.180 [6.000, 238.000] - loss: 138.724 - mae: 107.705 - mean_q: 116.212\n",
      "\n",
      "Interval 414 (4130000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.5934\n",
      "200 episodes - episode_reward: 129.670 [-8.000, 266.000] - loss: 140.693 - mae: 108.757 - mean_q: 117.377\n",
      "\n",
      "Interval 415 (4140000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7134\n",
      "200 episodes - episode_reward: 135.670 [24.000, 252.000] - loss: 145.495 - mae: 110.836 - mean_q: 119.739\n",
      "\n",
      "Interval 416 (4150000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6986\n",
      "200 episodes - episode_reward: 134.930 [-30.000, 256.000] - loss: 153.090 - mae: 112.556 - mean_q: 121.579\n",
      "\n",
      "Interval 417 (4160000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6292\n",
      "200 episodes - episode_reward: 131.460 [8.000, 236.000] - loss: 154.147 - mae: 113.177 - mean_q: 122.415\n",
      "\n",
      "Interval 418 (4170000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7606\n",
      "200 episodes - episode_reward: 138.030 [-32.000, 234.000] - loss: 151.422 - mae: 111.955 - mean_q: 121.244\n",
      "\n",
      "Interval 419 (4180000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6448\n",
      "200 episodes - episode_reward: 132.240 [-6.000, 250.000] - loss: 155.998 - mae: 114.731 - mean_q: 124.082\n",
      "\n",
      "Interval 420 (4190000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6014\n",
      "200 episodes - episode_reward: 130.070 [-26.000, 240.000] - loss: 150.591 - mae: 111.897 - mean_q: 120.941\n",
      "\n",
      "Interval 421 (4200000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6924\n",
      "200 episodes - episode_reward: 134.620 [-26.000, 270.000] - loss: 148.502 - mae: 112.027 - mean_q: 121.244\n",
      "\n",
      "Interval 422 (4210000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6620\n",
      "200 episodes - episode_reward: 133.100 [-30.000, 236.000] - loss: 152.536 - mae: 112.078 - mean_q: 121.218\n",
      "\n",
      "Interval 423 (4220000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.3804\n",
      "200 episodes - episode_reward: 119.020 [-62.000, 248.000] - loss: 149.148 - mae: 111.402 - mean_q: 120.407\n",
      "\n",
      "Interval 424 (4230000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7362\n",
      "200 episodes - episode_reward: 136.810 [20.000, 242.000] - loss: 142.027 - mae: 109.053 - mean_q: 118.063\n",
      "\n",
      "Interval 425 (4240000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7794\n",
      "200 episodes - episode_reward: 138.970 [-12.000, 232.000] - loss: 149.270 - mae: 110.449 - mean_q: 119.752\n",
      "\n",
      "Interval 426 (4250000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.9330\n",
      "200 episodes - episode_reward: 146.650 [34.000, 268.000] - loss: 151.891 - mae: 110.966 - mean_q: 120.455\n",
      "\n",
      "Interval 427 (4260000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8750\n",
      "200 episodes - episode_reward: 143.750 [-42.000, 268.000] - loss: 154.804 - mae: 113.486 - mean_q: 123.123\n",
      "\n",
      "Interval 428 (4270000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8784\n",
      "200 episodes - episode_reward: 143.920 [10.000, 246.000] - loss: 159.726 - mae: 115.395 - mean_q: 125.030\n",
      "\n",
      "Interval 429 (4280000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6502\n",
      "200 episodes - episode_reward: 132.510 [-2.000, 252.000] - loss: 163.916 - mae: 116.694 - mean_q: 126.258\n",
      "\n",
      "Interval 430 (4290000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8116\n",
      "200 episodes - episode_reward: 140.580 [-52.000, 228.000] - loss: 159.639 - mae: 116.340 - mean_q: 125.648\n",
      "\n",
      "Interval 431 (4300000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6314\n",
      "200 episodes - episode_reward: 131.570 [-40.000, 262.000] - loss: 154.336 - mae: 114.310 - mean_q: 123.225\n",
      "\n",
      "Interval 432 (4310000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7462\n",
      "200 episodes - episode_reward: 137.310 [0.000, 244.000] - loss: 153.803 - mae: 114.118 - mean_q: 122.920\n",
      "\n",
      "Interval 433 (4320000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7778\n",
      "200 episodes - episode_reward: 138.890 [-20.000, 234.000] - loss: 152.500 - mae: 112.800 - mean_q: 121.779\n",
      "\n",
      "Interval 434 (4330000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8480\n",
      "200 episodes - episode_reward: 142.400 [-10.000, 262.000] - loss: 154.380 - mae: 113.798 - mean_q: 122.938\n",
      "\n",
      "Interval 435 (4340000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8548\n",
      "200 episodes - episode_reward: 142.740 [-22.000, 256.000] - loss: 156.735 - mae: 113.907 - mean_q: 123.079\n",
      "\n",
      "Interval 436 (4350000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7570\n",
      "200 episodes - episode_reward: 137.850 [-8.000, 296.000] - loss: 161.330 - mae: 115.128 - mean_q: 124.442\n",
      "\n",
      "Interval 437 (4360000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8226\n",
      "200 episodes - episode_reward: 141.130 [-8.000, 244.000] - loss: 157.635 - mae: 114.242 - mean_q: 123.423\n",
      "\n",
      "Interval 438 (4370000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8548\n",
      "200 episodes - episode_reward: 142.740 [-30.000, 260.000] - loss: 154.194 - mae: 113.397 - mean_q: 122.362\n",
      "\n",
      "Interval 439 (4380000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6746\n",
      "200 episodes - episode_reward: 133.730 [-40.000, 262.000] - loss: 155.692 - mae: 113.741 - mean_q: 122.772\n",
      "\n",
      "Interval 440 (4390000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7124\n",
      "200 episodes - episode_reward: 135.620 [8.000, 244.000] - loss: 153.982 - mae: 114.037 - mean_q: 122.985\n",
      "\n",
      "Interval 441 (4400000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.2716\n",
      "200 episodes - episode_reward: 113.580 [-50.000, 266.000] - loss: 157.428 - mae: 115.747 - mean_q: 124.889\n",
      "\n",
      "Interval 442 (4410000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8286\n",
      "200 episodes - episode_reward: 141.430 [-34.000, 244.000] - loss: 155.619 - mae: 113.304 - mean_q: 122.500\n",
      "\n",
      "Interval 443 (4420000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8746\n",
      "200 episodes - episode_reward: 143.730 [-4.000, 254.000] - loss: 153.251 - mae: 112.807 - mean_q: 121.903\n",
      "\n",
      "Interval 444 (4430000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8968\n",
      "200 episodes - episode_reward: 144.840 [12.000, 252.000] - loss: 156.142 - mae: 114.456 - mean_q: 123.694\n",
      "\n",
      "Interval 445 (4440000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7198\n",
      "200 episodes - episode_reward: 135.990 [-20.000, 244.000] - loss: 159.742 - mae: 116.891 - mean_q: 126.178\n",
      "\n",
      "Interval 446 (4450000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8272: 0s - rew\n",
      "200 episodes - episode_reward: 141.360 [8.000, 238.000] - loss: 160.844 - mae: 115.448 - mean_q: 124.643\n",
      "\n",
      "Interval 447 (4460000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.2674\n",
      "200 episodes - episode_reward: 113.370 [-14.000, 232.000] - loss: 166.034 - mae: 118.580 - mean_q: 127.973\n",
      "\n",
      "Interval 448 (4470000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 52s 5ms/step - reward: 1.7630\n",
      "200 episodes - episode_reward: 88.150 [-50.000, 232.000] - loss: 168.413 - mae: 120.655 - mean_q: 129.850\n",
      "\n",
      "Interval 449 (4480000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.3992\n",
      "200 episodes - episode_reward: 119.960 [-30.000, 248.000] - loss: 143.629 - mae: 109.790 - mean_q: 118.452\n",
      "\n",
      "Interval 450 (4490000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.3642\n",
      "200 episodes - episode_reward: 118.210 [-58.000, 244.000] - loss: 133.005 - mae: 105.813 - mean_q: 114.321\n",
      "\n",
      "Interval 451 (4500000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.5222\n",
      "200 episodes - episode_reward: 126.110 [10.000, 230.000] - loss: 124.975 - mae: 102.167 - mean_q: 110.433\n",
      "\n",
      "Interval 452 (4510000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.4774\n",
      "200 episodes - episode_reward: 123.870 [-38.000, 264.000] - loss: 124.050 - mae: 100.509 - mean_q: 108.809\n",
      "\n",
      "Interval 453 (4520000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.4506\n",
      "200 episodes - episode_reward: 122.530 [-36.000, 260.000] - loss: 129.289 - mae: 101.980 - mean_q: 110.316\n",
      "\n",
      "Interval 454 (4530000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.3640\n",
      "200 episodes - episode_reward: 118.200 [-36.000, 228.000] - loss: 129.668 - mae: 102.841 - mean_q: 111.225\n",
      "\n",
      "Interval 455 (4540000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.4764\n",
      "200 episodes - episode_reward: 123.820 [-18.000, 236.000] - loss: 131.791 - mae: 104.566 - mean_q: 113.105\n",
      "\n",
      "Interval 456 (4550000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6796\n",
      "200 episodes - episode_reward: 133.980 [-12.000, 240.000] - loss: 131.913 - mae: 104.616 - mean_q: 113.136\n",
      "\n",
      "Interval 457 (4560000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7004\n",
      "200 episodes - episode_reward: 135.020 [-42.000, 260.000] - loss: 138.696 - mae: 106.266 - mean_q: 114.833\n",
      "\n",
      "Interval 458 (4570000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7776\n",
      "200 episodes - episode_reward: 138.880 [8.000, 240.000] - loss: 138.437 - mae: 106.389 - mean_q: 114.984\n",
      "\n",
      "Interval 459 (4580000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6262\n",
      "200 episodes - episode_reward: 131.310 [-22.000, 240.000] - loss: 143.732 - mae: 109.347 - mean_q: 118.248\n",
      "\n",
      "Interval 460 (4590000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6324\n",
      "200 episodes - episode_reward: 131.620 [-12.000, 250.000] - loss: 149.552 - mae: 111.532 - mean_q: 120.799\n",
      "\n",
      "Interval 461 (4600000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8212\n",
      "200 episodes - episode_reward: 141.060 [-2.000, 252.000] - loss: 149.909 - mae: 111.756 - mean_q: 120.912\n",
      "\n",
      "Interval 462 (4610000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8372\n",
      "200 episodes - episode_reward: 141.860 [22.000, 258.000] - loss: 151.616 - mae: 114.243 - mean_q: 123.611\n",
      "\n",
      "Interval 463 (4620000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7208\n",
      "200 episodes - episode_reward: 136.040 [-20.000, 248.000] - loss: 160.148 - mae: 114.856 - mean_q: 124.302\n",
      "\n",
      "Interval 464 (4630000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7028\n",
      "200 episodes - episode_reward: 135.140 [-12.000, 254.000] - loss: 160.931 - mae: 116.640 - mean_q: 126.272\n",
      "\n",
      "Interval 465 (4640000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6854\n",
      "200 episodes - episode_reward: 134.270 [-18.000, 244.000] - loss: 163.388 - mae: 116.986 - mean_q: 126.694\n",
      "\n",
      "Interval 466 (4650000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7900\n",
      "200 episodes - episode_reward: 139.500 [-8.000, 266.000] - loss: 159.553 - mae: 115.557 - mean_q: 125.030\n",
      "\n",
      "Interval 467 (4660000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6458\n",
      "200 episodes - episode_reward: 132.290 [-58.000, 244.000] - loss: 158.399 - mae: 115.217 - mean_q: 124.797\n",
      "\n",
      "Interval 468 (4670000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6622\n",
      "200 episodes - episode_reward: 133.110 [-8.000, 256.000] - loss: 156.797 - mae: 114.614 - mean_q: 124.190\n",
      "\n",
      "Interval 469 (4680000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6908\n",
      "200 episodes - episode_reward: 134.540 [-8.000, 230.000] - loss: 159.647 - mae: 114.506 - mean_q: 124.058\n",
      "\n",
      "Interval 470 (4690000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6628\n",
      "200 episodes - episode_reward: 133.140 [24.000, 258.000] - loss: 155.425 - mae: 113.938 - mean_q: 123.415\n",
      "\n",
      "Interval 471 (4700000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8300\n",
      "200 episodes - episode_reward: 141.500 [8.000, 268.000] - loss: 160.488 - mae: 114.109 - mean_q: 123.939\n",
      "\n",
      "Interval 472 (4710000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.9602\n",
      "200 episodes - episode_reward: 148.010 [-2.000, 230.000] - loss: 160.303 - mae: 114.934 - mean_q: 124.643\n",
      "\n",
      "Interval 473 (4720000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6698\n",
      "200 episodes - episode_reward: 133.490 [-70.000, 246.000] - loss: 159.971 - mae: 114.513 - mean_q: 124.110\n",
      "\n",
      "Interval 474 (4730000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.9594\n",
      "200 episodes - episode_reward: 147.970 [32.000, 268.000] - loss: 155.090 - mae: 113.534 - mean_q: 123.038\n",
      "\n",
      "Interval 475 (4740000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8756\n",
      "200 episodes - episode_reward: 143.780 [-26.000, 256.000] - loss: 155.150 - mae: 112.038 - mean_q: 121.248\n",
      "\n",
      "Interval 476 (4750000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8316\n",
      "200 episodes - episode_reward: 141.580 [-2.000, 266.000] - loss: 158.781 - mae: 114.991 - mean_q: 124.527\n",
      "\n",
      "Interval 477 (4760000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6966\n",
      "200 episodes - episode_reward: 134.830 [-34.000, 290.000] - loss: 159.654 - mae: 115.593 - mean_q: 125.170\n",
      "\n",
      "Interval 478 (4770000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7532\n",
      "200 episodes - episode_reward: 137.660 [-30.000, 250.000] - loss: 157.118 - mae: 116.029 - mean_q: 125.549\n",
      "\n",
      "Interval 479 (4780000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6364\n",
      "200 episodes - episode_reward: 131.820 [-28.000, 220.000] - loss: 156.250 - mae: 113.759 - mean_q: 123.009\n",
      "\n",
      "Interval 480 (4790000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7440\n",
      "200 episodes - episode_reward: 137.200 [-2.000, 240.000] - loss: 149.602 - mae: 112.058 - mean_q: 121.160\n",
      "\n",
      "Interval 481 (4800000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.5762\n",
      "200 episodes - episode_reward: 128.810 [-46.000, 264.000] - loss: 150.968 - mae: 111.798 - mean_q: 120.746\n",
      "\n",
      "Interval 482 (4810000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.4642\n",
      "200 episodes - episode_reward: 123.210 [-40.000, 232.000] - loss: 149.507 - mae: 111.424 - mean_q: 120.298\n",
      "\n",
      "Interval 483 (4820000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.3308\n",
      "200 episodes - episode_reward: 116.540 [-36.000, 240.000] - loss: 142.072 - mae: 107.808 - mean_q: 116.591\n",
      "\n",
      "Interval 484 (4830000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.2520\n",
      "200 episodes - episode_reward: 112.600 [-40.000, 216.000] - loss: 140.250 - mae: 106.860 - mean_q: 115.688\n",
      "\n",
      "Interval 485 (4840000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8556\n",
      "200 episodes - episode_reward: 142.780 [-50.000, 246.000] - loss: 139.904 - mae: 107.288 - mean_q: 116.464\n",
      "\n",
      "Interval 486 (4850000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 3.0362\n",
      "200 episodes - episode_reward: 151.810 [34.000, 274.000] - loss: 146.263 - mae: 109.165 - mean_q: 118.502\n",
      "\n",
      "Interval 487 (4860000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.9036\n",
      "200 episodes - episode_reward: 145.180 [-12.000, 250.000] - loss: 151.269 - mae: 112.686 - mean_q: 122.281\n",
      "\n",
      "Interval 488 (4870000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6512\n",
      "200 episodes - episode_reward: 132.560 [-40.000, 268.000] - loss: 160.915 - mae: 116.622 - mean_q: 126.323\n",
      "\n",
      "Interval 489 (4880000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 2.8400\n",
      "200 episodes - episode_reward: 142.000 [52.000, 246.000] - loss: 163.119 - mae: 117.634 - mean_q: 127.287\n",
      "\n",
      "Interval 490 (4890000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7904\n",
      "200 episodes - episode_reward: 139.520 [-50.000, 258.000] - loss: 160.698 - mae: 115.649 - mean_q: 125.045\n",
      "\n",
      "Interval 491 (4900000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8258\n",
      "200 episodes - episode_reward: 141.290 [30.000, 304.000] - loss: 153.917 - mae: 113.922 - mean_q: 123.067\n",
      "\n",
      "Interval 492 (4910000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.9142\n",
      "200 episodes - episode_reward: 145.710 [2.000, 266.000] - loss: 152.922 - mae: 113.090 - mean_q: 122.169\n",
      "\n",
      "Interval 493 (4920000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7506\n",
      "200 episodes - episode_reward: 137.530 [-50.000, 238.000] - loss: 154.629 - mae: 113.084 - mean_q: 122.300\n",
      "\n",
      "Interval 494 (4930000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8272\n",
      "200 episodes - episode_reward: 141.360 [2.000, 252.000] - loss: 153.054 - mae: 111.095 - mean_q: 120.207\n",
      "\n",
      "Interval 495 (4940000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6486\n",
      "200 episodes - episode_reward: 132.430 [-54.000, 244.000] - loss: 153.107 - mae: 113.261 - mean_q: 122.619\n",
      "\n",
      "Interval 496 (4950000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7310\n",
      "200 episodes - episode_reward: 136.550 [14.000, 242.000] - loss: 154.202 - mae: 113.149 - mean_q: 122.247\n",
      "\n",
      "Interval 497 (4960000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6784\n",
      "200 episodes - episode_reward: 133.920 [-42.000, 248.000] - loss: 148.898 - mae: 112.166 - mean_q: 121.249\n",
      "\n",
      "Interval 498 (4970000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7056\n",
      "200 episodes - episode_reward: 135.280 [-46.000, 260.000] - loss: 145.485 - mae: 109.995 - mean_q: 118.842\n",
      "\n",
      "Interval 499 (4980000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6438\n",
      "200 episodes - episode_reward: 132.190 [-30.000, 282.000] - loss: 144.705 - mae: 107.380 - mean_q: 116.069\n",
      "\n",
      "Interval 500 (4990000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.5536\n",
      "done, took 27317.211 seconds\n"
     ]
    }
   ],
   "source": [
    "history = dqn.fit(env, nb_steps=5*MILLION, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "429ea2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = history.history\n",
    "data['episode_reward'] = [float(v) for v in data['episode_reward']]\n",
    "data['nb_episode_steps'] = [int(v) for v in data['nb_episode_steps']]\n",
    "data['nb_steps'] = [int(v) for v in data['nb_steps']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a94e1373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'episode_reward': [-50.0,\n",
       "  -50.0,\n",
       "  -36.0,\n",
       "  -40.0,\n",
       "  -40.0,\n",
       "  -40.0,\n",
       "  -50.0,\n",
       "  -50.0,\n",
       "  -42.0,\n",
       "  -26.0,\n",
       "  -46.0,\n",
       "  -40.0,\n",
       "  -52.0,\n",
       "  -20.0,\n",
       "  -40.0,\n",
       "  -6.0,\n",
       "  20.0,\n",
       "  -50.0,\n",
       "  -12.0,\n",
       "  -40.0,\n",
       "  -50.0,\n",
       "  -20.0,\n",
       "  -50.0,\n",
       "  -40.0,\n",
       "  -32.0,\n",
       "  -40.0,\n",
       "  -6.0,\n",
       "  -46.0,\n",
       "  -46.0,\n",
       "  -50.0,\n",
       "  -22.0,\n",
       "  -6.0,\n",
       "  -26.0,\n",
       "  -40.0,\n",
       "  -32.0,\n",
       "  -40.0,\n",
       "  -26.0,\n",
       "  -40.0,\n",
       "  -40.0,\n",
       "  -46.0,\n",
       "  -32.0,\n",
       "  -2.0,\n",
       "  -20.0,\n",
       "  -30.0,\n",
       "  -30.0,\n",
       "  -36.0,\n",
       "  -30.0,\n",
       "  -50.0,\n",
       "  -40.0,\n",
       "  14.0,\n",
       "  -30.0,\n",
       "  -50.0,\n",
       "  -54.0,\n",
       "  -16.0,\n",
       "  -26.0,\n",
       "  -46.0,\n",
       "  -8.0,\n",
       "  -50.0,\n",
       "  -22.0,\n",
       "  -50.0,\n",
       "  -40.0,\n",
       "  -44.0,\n",
       "  -46.0,\n",
       "  -8.0,\n",
       "  -24.0,\n",
       "  -50.0,\n",
       "  18.0,\n",
       "  -28.0,\n",
       "  -10.0,\n",
       "  -50.0,\n",
       "  10.0,\n",
       "  -50.0,\n",
       "  -50.0,\n",
       "  -32.0,\n",
       "  -36.0,\n",
       "  -52.0,\n",
       "  0.0,\n",
       "  -40.0,\n",
       "  -40.0,\n",
       "  -42.0,\n",
       "  -40.0,\n",
       "  -40.0,\n",
       "  -40.0,\n",
       "  -40.0,\n",
       "  -26.0,\n",
       "  -50.0,\n",
       "  -50.0,\n",
       "  -50.0,\n",
       "  -30.0,\n",
       "  -50.0,\n",
       "  -50.0,\n",
       "  -20.0,\n",
       "  -26.0,\n",
       "  -50.0,\n",
       "  -30.0,\n",
       "  -50.0,\n",
       "  -2.0,\n",
       "  -38.0,\n",
       "  -50.0,\n",
       "  -36.0,\n",
       "  -36.0,\n",
       "  20.0,\n",
       "  -50.0,\n",
       "  -20.0,\n",
       "  -50.0,\n",
       "  -40.0,\n",
       "  -50.0,\n",
       "  -8.0,\n",
       "  -2.0,\n",
       "  -40.0,\n",
       "  -40.0,\n",
       "  -36.0,\n",
       "  -36.0,\n",
       "  -50.0,\n",
       "  -34.0,\n",
       "  -40.0,\n",
       "  -50.0,\n",
       "  -50.0,\n",
       "  -36.0,\n",
       "  -20.0,\n",
       "  -38.0,\n",
       "  -40.0,\n",
       "  -40.0,\n",
       "  4.0,\n",
       "  -42.0,\n",
       "  -36.0,\n",
       "  -50.0,\n",
       "  -36.0,\n",
       "  -50.0,\n",
       "  -28.0,\n",
       "  -20.0,\n",
       "  -36.0,\n",
       "  -40.0,\n",
       "  -50.0,\n",
       "  -30.0,\n",
       "  -42.0,\n",
       "  -50.0,\n",
       "  -40.0,\n",
       "  -36.0,\n",
       "  -50.0,\n",
       "  -48.0,\n",
       "  -30.0,\n",
       "  -40.0,\n",
       "  -22.0,\n",
       "  -20.0,\n",
       "  -46.0,\n",
       "  -50.0,\n",
       "  -40.0,\n",
       "  -20.0,\n",
       "  -16.0,\n",
       "  -16.0,\n",
       "  -14.0,\n",
       "  -40.0,\n",
       "  -42.0,\n",
       "  -30.0,\n",
       "  -46.0,\n",
       "  -40.0,\n",
       "  -50.0,\n",
       "  -30.0,\n",
       "  -40.0,\n",
       "  -22.0,\n",
       "  -40.0,\n",
       "  -36.0,\n",
       "  -34.0,\n",
       "  -2.0,\n",
       "  -22.0,\n",
       "  -14.0,\n",
       "  14.0,\n",
       "  -12.0,\n",
       "  -40.0,\n",
       "  -40.0,\n",
       "  4.0,\n",
       "  -38.0,\n",
       "  -40.0,\n",
       "  -10.0,\n",
       "  -8.0,\n",
       "  -22.0,\n",
       "  -58.0,\n",
       "  -4.0,\n",
       "  -52.0,\n",
       "  -30.0,\n",
       "  -40.0,\n",
       "  -16.0,\n",
       "  -50.0,\n",
       "  -26.0,\n",
       "  -22.0,\n",
       "  22.0,\n",
       "  26.0,\n",
       "  -32.0,\n",
       "  -36.0,\n",
       "  -36.0,\n",
       "  -24.0,\n",
       "  -50.0,\n",
       "  -50.0,\n",
       "  -36.0,\n",
       "  8.0,\n",
       "  -30.0,\n",
       "  0.0,\n",
       "  -42.0,\n",
       "  -36.0,\n",
       "  -46.0,\n",
       "  6.0,\n",
       "  -10.0,\n",
       "  -48.0,\n",
       "  -46.0,\n",
       "  -46.0,\n",
       "  -20.0,\n",
       "  -40.0,\n",
       "  -40.0,\n",
       "  16.0,\n",
       "  -12.0,\n",
       "  -42.0,\n",
       "  -8.0,\n",
       "  -40.0,\n",
       "  34.0,\n",
       "  -10.0,\n",
       "  -38.0,\n",
       "  10.0,\n",
       "  -28.0,\n",
       "  -50.0,\n",
       "  -30.0,\n",
       "  -36.0,\n",
       "  -50.0,\n",
       "  14.0,\n",
       "  -18.0,\n",
       "  14.0,\n",
       "  -40.0,\n",
       "  -8.0,\n",
       "  -36.0,\n",
       "  -40.0,\n",
       "  -40.0,\n",
       "  -30.0,\n",
       "  -30.0,\n",
       "  -50.0,\n",
       "  8.0,\n",
       "  38.0,\n",
       "  -2.0,\n",
       "  70.0,\n",
       "  -50.0,\n",
       "  -36.0,\n",
       "  -40.0,\n",
       "  -8.0,\n",
       "  -28.0,\n",
       "  -50.0,\n",
       "  24.0,\n",
       "  -24.0,\n",
       "  -36.0,\n",
       "  24.0,\n",
       "  -22.0,\n",
       "  -50.0,\n",
       "  -12.0,\n",
       "  -6.0,\n",
       "  0.0,\n",
       "  -30.0,\n",
       "  6.0,\n",
       "  18.0,\n",
       "  -50.0,\n",
       "  -30.0,\n",
       "  -50.0,\n",
       "  -16.0,\n",
       "  -14.0,\n",
       "  -20.0,\n",
       "  -10.0,\n",
       "  -50.0,\n",
       "  -40.0,\n",
       "  -16.0,\n",
       "  -20.0,\n",
       "  18.0,\n",
       "  -16.0,\n",
       "  44.0,\n",
       "  86.0,\n",
       "  0.0,\n",
       "  -42.0,\n",
       "  64.0,\n",
       "  -16.0,\n",
       "  66.0,\n",
       "  -40.0,\n",
       "  46.0,\n",
       "  12.0,\n",
       "  40.0,\n",
       "  -46.0,\n",
       "  -40.0,\n",
       "  -6.0,\n",
       "  20.0,\n",
       "  -2.0,\n",
       "  -50.0,\n",
       "  -6.0,\n",
       "  -50.0,\n",
       "  -50.0,\n",
       "  4.0,\n",
       "  -42.0,\n",
       "  -4.0,\n",
       "  78.0,\n",
       "  -50.0,\n",
       "  -28.0,\n",
       "  4.0,\n",
       "  -50.0,\n",
       "  -46.0,\n",
       "  24.0,\n",
       "  -50.0,\n",
       "  0.0,\n",
       "  -36.0,\n",
       "  -40.0,\n",
       "  -24.0,\n",
       "  58.0,\n",
       "  20.0,\n",
       "  -2.0,\n",
       "  -36.0,\n",
       "  16.0,\n",
       "  18.0,\n",
       "  -50.0,\n",
       "  -40.0,\n",
       "  -50.0,\n",
       "  32.0,\n",
       "  -36.0,\n",
       "  -20.0,\n",
       "  52.0,\n",
       "  4.0,\n",
       "  80.0,\n",
       "  16.0,\n",
       "  -10.0,\n",
       "  -8.0,\n",
       "  -30.0,\n",
       "  40.0,\n",
       "  42.0,\n",
       "  28.0,\n",
       "  -40.0,\n",
       "  -8.0,\n",
       "  -2.0,\n",
       "  30.0,\n",
       "  88.0,\n",
       "  98.0,\n",
       "  4.0,\n",
       "  28.0,\n",
       "  68.0,\n",
       "  -42.0,\n",
       "  -22.0,\n",
       "  0.0,\n",
       "  -50.0,\n",
       "  -16.0,\n",
       "  -50.0,\n",
       "  28.0,\n",
       "  36.0,\n",
       "  4.0,\n",
       "  -50.0,\n",
       "  -28.0,\n",
       "  30.0,\n",
       "  -50.0,\n",
       "  -10.0,\n",
       "  -30.0,\n",
       "  -6.0,\n",
       "  -52.0,\n",
       "  -26.0,\n",
       "  2.0,\n",
       "  12.0,\n",
       "  30.0,\n",
       "  -46.0,\n",
       "  -2.0,\n",
       "  -54.0,\n",
       "  -46.0,\n",
       "  -30.0,\n",
       "  2.0,\n",
       "  -32.0,\n",
       "  52.0,\n",
       "  -10.0,\n",
       "  -4.0,\n",
       "  -26.0,\n",
       "  28.0,\n",
       "  -20.0,\n",
       "  -36.0,\n",
       "  -16.0,\n",
       "  -6.0,\n",
       "  -16.0,\n",
       "  2.0,\n",
       "  -30.0,\n",
       "  -30.0,\n",
       "  34.0,\n",
       "  36.0,\n",
       "  -30.0,\n",
       "  -24.0,\n",
       "  0.0,\n",
       "  80.0,\n",
       "  50.0,\n",
       "  -2.0,\n",
       "  -6.0,\n",
       "  30.0,\n",
       "  18.0,\n",
       "  -20.0,\n",
       "  -2.0,\n",
       "  4.0,\n",
       "  20.0,\n",
       "  -28.0,\n",
       "  18.0,\n",
       "  20.0,\n",
       "  -22.0,\n",
       "  100.0,\n",
       "  -44.0,\n",
       "  52.0,\n",
       "  58.0,\n",
       "  32.0,\n",
       "  10.0,\n",
       "  -30.0,\n",
       "  70.0,\n",
       "  14.0,\n",
       "  -40.0,\n",
       "  -20.0,\n",
       "  20.0,\n",
       "  -8.0,\n",
       "  34.0,\n",
       "  -22.0,\n",
       "  -40.0,\n",
       "  28.0,\n",
       "  10.0,\n",
       "  -24.0,\n",
       "  50.0,\n",
       "  36.0,\n",
       "  -30.0,\n",
       "  -20.0,\n",
       "  28.0,\n",
       "  44.0,\n",
       "  -40.0,\n",
       "  -8.0,\n",
       "  50.0,\n",
       "  8.0,\n",
       "  -8.0,\n",
       "  30.0,\n",
       "  20.0,\n",
       "  -46.0,\n",
       "  38.0,\n",
       "  84.0,\n",
       "  -8.0,\n",
       "  -50.0,\n",
       "  -34.0,\n",
       "  30.0,\n",
       "  14.0,\n",
       "  -40.0,\n",
       "  -22.0,\n",
       "  -10.0,\n",
       "  -32.0,\n",
       "  -50.0,\n",
       "  -40.0,\n",
       "  16.0,\n",
       "  16.0,\n",
       "  -8.0,\n",
       "  -2.0,\n",
       "  42.0,\n",
       "  38.0,\n",
       "  -22.0,\n",
       "  -4.0,\n",
       "  -34.0,\n",
       "  56.0,\n",
       "  18.0,\n",
       "  52.0,\n",
       "  -10.0,\n",
       "  -32.0,\n",
       "  4.0,\n",
       "  -6.0,\n",
       "  48.0,\n",
       "  -2.0,\n",
       "  -10.0,\n",
       "  -40.0,\n",
       "  28.0,\n",
       "  16.0,\n",
       "  38.0,\n",
       "  14.0,\n",
       "  -46.0,\n",
       "  64.0,\n",
       "  -10.0,\n",
       "  18.0,\n",
       "  -30.0,\n",
       "  -18.0,\n",
       "  -50.0,\n",
       "  8.0,\n",
       "  -10.0,\n",
       "  -28.0,\n",
       "  2.0,\n",
       "  24.0,\n",
       "  12.0,\n",
       "  -6.0,\n",
       "  14.0,\n",
       "  -30.0,\n",
       "  26.0,\n",
       "  -28.0,\n",
       "  -18.0,\n",
       "  -30.0,\n",
       "  0.0,\n",
       "  54.0,\n",
       "  14.0,\n",
       "  -18.0,\n",
       "  0.0,\n",
       "  -44.0,\n",
       "  -38.0,\n",
       "  -40.0,\n",
       "  18.0,\n",
       "  -26.0,\n",
       "  -38.0,\n",
       "  -8.0,\n",
       "  4.0,\n",
       "  -20.0,\n",
       "  10.0,\n",
       "  -10.0,\n",
       "  22.0,\n",
       "  -30.0,\n",
       "  72.0,\n",
       "  -30.0,\n",
       "  14.0,\n",
       "  40.0,\n",
       "  -30.0,\n",
       "  -26.0,\n",
       "  58.0,\n",
       "  28.0,\n",
       "  0.0,\n",
       "  -30.0,\n",
       "  -30.0,\n",
       "  4.0,\n",
       "  -36.0,\n",
       "  20.0,\n",
       "  -44.0,\n",
       "  80.0,\n",
       "  26.0,\n",
       "  12.0,\n",
       "  40.0,\n",
       "  14.0,\n",
       "  64.0,\n",
       "  52.0,\n",
       "  8.0,\n",
       "  -46.0,\n",
       "  -16.0,\n",
       "  -22.0,\n",
       "  10.0,\n",
       "  -28.0,\n",
       "  -26.0,\n",
       "  -14.0,\n",
       "  58.0,\n",
       "  14.0,\n",
       "  48.0,\n",
       "  0.0,\n",
       "  -30.0,\n",
       "  -50.0,\n",
       "  10.0,\n",
       "  6.0,\n",
       "  44.0,\n",
       "  32.0,\n",
       "  78.0,\n",
       "  -50.0,\n",
       "  60.0,\n",
       "  56.0,\n",
       "  104.0,\n",
       "  12.0,\n",
       "  26.0,\n",
       "  0.0,\n",
       "  4.0,\n",
       "  -2.0,\n",
       "  30.0,\n",
       "  32.0,\n",
       "  -6.0,\n",
       "  -20.0,\n",
       "  14.0,\n",
       "  68.0,\n",
       "  58.0,\n",
       "  16.0,\n",
       "  16.0,\n",
       "  20.0,\n",
       "  4.0,\n",
       "  -10.0,\n",
       "  58.0,\n",
       "  32.0,\n",
       "  20.0,\n",
       "  -26.0,\n",
       "  -40.0,\n",
       "  -26.0,\n",
       "  14.0,\n",
       "  4.0,\n",
       "  42.0,\n",
       "  -16.0,\n",
       "  70.0,\n",
       "  40.0,\n",
       "  28.0,\n",
       "  44.0,\n",
       "  60.0,\n",
       "  -40.0,\n",
       "  46.0,\n",
       "  30.0,\n",
       "  -16.0,\n",
       "  58.0,\n",
       "  26.0,\n",
       "  22.0,\n",
       "  -46.0,\n",
       "  24.0,\n",
       "  52.0,\n",
       "  8.0,\n",
       "  40.0,\n",
       "  96.0,\n",
       "  50.0,\n",
       "  -20.0,\n",
       "  70.0,\n",
       "  112.0,\n",
       "  -20.0,\n",
       "  -30.0,\n",
       "  10.0,\n",
       "  10.0,\n",
       "  120.0,\n",
       "  14.0,\n",
       "  16.0,\n",
       "  104.0,\n",
       "  -50.0,\n",
       "  100.0,\n",
       "  -10.0,\n",
       "  4.0,\n",
       "  -22.0,\n",
       "  -20.0,\n",
       "  -20.0,\n",
       "  36.0,\n",
       "  34.0,\n",
       "  102.0,\n",
       "  -44.0,\n",
       "  14.0,\n",
       "  32.0,\n",
       "  52.0,\n",
       "  48.0,\n",
       "  -52.0,\n",
       "  -30.0,\n",
       "  80.0,\n",
       "  86.0,\n",
       "  30.0,\n",
       "  -10.0,\n",
       "  66.0,\n",
       "  26.0,\n",
       "  -40.0,\n",
       "  24.0,\n",
       "  22.0,\n",
       "  90.0,\n",
       "  40.0,\n",
       "  -30.0,\n",
       "  -50.0,\n",
       "  -60.0,\n",
       "  70.0,\n",
       "  4.0,\n",
       "  58.0,\n",
       "  -20.0,\n",
       "  0.0,\n",
       "  -50.0,\n",
       "  26.0,\n",
       "  20.0,\n",
       "  14.0,\n",
       "  80.0,\n",
       "  34.0,\n",
       "  52.0,\n",
       "  24.0,\n",
       "  36.0,\n",
       "  -36.0,\n",
       "  52.0,\n",
       "  60.0,\n",
       "  -26.0,\n",
       "  46.0,\n",
       "  52.0,\n",
       "  -20.0,\n",
       "  -36.0,\n",
       "  98.0,\n",
       "  36.0,\n",
       "  34.0,\n",
       "  -26.0,\n",
       "  12.0,\n",
       "  96.0,\n",
       "  36.0,\n",
       "  -16.0,\n",
       "  50.0,\n",
       "  18.0,\n",
       "  8.0,\n",
       "  -30.0,\n",
       "  38.0,\n",
       "  6.0,\n",
       "  -2.0,\n",
       "  120.0,\n",
       "  -50.0,\n",
       "  86.0,\n",
       "  -18.0,\n",
       "  50.0,\n",
       "  54.0,\n",
       "  38.0,\n",
       "  28.0,\n",
       "  50.0,\n",
       "  22.0,\n",
       "  58.0,\n",
       "  -24.0,\n",
       "  50.0,\n",
       "  60.0,\n",
       "  96.0,\n",
       "  6.0,\n",
       "  118.0,\n",
       "  46.0,\n",
       "  42.0,\n",
       "  -20.0,\n",
       "  32.0,\n",
       "  -48.0,\n",
       "  -16.0,\n",
       "  110.0,\n",
       "  4.0,\n",
       "  24.0,\n",
       "  -8.0,\n",
       "  58.0,\n",
       "  38.0,\n",
       "  -6.0,\n",
       "  22.0,\n",
       "  18.0,\n",
       "  48.0,\n",
       "  72.0,\n",
       "  -8.0,\n",
       "  38.0,\n",
       "  70.0,\n",
       "  20.0,\n",
       "  44.0,\n",
       "  50.0,\n",
       "  68.0,\n",
       "  10.0,\n",
       "  4.0,\n",
       "  -26.0,\n",
       "  102.0,\n",
       "  72.0,\n",
       "  40.0,\n",
       "  48.0,\n",
       "  -8.0,\n",
       "  68.0,\n",
       "  -20.0,\n",
       "  94.0,\n",
       "  20.0,\n",
       "  44.0,\n",
       "  6.0,\n",
       "  -20.0,\n",
       "  -26.0,\n",
       "  34.0,\n",
       "  88.0,\n",
       "  54.0,\n",
       "  62.0,\n",
       "  40.0,\n",
       "  52.0,\n",
       "  52.0,\n",
       "  20.0,\n",
       "  50.0,\n",
       "  54.0,\n",
       "  132.0,\n",
       "  72.0,\n",
       "  34.0,\n",
       "  34.0,\n",
       "  24.0,\n",
       "  72.0,\n",
       "  74.0,\n",
       "  60.0,\n",
       "  100.0,\n",
       "  94.0,\n",
       "  44.0,\n",
       "  -10.0,\n",
       "  52.0,\n",
       "  32.0,\n",
       "  -36.0,\n",
       "  -10.0,\n",
       "  0.0,\n",
       "  74.0,\n",
       "  -14.0,\n",
       "  76.0,\n",
       "  64.0,\n",
       "  84.0,\n",
       "  50.0,\n",
       "  86.0,\n",
       "  62.0,\n",
       "  46.0,\n",
       "  -28.0,\n",
       "  66.0,\n",
       "  12.0,\n",
       "  24.0,\n",
       "  112.0,\n",
       "  8.0,\n",
       "  100.0,\n",
       "  88.0,\n",
       "  0.0,\n",
       "  -12.0,\n",
       "  -10.0,\n",
       "  -18.0,\n",
       "  112.0,\n",
       "  118.0,\n",
       "  28.0,\n",
       "  12.0,\n",
       "  -20.0,\n",
       "  -16.0,\n",
       "  44.0,\n",
       "  46.0,\n",
       "  2.0,\n",
       "  80.0,\n",
       "  -50.0,\n",
       "  28.0,\n",
       "  -30.0,\n",
       "  52.0,\n",
       "  110.0,\n",
       "  26.0,\n",
       "  46.0,\n",
       "  78.0,\n",
       "  16.0,\n",
       "  14.0,\n",
       "  -20.0,\n",
       "  74.0,\n",
       "  68.0,\n",
       "  0.0,\n",
       "  48.0,\n",
       "  72.0,\n",
       "  104.0,\n",
       "  18.0,\n",
       "  74.0,\n",
       "  120.0,\n",
       "  46.0,\n",
       "  48.0,\n",
       "  36.0,\n",
       "  -20.0,\n",
       "  68.0,\n",
       "  24.0,\n",
       "  -10.0,\n",
       "  78.0,\n",
       "  82.0,\n",
       "  88.0,\n",
       "  108.0,\n",
       "  98.0,\n",
       "  102.0,\n",
       "  -50.0,\n",
       "  -12.0,\n",
       "  42.0,\n",
       "  30.0,\n",
       "  44.0,\n",
       "  -30.0,\n",
       "  56.0,\n",
       "  0.0,\n",
       "  58.0,\n",
       "  100.0,\n",
       "  20.0,\n",
       "  68.0,\n",
       "  -16.0,\n",
       "  78.0,\n",
       "  90.0,\n",
       "  14.0,\n",
       "  16.0,\n",
       "  58.0,\n",
       "  58.0,\n",
       "  -6.0,\n",
       "  76.0,\n",
       "  -6.0,\n",
       "  6.0,\n",
       "  -30.0,\n",
       "  94.0,\n",
       "  -20.0,\n",
       "  44.0,\n",
       "  -26.0,\n",
       "  42.0,\n",
       "  -20.0,\n",
       "  38.0,\n",
       "  14.0,\n",
       "  98.0,\n",
       "  34.0,\n",
       "  42.0,\n",
       "  90.0,\n",
       "  38.0,\n",
       "  54.0,\n",
       "  40.0,\n",
       "  -24.0,\n",
       "  64.0,\n",
       "  24.0,\n",
       "  32.0,\n",
       "  30.0,\n",
       "  70.0,\n",
       "  38.0,\n",
       "  46.0,\n",
       "  10.0,\n",
       "  76.0,\n",
       "  74.0,\n",
       "  18.0,\n",
       "  114.0,\n",
       "  90.0,\n",
       "  8.0,\n",
       "  34.0,\n",
       "  28.0,\n",
       "  76.0,\n",
       "  54.0,\n",
       "  -6.0,\n",
       "  14.0,\n",
       "  4.0,\n",
       "  112.0,\n",
       "  30.0,\n",
       "  60.0,\n",
       "  88.0,\n",
       "  76.0,\n",
       "  -50.0,\n",
       "  16.0,\n",
       "  44.0,\n",
       "  112.0,\n",
       "  24.0,\n",
       "  -18.0,\n",
       "  34.0,\n",
       "  8.0,\n",
       "  -16.0,\n",
       "  98.0,\n",
       "  14.0,\n",
       "  58.0,\n",
       "  66.0,\n",
       "  -24.0,\n",
       "  -20.0,\n",
       "  46.0,\n",
       "  44.0,\n",
       "  28.0,\n",
       "  28.0,\n",
       "  -26.0,\n",
       "  96.0,\n",
       "  -30.0,\n",
       "  40.0,\n",
       "  58.0,\n",
       "  114.0,\n",
       "  92.0,\n",
       "  120.0,\n",
       "  16.0,\n",
       "  40.0,\n",
       "  30.0,\n",
       "  -10.0,\n",
       "  78.0,\n",
       "  108.0,\n",
       "  62.0,\n",
       "  82.0,\n",
       "  88.0,\n",
       "  24.0,\n",
       "  6.0,\n",
       "  0.0,\n",
       "  42.0,\n",
       "  4.0,\n",
       "  36.0,\n",
       "  22.0,\n",
       "  -50.0,\n",
       "  14.0,\n",
       "  -20.0,\n",
       "  -28.0,\n",
       "  112.0,\n",
       "  -8.0,\n",
       "  42.0,\n",
       "  28.0,\n",
       "  74.0,\n",
       "  4.0,\n",
       "  8.0,\n",
       "  100.0,\n",
       "  106.0,\n",
       "  -20.0,\n",
       "  -40.0,\n",
       "  14.0,\n",
       "  54.0,\n",
       "  34.0,\n",
       "  -10.0,\n",
       "  -30.0,\n",
       "  46.0,\n",
       "  36.0,\n",
       "  70.0,\n",
       "  -18.0,\n",
       "  82.0,\n",
       "  18.0,\n",
       "  -2.0,\n",
       "  92.0,\n",
       "  22.0,\n",
       "  84.0,\n",
       "  106.0,\n",
       "  58.0,\n",
       "  50.0,\n",
       "  36.0,\n",
       "  102.0,\n",
       "  34.0,\n",
       "  4.0,\n",
       "  42.0,\n",
       "  78.0,\n",
       "  112.0,\n",
       "  20.0,\n",
       "  22.0,\n",
       "  -10.0,\n",
       "  56.0,\n",
       "  80.0,\n",
       "  -40.0,\n",
       "  -16.0,\n",
       "  -2.0,\n",
       "  64.0,\n",
       "  -36.0,\n",
       "  26.0,\n",
       "  8.0,\n",
       "  82.0,\n",
       "  60.0,\n",
       "  42.0,\n",
       "  78.0,\n",
       "  66.0,\n",
       "  76.0,\n",
       "  84.0,\n",
       "  -40.0,\n",
       "  32.0,\n",
       "  -18.0,\n",
       "  68.0,\n",
       "  -10.0,\n",
       "  10.0,\n",
       "  40.0,\n",
       "  -10.0,\n",
       "  12.0,\n",
       "  122.0,\n",
       "  128.0,\n",
       "  ...],\n",
       " 'nb_episode_steps': [50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  ...],\n",
       " 'nb_steps': [50,\n",
       "  100,\n",
       "  150,\n",
       "  200,\n",
       "  250,\n",
       "  300,\n",
       "  350,\n",
       "  400,\n",
       "  450,\n",
       "  500,\n",
       "  550,\n",
       "  600,\n",
       "  650,\n",
       "  700,\n",
       "  750,\n",
       "  800,\n",
       "  850,\n",
       "  900,\n",
       "  950,\n",
       "  1000,\n",
       "  1050,\n",
       "  1100,\n",
       "  1150,\n",
       "  1200,\n",
       "  1250,\n",
       "  1300,\n",
       "  1350,\n",
       "  1400,\n",
       "  1450,\n",
       "  1500,\n",
       "  1550,\n",
       "  1600,\n",
       "  1650,\n",
       "  1700,\n",
       "  1750,\n",
       "  1800,\n",
       "  1850,\n",
       "  1900,\n",
       "  1950,\n",
       "  2000,\n",
       "  2050,\n",
       "  2100,\n",
       "  2150,\n",
       "  2200,\n",
       "  2250,\n",
       "  2300,\n",
       "  2350,\n",
       "  2400,\n",
       "  2450,\n",
       "  2500,\n",
       "  2550,\n",
       "  2600,\n",
       "  2650,\n",
       "  2700,\n",
       "  2750,\n",
       "  2800,\n",
       "  2850,\n",
       "  2900,\n",
       "  2950,\n",
       "  3000,\n",
       "  3050,\n",
       "  3100,\n",
       "  3150,\n",
       "  3200,\n",
       "  3250,\n",
       "  3300,\n",
       "  3350,\n",
       "  3400,\n",
       "  3450,\n",
       "  3500,\n",
       "  3550,\n",
       "  3600,\n",
       "  3650,\n",
       "  3700,\n",
       "  3750,\n",
       "  3800,\n",
       "  3850,\n",
       "  3900,\n",
       "  3950,\n",
       "  4000,\n",
       "  4050,\n",
       "  4100,\n",
       "  4150,\n",
       "  4200,\n",
       "  4250,\n",
       "  4300,\n",
       "  4350,\n",
       "  4400,\n",
       "  4450,\n",
       "  4500,\n",
       "  4550,\n",
       "  4600,\n",
       "  4650,\n",
       "  4700,\n",
       "  4750,\n",
       "  4800,\n",
       "  4850,\n",
       "  4900,\n",
       "  4950,\n",
       "  5000,\n",
       "  5050,\n",
       "  5100,\n",
       "  5150,\n",
       "  5200,\n",
       "  5250,\n",
       "  5300,\n",
       "  5350,\n",
       "  5400,\n",
       "  5450,\n",
       "  5500,\n",
       "  5550,\n",
       "  5600,\n",
       "  5650,\n",
       "  5700,\n",
       "  5750,\n",
       "  5800,\n",
       "  5850,\n",
       "  5900,\n",
       "  5950,\n",
       "  6000,\n",
       "  6050,\n",
       "  6100,\n",
       "  6150,\n",
       "  6200,\n",
       "  6250,\n",
       "  6300,\n",
       "  6350,\n",
       "  6400,\n",
       "  6450,\n",
       "  6500,\n",
       "  6550,\n",
       "  6600,\n",
       "  6650,\n",
       "  6700,\n",
       "  6750,\n",
       "  6800,\n",
       "  6850,\n",
       "  6900,\n",
       "  6950,\n",
       "  7000,\n",
       "  7050,\n",
       "  7100,\n",
       "  7150,\n",
       "  7200,\n",
       "  7250,\n",
       "  7300,\n",
       "  7350,\n",
       "  7400,\n",
       "  7450,\n",
       "  7500,\n",
       "  7550,\n",
       "  7600,\n",
       "  7650,\n",
       "  7700,\n",
       "  7750,\n",
       "  7800,\n",
       "  7850,\n",
       "  7900,\n",
       "  7950,\n",
       "  8000,\n",
       "  8050,\n",
       "  8100,\n",
       "  8150,\n",
       "  8200,\n",
       "  8250,\n",
       "  8300,\n",
       "  8350,\n",
       "  8400,\n",
       "  8450,\n",
       "  8500,\n",
       "  8550,\n",
       "  8600,\n",
       "  8650,\n",
       "  8700,\n",
       "  8750,\n",
       "  8800,\n",
       "  8850,\n",
       "  8900,\n",
       "  8950,\n",
       "  9000,\n",
       "  9050,\n",
       "  9100,\n",
       "  9150,\n",
       "  9200,\n",
       "  9250,\n",
       "  9300,\n",
       "  9350,\n",
       "  9400,\n",
       "  9450,\n",
       "  9500,\n",
       "  9550,\n",
       "  9600,\n",
       "  9650,\n",
       "  9700,\n",
       "  9750,\n",
       "  9800,\n",
       "  9850,\n",
       "  9900,\n",
       "  9950,\n",
       "  10000,\n",
       "  10050,\n",
       "  10100,\n",
       "  10150,\n",
       "  10200,\n",
       "  10250,\n",
       "  10300,\n",
       "  10350,\n",
       "  10400,\n",
       "  10450,\n",
       "  10500,\n",
       "  10550,\n",
       "  10600,\n",
       "  10650,\n",
       "  10700,\n",
       "  10750,\n",
       "  10800,\n",
       "  10850,\n",
       "  10900,\n",
       "  10950,\n",
       "  11000,\n",
       "  11050,\n",
       "  11100,\n",
       "  11150,\n",
       "  11200,\n",
       "  11250,\n",
       "  11300,\n",
       "  11350,\n",
       "  11400,\n",
       "  11450,\n",
       "  11500,\n",
       "  11550,\n",
       "  11600,\n",
       "  11650,\n",
       "  11700,\n",
       "  11750,\n",
       "  11800,\n",
       "  11850,\n",
       "  11900,\n",
       "  11950,\n",
       "  12000,\n",
       "  12050,\n",
       "  12100,\n",
       "  12150,\n",
       "  12200,\n",
       "  12250,\n",
       "  12300,\n",
       "  12350,\n",
       "  12400,\n",
       "  12450,\n",
       "  12500,\n",
       "  12550,\n",
       "  12600,\n",
       "  12650,\n",
       "  12700,\n",
       "  12750,\n",
       "  12800,\n",
       "  12850,\n",
       "  12900,\n",
       "  12950,\n",
       "  13000,\n",
       "  13050,\n",
       "  13100,\n",
       "  13150,\n",
       "  13200,\n",
       "  13250,\n",
       "  13300,\n",
       "  13350,\n",
       "  13400,\n",
       "  13450,\n",
       "  13500,\n",
       "  13550,\n",
       "  13600,\n",
       "  13650,\n",
       "  13700,\n",
       "  13750,\n",
       "  13800,\n",
       "  13850,\n",
       "  13900,\n",
       "  13950,\n",
       "  14000,\n",
       "  14050,\n",
       "  14100,\n",
       "  14150,\n",
       "  14200,\n",
       "  14250,\n",
       "  14300,\n",
       "  14350,\n",
       "  14400,\n",
       "  14450,\n",
       "  14500,\n",
       "  14550,\n",
       "  14600,\n",
       "  14650,\n",
       "  14700,\n",
       "  14750,\n",
       "  14800,\n",
       "  14850,\n",
       "  14900,\n",
       "  14950,\n",
       "  15000,\n",
       "  15050,\n",
       "  15100,\n",
       "  15150,\n",
       "  15200,\n",
       "  15250,\n",
       "  15300,\n",
       "  15350,\n",
       "  15400,\n",
       "  15450,\n",
       "  15500,\n",
       "  15550,\n",
       "  15600,\n",
       "  15650,\n",
       "  15700,\n",
       "  15750,\n",
       "  15800,\n",
       "  15850,\n",
       "  15900,\n",
       "  15950,\n",
       "  16000,\n",
       "  16050,\n",
       "  16100,\n",
       "  16150,\n",
       "  16200,\n",
       "  16250,\n",
       "  16300,\n",
       "  16350,\n",
       "  16400,\n",
       "  16450,\n",
       "  16500,\n",
       "  16550,\n",
       "  16600,\n",
       "  16650,\n",
       "  16700,\n",
       "  16750,\n",
       "  16800,\n",
       "  16850,\n",
       "  16900,\n",
       "  16950,\n",
       "  17000,\n",
       "  17050,\n",
       "  17100,\n",
       "  17150,\n",
       "  17200,\n",
       "  17250,\n",
       "  17300,\n",
       "  17350,\n",
       "  17400,\n",
       "  17450,\n",
       "  17500,\n",
       "  17550,\n",
       "  17600,\n",
       "  17650,\n",
       "  17700,\n",
       "  17750,\n",
       "  17800,\n",
       "  17850,\n",
       "  17900,\n",
       "  17950,\n",
       "  18000,\n",
       "  18050,\n",
       "  18100,\n",
       "  18150,\n",
       "  18200,\n",
       "  18250,\n",
       "  18300,\n",
       "  18350,\n",
       "  18400,\n",
       "  18450,\n",
       "  18500,\n",
       "  18550,\n",
       "  18600,\n",
       "  18650,\n",
       "  18700,\n",
       "  18750,\n",
       "  18800,\n",
       "  18850,\n",
       "  18900,\n",
       "  18950,\n",
       "  19000,\n",
       "  19050,\n",
       "  19100,\n",
       "  19150,\n",
       "  19200,\n",
       "  19250,\n",
       "  19300,\n",
       "  19350,\n",
       "  19400,\n",
       "  19450,\n",
       "  19500,\n",
       "  19550,\n",
       "  19600,\n",
       "  19650,\n",
       "  19700,\n",
       "  19750,\n",
       "  19800,\n",
       "  19850,\n",
       "  19900,\n",
       "  19950,\n",
       "  20000,\n",
       "  20050,\n",
       "  20100,\n",
       "  20150,\n",
       "  20200,\n",
       "  20250,\n",
       "  20300,\n",
       "  20350,\n",
       "  20400,\n",
       "  20450,\n",
       "  20500,\n",
       "  20550,\n",
       "  20600,\n",
       "  20650,\n",
       "  20700,\n",
       "  20750,\n",
       "  20800,\n",
       "  20850,\n",
       "  20900,\n",
       "  20950,\n",
       "  21000,\n",
       "  21050,\n",
       "  21100,\n",
       "  21150,\n",
       "  21200,\n",
       "  21250,\n",
       "  21300,\n",
       "  21350,\n",
       "  21400,\n",
       "  21450,\n",
       "  21500,\n",
       "  21550,\n",
       "  21600,\n",
       "  21650,\n",
       "  21700,\n",
       "  21750,\n",
       "  21800,\n",
       "  21850,\n",
       "  21900,\n",
       "  21950,\n",
       "  22000,\n",
       "  22050,\n",
       "  22100,\n",
       "  22150,\n",
       "  22200,\n",
       "  22250,\n",
       "  22300,\n",
       "  22350,\n",
       "  22400,\n",
       "  22450,\n",
       "  22500,\n",
       "  22550,\n",
       "  22600,\n",
       "  22650,\n",
       "  22700,\n",
       "  22750,\n",
       "  22800,\n",
       "  22850,\n",
       "  22900,\n",
       "  22950,\n",
       "  23000,\n",
       "  23050,\n",
       "  23100,\n",
       "  23150,\n",
       "  23200,\n",
       "  23250,\n",
       "  23300,\n",
       "  23350,\n",
       "  23400,\n",
       "  23450,\n",
       "  23500,\n",
       "  23550,\n",
       "  23600,\n",
       "  23650,\n",
       "  23700,\n",
       "  23750,\n",
       "  23800,\n",
       "  23850,\n",
       "  23900,\n",
       "  23950,\n",
       "  24000,\n",
       "  24050,\n",
       "  24100,\n",
       "  24150,\n",
       "  24200,\n",
       "  24250,\n",
       "  24300,\n",
       "  24350,\n",
       "  24400,\n",
       "  24450,\n",
       "  24500,\n",
       "  24550,\n",
       "  24600,\n",
       "  24650,\n",
       "  24700,\n",
       "  24750,\n",
       "  24800,\n",
       "  24850,\n",
       "  24900,\n",
       "  24950,\n",
       "  25000,\n",
       "  25050,\n",
       "  25100,\n",
       "  25150,\n",
       "  25200,\n",
       "  25250,\n",
       "  25300,\n",
       "  25350,\n",
       "  25400,\n",
       "  25450,\n",
       "  25500,\n",
       "  25550,\n",
       "  25600,\n",
       "  25650,\n",
       "  25700,\n",
       "  25750,\n",
       "  25800,\n",
       "  25850,\n",
       "  25900,\n",
       "  25950,\n",
       "  26000,\n",
       "  26050,\n",
       "  26100,\n",
       "  26150,\n",
       "  26200,\n",
       "  26250,\n",
       "  26300,\n",
       "  26350,\n",
       "  26400,\n",
       "  26450,\n",
       "  26500,\n",
       "  26550,\n",
       "  26600,\n",
       "  26650,\n",
       "  26700,\n",
       "  26750,\n",
       "  26800,\n",
       "  26850,\n",
       "  26900,\n",
       "  26950,\n",
       "  27000,\n",
       "  27050,\n",
       "  27100,\n",
       "  27150,\n",
       "  27200,\n",
       "  27250,\n",
       "  27300,\n",
       "  27350,\n",
       "  27400,\n",
       "  27450,\n",
       "  27500,\n",
       "  27550,\n",
       "  27600,\n",
       "  27650,\n",
       "  27700,\n",
       "  27750,\n",
       "  27800,\n",
       "  27850,\n",
       "  27900,\n",
       "  27950,\n",
       "  28000,\n",
       "  28050,\n",
       "  28100,\n",
       "  28150,\n",
       "  28200,\n",
       "  28250,\n",
       "  28300,\n",
       "  28350,\n",
       "  28400,\n",
       "  28450,\n",
       "  28500,\n",
       "  28550,\n",
       "  28600,\n",
       "  28650,\n",
       "  28700,\n",
       "  28750,\n",
       "  28800,\n",
       "  28850,\n",
       "  28900,\n",
       "  28950,\n",
       "  29000,\n",
       "  29050,\n",
       "  29100,\n",
       "  29150,\n",
       "  29200,\n",
       "  29250,\n",
       "  29300,\n",
       "  29350,\n",
       "  29400,\n",
       "  29450,\n",
       "  29500,\n",
       "  29550,\n",
       "  29600,\n",
       "  29650,\n",
       "  29700,\n",
       "  29750,\n",
       "  29800,\n",
       "  29850,\n",
       "  29900,\n",
       "  29950,\n",
       "  30000,\n",
       "  30050,\n",
       "  30100,\n",
       "  30150,\n",
       "  30200,\n",
       "  30250,\n",
       "  30300,\n",
       "  30350,\n",
       "  30400,\n",
       "  30450,\n",
       "  30500,\n",
       "  30550,\n",
       "  30600,\n",
       "  30650,\n",
       "  30700,\n",
       "  30750,\n",
       "  30800,\n",
       "  30850,\n",
       "  30900,\n",
       "  30950,\n",
       "  31000,\n",
       "  31050,\n",
       "  31100,\n",
       "  31150,\n",
       "  31200,\n",
       "  31250,\n",
       "  31300,\n",
       "  31350,\n",
       "  31400,\n",
       "  31450,\n",
       "  31500,\n",
       "  31550,\n",
       "  31600,\n",
       "  31650,\n",
       "  31700,\n",
       "  31750,\n",
       "  31800,\n",
       "  31850,\n",
       "  31900,\n",
       "  31950,\n",
       "  32000,\n",
       "  32050,\n",
       "  32100,\n",
       "  32150,\n",
       "  32200,\n",
       "  32250,\n",
       "  32300,\n",
       "  32350,\n",
       "  32400,\n",
       "  32450,\n",
       "  32500,\n",
       "  32550,\n",
       "  32600,\n",
       "  32650,\n",
       "  32700,\n",
       "  32750,\n",
       "  32800,\n",
       "  32850,\n",
       "  32900,\n",
       "  32950,\n",
       "  33000,\n",
       "  33050,\n",
       "  33100,\n",
       "  33150,\n",
       "  33200,\n",
       "  33250,\n",
       "  33300,\n",
       "  33350,\n",
       "  33400,\n",
       "  33450,\n",
       "  33500,\n",
       "  33550,\n",
       "  33600,\n",
       "  33650,\n",
       "  33700,\n",
       "  33750,\n",
       "  33800,\n",
       "  33850,\n",
       "  33900,\n",
       "  33950,\n",
       "  34000,\n",
       "  34050,\n",
       "  34100,\n",
       "  34150,\n",
       "  34200,\n",
       "  34250,\n",
       "  34300,\n",
       "  34350,\n",
       "  34400,\n",
       "  34450,\n",
       "  34500,\n",
       "  34550,\n",
       "  34600,\n",
       "  34650,\n",
       "  34700,\n",
       "  34750,\n",
       "  34800,\n",
       "  34850,\n",
       "  34900,\n",
       "  34950,\n",
       "  35000,\n",
       "  35050,\n",
       "  35100,\n",
       "  35150,\n",
       "  35200,\n",
       "  35250,\n",
       "  35300,\n",
       "  35350,\n",
       "  35400,\n",
       "  35450,\n",
       "  35500,\n",
       "  35550,\n",
       "  35600,\n",
       "  35650,\n",
       "  35700,\n",
       "  35750,\n",
       "  35800,\n",
       "  35850,\n",
       "  35900,\n",
       "  35950,\n",
       "  36000,\n",
       "  36050,\n",
       "  36100,\n",
       "  36150,\n",
       "  36200,\n",
       "  36250,\n",
       "  36300,\n",
       "  36350,\n",
       "  36400,\n",
       "  36450,\n",
       "  36500,\n",
       "  36550,\n",
       "  36600,\n",
       "  36650,\n",
       "  36700,\n",
       "  36750,\n",
       "  36800,\n",
       "  36850,\n",
       "  36900,\n",
       "  36950,\n",
       "  37000,\n",
       "  37050,\n",
       "  37100,\n",
       "  37150,\n",
       "  37200,\n",
       "  37250,\n",
       "  37300,\n",
       "  37350,\n",
       "  37400,\n",
       "  37450,\n",
       "  37500,\n",
       "  37550,\n",
       "  37600,\n",
       "  37650,\n",
       "  37700,\n",
       "  37750,\n",
       "  37800,\n",
       "  37850,\n",
       "  37900,\n",
       "  37950,\n",
       "  38000,\n",
       "  38050,\n",
       "  38100,\n",
       "  38150,\n",
       "  38200,\n",
       "  38250,\n",
       "  38300,\n",
       "  38350,\n",
       "  38400,\n",
       "  38450,\n",
       "  38500,\n",
       "  38550,\n",
       "  38600,\n",
       "  38650,\n",
       "  38700,\n",
       "  38750,\n",
       "  38800,\n",
       "  38850,\n",
       "  38900,\n",
       "  38950,\n",
       "  39000,\n",
       "  39050,\n",
       "  39100,\n",
       "  39150,\n",
       "  39200,\n",
       "  39250,\n",
       "  39300,\n",
       "  39350,\n",
       "  39400,\n",
       "  39450,\n",
       "  39500,\n",
       "  39550,\n",
       "  39600,\n",
       "  39650,\n",
       "  39700,\n",
       "  39750,\n",
       "  39800,\n",
       "  39850,\n",
       "  39900,\n",
       "  39950,\n",
       "  40000,\n",
       "  40050,\n",
       "  40100,\n",
       "  40150,\n",
       "  40200,\n",
       "  40250,\n",
       "  40300,\n",
       "  40350,\n",
       "  40400,\n",
       "  40450,\n",
       "  40500,\n",
       "  40550,\n",
       "  40600,\n",
       "  40650,\n",
       "  40700,\n",
       "  40750,\n",
       "  40800,\n",
       "  40850,\n",
       "  40900,\n",
       "  40950,\n",
       "  41000,\n",
       "  41050,\n",
       "  41100,\n",
       "  41150,\n",
       "  41200,\n",
       "  41250,\n",
       "  41300,\n",
       "  41350,\n",
       "  41400,\n",
       "  41450,\n",
       "  41500,\n",
       "  41550,\n",
       "  41600,\n",
       "  41650,\n",
       "  41700,\n",
       "  41750,\n",
       "  41800,\n",
       "  41850,\n",
       "  41900,\n",
       "  41950,\n",
       "  42000,\n",
       "  42050,\n",
       "  42100,\n",
       "  42150,\n",
       "  42200,\n",
       "  42250,\n",
       "  42300,\n",
       "  42350,\n",
       "  42400,\n",
       "  42450,\n",
       "  42500,\n",
       "  42550,\n",
       "  42600,\n",
       "  42650,\n",
       "  42700,\n",
       "  42750,\n",
       "  42800,\n",
       "  42850,\n",
       "  42900,\n",
       "  42950,\n",
       "  43000,\n",
       "  43050,\n",
       "  43100,\n",
       "  43150,\n",
       "  43200,\n",
       "  43250,\n",
       "  43300,\n",
       "  43350,\n",
       "  43400,\n",
       "  43450,\n",
       "  43500,\n",
       "  43550,\n",
       "  43600,\n",
       "  43650,\n",
       "  43700,\n",
       "  43750,\n",
       "  43800,\n",
       "  43850,\n",
       "  43900,\n",
       "  43950,\n",
       "  44000,\n",
       "  44050,\n",
       "  44100,\n",
       "  44150,\n",
       "  44200,\n",
       "  44250,\n",
       "  44300,\n",
       "  44350,\n",
       "  44400,\n",
       "  44450,\n",
       "  44500,\n",
       "  44550,\n",
       "  44600,\n",
       "  44650,\n",
       "  44700,\n",
       "  44750,\n",
       "  44800,\n",
       "  44850,\n",
       "  44900,\n",
       "  44950,\n",
       "  45000,\n",
       "  45050,\n",
       "  45100,\n",
       "  45150,\n",
       "  45200,\n",
       "  45250,\n",
       "  45300,\n",
       "  45350,\n",
       "  45400,\n",
       "  45450,\n",
       "  45500,\n",
       "  45550,\n",
       "  45600,\n",
       "  45650,\n",
       "  45700,\n",
       "  45750,\n",
       "  45800,\n",
       "  45850,\n",
       "  45900,\n",
       "  45950,\n",
       "  46000,\n",
       "  46050,\n",
       "  46100,\n",
       "  46150,\n",
       "  46200,\n",
       "  46250,\n",
       "  46300,\n",
       "  46350,\n",
       "  46400,\n",
       "  46450,\n",
       "  46500,\n",
       "  46550,\n",
       "  46600,\n",
       "  46650,\n",
       "  46700,\n",
       "  46750,\n",
       "  46800,\n",
       "  46850,\n",
       "  46900,\n",
       "  46950,\n",
       "  47000,\n",
       "  47050,\n",
       "  47100,\n",
       "  47150,\n",
       "  47200,\n",
       "  47250,\n",
       "  47300,\n",
       "  47350,\n",
       "  47400,\n",
       "  47450,\n",
       "  47500,\n",
       "  47550,\n",
       "  47600,\n",
       "  47650,\n",
       "  47700,\n",
       "  47750,\n",
       "  47800,\n",
       "  47850,\n",
       "  47900,\n",
       "  47950,\n",
       "  48000,\n",
       "  48050,\n",
       "  48100,\n",
       "  48150,\n",
       "  48200,\n",
       "  48250,\n",
       "  48300,\n",
       "  48350,\n",
       "  48400,\n",
       "  48450,\n",
       "  48500,\n",
       "  48550,\n",
       "  48600,\n",
       "  48650,\n",
       "  48700,\n",
       "  48750,\n",
       "  48800,\n",
       "  48850,\n",
       "  48900,\n",
       "  48950,\n",
       "  49000,\n",
       "  49050,\n",
       "  49100,\n",
       "  49150,\n",
       "  49200,\n",
       "  49250,\n",
       "  49300,\n",
       "  49350,\n",
       "  49400,\n",
       "  49450,\n",
       "  49500,\n",
       "  49550,\n",
       "  49600,\n",
       "  49650,\n",
       "  49700,\n",
       "  49750,\n",
       "  49800,\n",
       "  49850,\n",
       "  49900,\n",
       "  49950,\n",
       "  50000,\n",
       "  ...]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "523042d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('agents/{}'.format(name))  # If the directory does not exist we cannot write the file\n",
    "with open(get_training_path(name), 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6025dba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "\n",
    "ax.plot(history.history['nb_steps'], history.history['episode_reward'])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf73776",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dqn.test(env, nb_episodes=10, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d1d24c",
   "metadata": {},
   "source": [
    "Save agent to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1759baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights(get_agent_path(name), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910b175d",
   "metadata": {},
   "source": [
    "## 4. Reloading training from Memory ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb14792",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
