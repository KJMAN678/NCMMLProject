{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4221e1e3",
   "metadata": {},
   "source": [
    "# Training a basic setting with a Deep Q Network (DQN) #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d628c23d",
   "metadata": {},
   "source": [
    "Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b9fe8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "rc('text', usetex=True)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "761fd1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f09c161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory  # For experience replay!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c9f747a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.1 (SDL 2.0.14, Python 3.7.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from gym_environment_ncml import *\n",
    "from learning import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a0674c",
   "metadata": {},
   "source": [
    "Useful numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cf11480",
   "metadata": {},
   "outputs": [],
   "source": [
    "MILLION = 1000000\n",
    "HTHOUSAND = 100000\n",
    "THOUSAND = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfb0b18",
   "metadata": {},
   "source": [
    "## 1. Create environment ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b9a3f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridworldMultiAgentv1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "708f7943",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eca1527b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 25)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states, actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0343c79a",
   "metadata": {},
   "source": [
    "## 2. Create a Deep Learning Model with Keras ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "941ff796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/copernico/opt/anaconda3/envs/RL/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "model = build_model(states, actions, [32, 16], ['relu', 'relu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f136bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 25)                425       \n",
      "=================================================================\n",
      "Total params: 1,241\n",
      "Trainable params: 1,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0b28d2",
   "metadata": {},
   "source": [
    "## 3. Build Agent with Keras-RL ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a418e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = build_agent(model, actions, 0.01, EpsGreedyQPolicy(), 50000)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "# dqn.compile(Adam(lr=1e-2), metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "674f77a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'dqn1_5b5_3216_adam_lr0.001_tmu0.01_ml50K_ns5M_eps0.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "073ef900",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: -0.4370\n",
      "200 episodes - episode_reward: -21.850 [-50.000, 80.000] - loss: 3.741 - mae: 9.218 - mean_q: 11.168\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: -0.0240\n",
      "200 episodes - episode_reward: -1.200 [-50.000, 120.000] - loss: 9.952 - mae: 21.438 - mean_q: 24.182\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: -0.0450\n",
      "200 episodes - episode_reward: -2.250 [-50.000, 100.000] - loss: 15.679 - mae: 31.044 - mean_q: 34.276\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 61s 6ms/step - reward: 0.1650\n",
      "200 episodes - episode_reward: 8.250 [-50.000, 130.000] - loss: 17.189 - mae: 32.623 - mean_q: 35.896\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 0.2480\n",
      "200 episodes - episode_reward: 12.400 [-50.000, 170.000] - loss: 18.865 - mae: 34.486 - mean_q: 37.835\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 0.2350\n",
      "200 episodes - episode_reward: 11.750 [-50.000, 150.000] - loss: 20.752 - mae: 35.270 - mean_q: 38.622\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 0.3190\n",
      "200 episodes - episode_reward: 15.950 [-50.000, 190.000] - loss: 23.286 - mae: 37.709 - mean_q: 41.240\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 0.4020\n",
      "200 episodes - episode_reward: 20.100 [-50.000, 130.000] - loss: 27.103 - mae: 41.135 - mean_q: 44.852\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 0.3970\n",
      "200 episodes - episode_reward: 19.850 [-50.000, 130.000] - loss: 30.199 - mae: 44.509 - mean_q: 48.482\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 0.2700\n",
      "200 episodes - episode_reward: 13.500 [-50.000, 150.000] - loss: 31.250 - mae: 45.044 - mean_q: 49.031\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 0.5560\n",
      "200 episodes - episode_reward: 27.800 [-50.000, 140.000] - loss: 33.902 - mae: 47.801 - mean_q: 52.135\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 0.7060\n",
      "200 episodes - episode_reward: 35.300 [-50.000, 160.000] - loss: 40.469 - mae: 53.545 - mean_q: 58.318\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 0.9840\n",
      "200 episodes - episode_reward: 49.200 [-50.000, 190.000] - loss: 46.249 - mae: 58.047 - mean_q: 63.196\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 1.0370\n",
      "200 episodes - episode_reward: 51.850 [-50.000, 190.000] - loss: 58.564 - mae: 66.060 - mean_q: 71.932\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 1.0770\n",
      "200 episodes - episode_reward: 53.850 [-50.000, 220.000] - loss: 70.808 - mae: 73.755 - mean_q: 80.195\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.4610\n",
      "200 episodes - episode_reward: 73.050 [-50.000, 190.000] - loss: 82.350 - mae: 79.723 - mean_q: 86.754\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 1.3910\n",
      "200 episodes - episode_reward: 69.550 [-50.000, 190.000] - loss: 99.100 - mae: 89.898 - mean_q: 97.693\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 1.2620\n",
      "200 episodes - episode_reward: 63.100 [-50.000, 190.000] - loss: 114.371 - mae: 96.081 - mean_q: 104.487\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.7450\n",
      "200 episodes - episode_reward: 87.250 [-50.000, 230.000] - loss: 124.932 - mae: 100.379 - mean_q: 109.296\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.8350\n",
      "200 episodes - episode_reward: 91.750 [-50.000, 270.000] - loss: 133.507 - mae: 104.940 - mean_q: 114.299\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.0760\n",
      "200 episodes - episode_reward: 103.800 [-40.000, 220.000] - loss: 151.886 - mae: 112.864 - mean_q: 122.808\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.0130\n",
      "200 episodes - episode_reward: 100.650 [-50.000, 230.000] - loss: 164.717 - mae: 116.082 - mean_q: 126.347\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 2.0180\n",
      "200 episodes - episode_reward: 100.900 [-50.000, 230.000] - loss: 175.454 - mae: 120.582 - mean_q: 131.133\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.8040\n",
      "200 episodes - episode_reward: 90.200 [-40.000, 250.000] - loss: 189.069 - mae: 125.462 - mean_q: 136.356\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.0760\n",
      "200 episodes - episode_reward: 103.800 [-50.000, 240.000] - loss: 192.059 - mae: 125.837 - mean_q: 136.790\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 2.5360\n",
      "200 episodes - episode_reward: 126.800 [-50.000, 250.000] - loss: 201.604 - mae: 130.773 - mean_q: 142.173\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 2.1380\n",
      "200 episodes - episode_reward: 106.900 [-40.000, 230.000] - loss: 202.462 - mae: 130.752 - mean_q: 141.680\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.3350\n",
      "200 episodes - episode_reward: 116.750 [-30.000, 280.000] - loss: 210.064 - mae: 134.688 - mean_q: 146.015\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 2.4980\n",
      "200 episodes - episode_reward: 124.900 [-50.000, 290.000] - loss: 215.088 - mae: 136.097 - mean_q: 147.560\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.5320\n",
      "200 episodes - episode_reward: 126.600 [-30.000, 250.000] - loss: 233.032 - mae: 139.775 - mean_q: 151.782\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.6180\n",
      "200 episodes - episode_reward: 130.900 [-50.000, 280.000] - loss: 246.172 - mae: 143.115 - mean_q: 155.523\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.7800\n",
      "200 episodes - episode_reward: 139.000 [-30.000, 280.000] - loss: 261.339 - mae: 147.605 - mean_q: 160.457\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.7900\n",
      "200 episodes - episode_reward: 139.500 [-40.000, 240.000] - loss: 273.232 - mae: 152.587 - mean_q: 165.765\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.5170\n",
      "200 episodes - episode_reward: 125.850 [-50.000, 260.000] - loss: 282.588 - mae: 154.149 - mean_q: 167.435\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.8680\n",
      "200 episodes - episode_reward: 143.400 [-40.000, 270.000] - loss: 284.040 - mae: 155.391 - mean_q: 168.850\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.6670\n",
      "200 episodes - episode_reward: 133.350 [-40.000, 270.000] - loss: 292.743 - mae: 158.247 - mean_q: 171.823\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.7340\n",
      "200 episodes - episode_reward: 136.700 [-50.000, 270.000] - loss: 285.068 - mae: 154.520 - mean_q: 168.042\n",
      "\n",
      "Interval 38 (370000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.0120\n",
      "200 episodes - episode_reward: 150.600 [-20.000, 270.000] - loss: 286.223 - mae: 156.156 - mean_q: 169.725\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 2.9870\n",
      "200 episodes - episode_reward: 149.350 [-20.000, 260.000] - loss: 299.244 - mae: 160.773 - mean_q: 174.601\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 2.7970\n",
      "200 episodes - episode_reward: 139.850 [-10.000, 280.000] - loss: 313.565 - mae: 165.469 - mean_q: 179.531\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.9210\n",
      "200 episodes - episode_reward: 146.050 [-50.000, 280.000] - loss: 311.253 - mae: 163.974 - mean_q: 177.593\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.0420\n",
      "200 episodes - episode_reward: 152.100 [-50.000, 290.000] - loss: 310.279 - mae: 164.136 - mean_q: 178.161\n",
      "\n",
      "Interval 43 (420000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 2.9100\n",
      "200 episodes - episode_reward: 145.500 [-40.000, 290.000] - loss: 307.751 - mae: 162.180 - mean_q: 175.793\n",
      "\n",
      "Interval 44 (430000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 2.9450\n",
      "200 episodes - episode_reward: 147.250 [-40.000, 280.000] - loss: 291.994 - mae: 157.274 - mean_q: 170.681\n",
      "\n",
      "Interval 45 (440000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 2.7960\n",
      "200 episodes - episode_reward: 139.800 [-50.000, 290.000] - loss: 297.567 - mae: 159.030 - mean_q: 172.600\n",
      "\n",
      "Interval 46 (450000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 2.8310\n",
      "200 episodes - episode_reward: 141.550 [-20.000, 260.000] - loss: 294.834 - mae: 160.070 - mean_q: 173.526\n",
      "\n",
      "Interval 47 (460000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.7750\n",
      "200 episodes - episode_reward: 138.750 [-10.000, 270.000] - loss: 292.295 - mae: 160.014 - mean_q: 173.400\n",
      "\n",
      "Interval 48 (470000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.7470\n",
      "200 episodes - episode_reward: 137.350 [-50.000, 280.000] - loss: 291.452 - mae: 158.587 - mean_q: 171.603\n",
      "\n",
      "Interval 49 (480000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 2.5120\n",
      "200 episodes - episode_reward: 125.600 [-50.000, 260.000] - loss: 279.073 - mae: 154.278 - mean_q: 167.145\n",
      "\n",
      "Interval 50 (490000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 2.5710\n",
      "200 episodes - episode_reward: 128.550 [-40.000, 270.000] - loss: 270.001 - mae: 151.105 - mean_q: 163.744\n",
      "\n",
      "Interval 51 (500000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.0010\n",
      "200 episodes - episode_reward: 150.050 [-10.000, 270.000] - loss: 279.805 - mae: 154.705 - mean_q: 167.723\n",
      "\n",
      "Interval 52 (510000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 2.8780\n",
      "200 episodes - episode_reward: 143.900 [-30.000, 280.000] - loss: 278.568 - mae: 154.993 - mean_q: 168.178\n",
      "\n",
      "Interval 53 (520000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.0090\n",
      "200 episodes - episode_reward: 150.450 [-50.000, 290.000] - loss: 300.118 - mae: 159.238 - mean_q: 173.040\n",
      "\n",
      "Interval 54 (530000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.1770\n",
      "200 episodes - episode_reward: 158.850 [-50.000, 300.000] - loss: 307.198 - mae: 162.543 - mean_q: 176.717\n",
      "\n",
      "Interval 55 (540000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.3700\n",
      "200 episodes - episode_reward: 168.500 [20.000, 280.000] - loss: 312.984 - mae: 164.940 - mean_q: 178.944\n",
      "\n",
      "Interval 56 (550000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.0180\n",
      "200 episodes - episode_reward: 150.900 [-30.000, 300.000] - loss: 329.289 - mae: 167.765 - mean_q: 182.163\n",
      "\n",
      "Interval 57 (560000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.3690\n",
      "200 episodes - episode_reward: 168.450 [-20.000, 270.000] - loss: 312.510 - mae: 161.609 - mean_q: 175.496\n",
      "\n",
      "Interval 58 (570000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 3.2800\n",
      "200 episodes - episode_reward: 164.000 [-50.000, 290.000] - loss: 309.996 - mae: 164.045 - mean_q: 178.004\n",
      "\n",
      "Interval 59 (580000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.1770\n",
      "200 episodes - episode_reward: 158.850 [-40.000, 280.000] - loss: 322.027 - mae: 165.064 - mean_q: 179.080\n",
      "\n",
      "Interval 60 (590000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.0260\n",
      "200 episodes - episode_reward: 151.300 [-50.000, 250.000] - loss: 321.912 - mae: 165.049 - mean_q: 179.260\n",
      "\n",
      "Interval 61 (600000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 3.3060\n",
      "200 episodes - episode_reward: 165.300 [-50.000, 270.000] - loss: 318.915 - mae: 165.390 - mean_q: 179.618\n",
      "\n",
      "Interval 62 (610000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.3550\n",
      "200 episodes - episode_reward: 167.750 [20.000, 290.000] - loss: 327.390 - mae: 166.215 - mean_q: 180.727\n",
      "\n",
      "Interval 63 (620000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.3820\n",
      "200 episodes - episode_reward: 169.100 [-20.000, 270.000] - loss: 331.547 - mae: 168.710 - mean_q: 183.242\n",
      "\n",
      "Interval 64 (630000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.2220\n",
      "200 episodes - episode_reward: 161.100 [-20.000, 290.000] - loss: 326.988 - mae: 169.296 - mean_q: 183.693\n",
      "\n",
      "Interval 65 (640000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.0730\n",
      "200 episodes - episode_reward: 153.650 [-50.000, 270.000] - loss: 330.951 - mae: 168.357 - mean_q: 182.833\n",
      "\n",
      "Interval 66 (650000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.0460\n",
      "200 episodes - episode_reward: 152.300 [-50.000, 270.000] - loss: 323.337 - mae: 166.628 - mean_q: 180.785\n",
      "\n",
      "Interval 67 (660000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.9200\n",
      "200 episodes - episode_reward: 146.000 [-50.000, 300.000] - loss: 311.953 - mae: 161.122 - mean_q: 174.849\n",
      "\n",
      "Interval 68 (670000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.8280\n",
      "200 episodes - episode_reward: 141.400 [-50.000, 250.000] - loss: 311.566 - mae: 163.059 - mean_q: 176.971\n",
      "\n",
      "Interval 69 (680000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.9750\n",
      "200 episodes - episode_reward: 148.750 [10.000, 270.000] - loss: 298.312 - mae: 157.916 - mean_q: 171.276\n",
      "\n",
      "Interval 70 (690000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.9520\n",
      "200 episodes - episode_reward: 147.600 [-50.000, 270.000] - loss: 286.232 - mae: 157.051 - mean_q: 170.210\n",
      "\n",
      "Interval 71 (700000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 3.0890\n",
      "200 episodes - episode_reward: 154.450 [-20.000, 270.000] - loss: 291.322 - mae: 157.890 - mean_q: 171.067\n",
      "\n",
      "Interval 72 (710000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.2190\n",
      "200 episodes - episode_reward: 160.950 [-20.000, 300.000] - loss: 296.305 - mae: 159.214 - mean_q: 172.604\n",
      "\n",
      "Interval 73 (720000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.2960\n",
      "200 episodes - episode_reward: 164.800 [-40.000, 270.000] - loss: 299.342 - mae: 163.393 - mean_q: 176.968\n",
      "\n",
      "Interval 74 (730000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 3.1490\n",
      "200 episodes - episode_reward: 157.450 [-20.000, 270.000] - loss: 320.861 - mae: 165.867 - mean_q: 179.882\n",
      "\n",
      "Interval 75 (740000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.2530\n",
      "200 episodes - episode_reward: 162.650 [20.000, 310.000] - loss: 317.789 - mae: 164.248 - mean_q: 178.265\n",
      "\n",
      "Interval 76 (750000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.4860\n",
      "200 episodes - episode_reward: 174.300 [-10.000, 280.000] - loss: 328.654 - mae: 166.855 - mean_q: 181.134\n",
      "\n",
      "Interval 77 (760000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.4750\n",
      "200 episodes - episode_reward: 173.750 [-20.000, 280.000] - loss: 327.048 - mae: 168.857 - mean_q: 183.444\n",
      "\n",
      "Interval 78 (770000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.1410\n",
      "200 episodes - episode_reward: 157.050 [-20.000, 310.000] - loss: 332.565 - mae: 170.224 - mean_q: 184.729\n",
      "\n",
      "Interval 79 (780000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.3080\n",
      "200 episodes - episode_reward: 165.400 [-10.000, 290.000] - loss: 329.721 - mae: 167.111 - mean_q: 181.347\n",
      "\n",
      "Interval 80 (790000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.2940\n",
      "200 episodes - episode_reward: 164.700 [-30.000, 280.000] - loss: 322.262 - mae: 166.916 - mean_q: 181.053\n",
      "\n",
      "Interval 81 (800000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.1840\n",
      "200 episodes - episode_reward: 159.200 [-40.000, 260.000] - loss: 324.994 - mae: 166.436 - mean_q: 180.477\n",
      "\n",
      "Interval 82 (810000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 2.9350\n",
      "200 episodes - episode_reward: 146.750 [-50.000, 280.000] - loss: 324.740 - mae: 167.781 - mean_q: 181.625\n",
      "\n",
      "Interval 83 (820000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.0050\n",
      "200 episodes - episode_reward: 150.250 [-30.000, 300.000] - loss: 307.651 - mae: 161.540 - mean_q: 175.152\n",
      "\n",
      "Interval 84 (830000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 3.0740\n",
      "200 episodes - episode_reward: 153.700 [-30.000, 290.000] - loss: 304.927 - mae: 161.914 - mean_q: 175.545\n",
      "\n",
      "Interval 85 (840000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.0530\n",
      "200 episodes - episode_reward: 152.650 [-50.000, 310.000] - loss: 298.457 - mae: 158.247 - mean_q: 171.492\n",
      "\n",
      "Interval 86 (850000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.4470\n",
      "200 episodes - episode_reward: 172.350 [-30.000, 280.000] - loss: 303.020 - mae: 160.527 - mean_q: 174.360\n",
      "\n",
      "Interval 87 (860000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.3830\n",
      "200 episodes - episode_reward: 169.150 [-10.000, 270.000] - loss: 320.172 - mae: 164.111 - mean_q: 178.360\n",
      "\n",
      "Interval 88 (870000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.4670\n",
      "200 episodes - episode_reward: 173.350 [-50.000, 270.000] - loss: 311.530 - mae: 161.845 - mean_q: 175.844\n",
      "\n",
      "Interval 89 (880000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.2430\n",
      "200 episodes - episode_reward: 162.150 [-30.000, 310.000] - loss: 311.870 - mae: 164.053 - mean_q: 178.301\n",
      "\n",
      "Interval 90 (890000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.2250\n",
      "200 episodes - episode_reward: 161.250 [-20.000, 280.000] - loss: 317.412 - mae: 165.416 - mean_q: 179.910\n",
      "\n",
      "Interval 91 (900000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.1880\n",
      "200 episodes - episode_reward: 159.400 [-50.000, 280.000] - loss: 317.171 - mae: 165.664 - mean_q: 179.874\n",
      "\n",
      "Interval 92 (910000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.0480\n",
      "200 episodes - episode_reward: 152.400 [-20.000, 260.000] - loss: 319.186 - mae: 164.905 - mean_q: 179.102\n",
      "\n",
      "Interval 93 (920000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.0460\n",
      "200 episodes - episode_reward: 152.300 [-30.000, 270.000] - loss: 310.768 - mae: 164.249 - mean_q: 178.237\n",
      "\n",
      "Interval 94 (930000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.1550\n",
      "200 episodes - episode_reward: 157.750 [-30.000, 300.000] - loss: 306.410 - mae: 161.594 - mean_q: 175.253\n",
      "\n",
      "Interval 95 (940000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.5390\n",
      "200 episodes - episode_reward: 176.950 [-20.000, 280.000] - loss: 306.304 - mae: 161.228 - mean_q: 175.088\n",
      "\n",
      "Interval 96 (950000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.5070\n",
      "200 episodes - episode_reward: 175.350 [-30.000, 280.000] - loss: 307.899 - mae: 163.171 - mean_q: 177.290\n",
      "\n",
      "Interval 97 (960000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.2570\n",
      "200 episodes - episode_reward: 162.850 [-30.000, 270.000] - loss: 329.835 - mae: 167.060 - mean_q: 181.351\n",
      "\n",
      "Interval 98 (970000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.1010\n",
      "200 episodes - episode_reward: 155.050 [-50.000, 260.000] - loss: 321.402 - mae: 165.672 - mean_q: 179.772\n",
      "\n",
      "Interval 99 (980000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.3360\n",
      "200 episodes - episode_reward: 166.800 [-50.000, 290.000] - loss: 324.221 - mae: 166.048 - mean_q: 180.064\n",
      "\n",
      "Interval 100 (990000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.5240\n",
      "200 episodes - episode_reward: 176.200 [-30.000, 290.000] - loss: 313.529 - mae: 165.040 - mean_q: 178.983\n",
      "\n",
      "Interval 101 (1000000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.0070\n",
      "200 episodes - episode_reward: 150.350 [-40.000, 270.000] - loss: 322.654 - mae: 166.492 - mean_q: 180.325\n",
      "\n",
      "Interval 102 (1010000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 3.3440\n",
      "200 episodes - episode_reward: 167.200 [0.000, 280.000] - loss: 302.269 - mae: 160.908 - mean_q: 174.297\n",
      "\n",
      "Interval 103 (1020000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 3.2380\n",
      "200 episodes - episode_reward: 161.900 [-50.000, 280.000] - loss: 306.742 - mae: 162.606 - mean_q: 176.264\n",
      "\n",
      "Interval 104 (1030000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.0510\n",
      "200 episodes - episode_reward: 152.550 [-30.000, 280.000] - loss: 311.537 - mae: 163.254 - mean_q: 176.959\n",
      "\n",
      "Interval 105 (1040000 steps performed)\n",
      "10000/10000 [==============================] - 60s 6ms/step - reward: 3.0400\n",
      "200 episodes - episode_reward: 152.000 [-40.000, 310.000] - loss: 308.423 - mae: 160.541 - mean_q: 174.091\n",
      "\n",
      "Interval 106 (1050000 steps performed)\n",
      "10000/10000 [==============================] - 72s 7ms/step - reward: 3.2210\n",
      "200 episodes - episode_reward: 161.050 [-20.000, 290.000] - loss: 299.866 - mae: 159.858 - mean_q: 173.150\n",
      "\n",
      "Interval 107 (1060000 steps performed)\n",
      "10000/10000 [==============================] - 67s 7ms/step - reward: 3.3530\n",
      "200 episodes - episode_reward: 167.650 [10.000, 270.000] - loss: 305.943 - mae: 161.812 - mean_q: 175.273\n",
      "\n",
      "Interval 108 (1070000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 2.9280\n",
      "200 episodes - episode_reward: 146.400 [-50.000, 330.000] - loss: 287.953 - mae: 157.145 - mean_q: 170.129\n",
      "\n",
      "Interval 109 (1080000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.2060\n",
      "200 episodes - episode_reward: 160.300 [-30.000, 270.000] - loss: 289.065 - mae: 156.702 - mean_q: 169.623\n",
      "\n",
      "Interval 110 (1090000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.3350\n",
      "200 episodes - episode_reward: 166.750 [-20.000, 290.000] - loss: 300.809 - mae: 161.511 - mean_q: 175.062\n",
      "\n",
      "Interval 111 (1100000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.4500\n",
      "200 episodes - episode_reward: 172.500 [30.000, 300.000] - loss: 305.153 - mae: 161.670 - mean_q: 175.461\n",
      "\n",
      "Interval 112 (1110000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.5260\n",
      "200 episodes - episode_reward: 176.300 [-10.000, 300.000] - loss: 314.998 - mae: 162.542 - mean_q: 176.604\n",
      "\n",
      "Interval 113 (1120000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.4000\n",
      "200 episodes - episode_reward: 170.000 [-30.000, 310.000] - loss: 324.603 - mae: 167.102 - mean_q: 181.278\n",
      "\n",
      "Interval 114 (1130000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.2830\n",
      "200 episodes - episode_reward: 164.150 [-50.000, 290.000] - loss: 324.525 - mae: 165.928 - mean_q: 180.116\n",
      "\n",
      "Interval 115 (1140000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 2.5040\n",
      "200 episodes - episode_reward: 125.200 [-40.000, 250.000] - loss: 336.396 - mae: 170.207 - mean_q: 184.655\n",
      "\n",
      "Interval 116 (1150000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 2.1390\n",
      "200 episodes - episode_reward: 106.950 [-50.000, 260.000] - loss: 311.870 - mae: 163.388 - mean_q: 176.809\n",
      "\n",
      "Interval 117 (1160000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 2.6990\n",
      "200 episodes - episode_reward: 134.950 [-20.000, 270.000] - loss: 276.037 - mae: 151.957 - mean_q: 164.375\n",
      "\n",
      "Interval 118 (1170000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 2.8870\n",
      "200 episodes - episode_reward: 144.350 [-40.000, 270.000] - loss: 256.364 - mae: 147.191 - mean_q: 159.395\n",
      "\n",
      "Interval 119 (1180000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 1.9300\n",
      "200 episodes - episode_reward: 96.500 [-50.000, 240.000] - loss: 253.158 - mae: 147.449 - mean_q: 159.633\n",
      "\n",
      "Interval 120 (1190000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 0.8240\n",
      "200 episodes - episode_reward: 41.200 [-50.000, 180.000] - loss: 252.187 - mae: 144.946 - mean_q: 156.602\n",
      "\n",
      "Interval 121 (1200000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 1.3760\n",
      "200 episodes - episode_reward: 68.800 [-50.000, 220.000] - loss: 194.612 - mae: 128.587 - mean_q: 139.014\n",
      "\n",
      "Interval 122 (1210000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 2.5010\n",
      "200 episodes - episode_reward: 125.050 [-50.000, 260.000] - loss: 208.766 - mae: 133.778 - mean_q: 144.996\n",
      "\n",
      "Interval 123 (1220000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.0820\n",
      "200 episodes - episode_reward: 154.100 [-50.000, 260.000] - loss: 219.078 - mae: 136.700 - mean_q: 148.354\n",
      "\n",
      "Interval 124 (1230000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.0360\n",
      "200 episodes - episode_reward: 151.800 [-50.000, 270.000] - loss: 232.128 - mae: 140.383 - mean_q: 152.394\n",
      "\n",
      "Interval 125 (1240000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 2.9330\n",
      "200 episodes - episode_reward: 146.650 [-50.000, 290.000] - loss: 245.641 - mae: 145.020 - mean_q: 157.406\n",
      "\n",
      "Interval 126 (1250000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.0410\n",
      "200 episodes - episode_reward: 152.050 [-30.000, 250.000] - loss: 256.260 - mae: 148.662 - mean_q: 161.240\n",
      "\n",
      "Interval 127 (1260000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.2010\n",
      "200 episodes - episode_reward: 160.050 [10.000, 270.000] - loss: 260.827 - mae: 148.177 - mean_q: 160.881\n",
      "\n",
      "Interval 128 (1270000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.0150\n",
      "200 episodes - episode_reward: 150.750 [-50.000, 240.000] - loss: 264.138 - mae: 151.604 - mean_q: 164.361\n",
      "\n",
      "Interval 129 (1280000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 2.8820\n",
      "200 episodes - episode_reward: 144.100 [-50.000, 270.000] - loss: 273.580 - mae: 151.819 - mean_q: 164.625\n",
      "\n",
      "Interval 130 (1290000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 2.8270\n",
      "200 episodes - episode_reward: 141.350 [-30.000, 290.000] - loss: 260.218 - mae: 149.144 - mean_q: 161.823\n",
      "\n",
      "Interval 131 (1300000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 2.9580\n",
      "200 episodes - episode_reward: 147.900 [-30.000, 270.000] - loss: 254.895 - mae: 148.129 - mean_q: 160.822\n",
      "\n",
      "Interval 132 (1310000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.7380\n",
      "200 episodes - episode_reward: 136.900 [-20.000, 270.000] - loss: 254.769 - mae: 147.871 - mean_q: 160.311\n",
      "\n",
      "Interval 133 (1320000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.0660\n",
      "200 episodes - episode_reward: 153.300 [-30.000, 310.000] - loss: 254.586 - mae: 146.119 - mean_q: 158.536\n",
      "\n",
      "Interval 134 (1330000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 2.7520\n",
      "200 episodes - episode_reward: 137.600 [-50.000, 260.000] - loss: 259.677 - mae: 147.105 - mean_q: 159.536\n",
      "\n",
      "Interval 135 (1340000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.0430\n",
      "200 episodes - episode_reward: 152.150 [-40.000, 290.000] - loss: 255.852 - mae: 148.353 - mean_q: 160.776\n",
      "\n",
      "Interval 136 (1350000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 2.7540\n",
      "200 episodes - episode_reward: 137.700 [-40.000, 270.000] - loss: 261.174 - mae: 151.339 - mean_q: 164.083\n",
      "\n",
      "Interval 137 (1360000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.8370\n",
      "200 episodes - episode_reward: 141.850 [-50.000, 280.000] - loss: 260.869 - mae: 148.694 - mean_q: 161.453\n",
      "\n",
      "Interval 138 (1370000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 2.9960\n",
      "200 episodes - episode_reward: 149.800 [-40.000, 270.000] - loss: 247.747 - mae: 143.878 - mean_q: 156.536\n",
      "\n",
      "Interval 139 (1380000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 3.0010\n",
      "200 episodes - episode_reward: 150.050 [-50.000, 270.000] - loss: 250.893 - mae: 146.383 - mean_q: 159.152\n",
      "\n",
      "Interval 140 (1390000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 2.9160\n",
      "200 episodes - episode_reward: 145.800 [-20.000, 260.000] - loss: 241.931 - mae: 143.229 - mean_q: 155.665\n",
      "\n",
      "Interval 141 (1400000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 3.1830\n",
      "200 episodes - episode_reward: 159.150 [-40.000, 290.000] - loss: 240.548 - mae: 141.118 - mean_q: 153.283\n",
      "\n",
      "Interval 142 (1410000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.2090\n",
      "200 episodes - episode_reward: 160.450 [-30.000, 280.000] - loss: 248.128 - mae: 145.320 - mean_q: 157.741\n",
      "\n",
      "Interval 143 (1420000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 3.3960\n",
      "200 episodes - episode_reward: 169.800 [-10.000, 300.000] - loss: 260.529 - mae: 150.121 - mean_q: 163.032\n",
      "\n",
      "Interval 144 (1430000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 3.4650\n",
      "200 episodes - episode_reward: 173.250 [-20.000, 310.000] - loss: 278.248 - mae: 154.070 - mean_q: 167.179\n",
      "\n",
      "Interval 145 (1440000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 3.4310\n",
      "200 episodes - episode_reward: 171.550 [-30.000, 290.000] - loss: 283.592 - mae: 156.724 - mean_q: 170.010\n",
      "\n",
      "Interval 146 (1450000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 3.1140\n",
      "200 episodes - episode_reward: 155.700 [-40.000, 280.000] - loss: 299.463 - mae: 161.251 - mean_q: 174.838\n",
      "\n",
      "Interval 147 (1460000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.0760\n",
      "200 episodes - episode_reward: 153.800 [-30.000, 280.000] - loss: 298.715 - mae: 162.307 - mean_q: 175.833\n",
      "\n",
      "Interval 148 (1470000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.5770\n",
      "200 episodes - episode_reward: 178.850 [-30.000, 300.000] - loss: 301.652 - mae: 161.544 - mean_q: 175.151\n",
      "\n",
      "Interval 149 (1480000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 56s 6ms/step - reward: 3.6120\n",
      "200 episodes - episode_reward: 180.600 [-40.000, 300.000] - loss: 296.506 - mae: 162.104 - mean_q: 175.660\n",
      "\n",
      "Interval 150 (1490000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.2100\n",
      "200 episodes - episode_reward: 160.500 [-30.000, 290.000] - loss: 307.810 - mae: 162.470 - mean_q: 175.982\n",
      "\n",
      "Interval 151 (1500000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.2710\n",
      "200 episodes - episode_reward: 163.550 [-20.000, 290.000] - loss: 293.765 - mae: 157.618 - mean_q: 171.114\n",
      "\n",
      "Interval 152 (1510000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 3.3720\n",
      "200 episodes - episode_reward: 168.600 [-50.000, 310.000] - loss: 311.460 - mae: 163.999 - mean_q: 177.821\n",
      "\n",
      "Interval 153 (1520000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.0560\n",
      "200 episodes - episode_reward: 152.800 [-50.000, 280.000] - loss: 319.341 - mae: 163.482 - mean_q: 177.492\n",
      "\n",
      "Interval 154 (1530000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.1440\n",
      "200 episodes - episode_reward: 157.200 [-30.000, 280.000] - loss: 311.225 - mae: 163.961 - mean_q: 177.707\n",
      "\n",
      "Interval 155 (1540000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 3.1230\n",
      "200 episodes - episode_reward: 156.150 [-10.000, 260.000] - loss: 303.253 - mae: 161.377 - mean_q: 174.682\n",
      "\n",
      "Interval 156 (1550000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.2920\n",
      "200 episodes - episode_reward: 164.600 [-30.000, 280.000] - loss: 287.982 - mae: 157.985 - mean_q: 170.986\n",
      "\n",
      "Interval 157 (1560000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.4410\n",
      "200 episodes - episode_reward: 172.050 [-40.000, 270.000] - loss: 293.578 - mae: 157.892 - mean_q: 170.822\n",
      "\n",
      "Interval 158 (1570000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 3.0950\n",
      "200 episodes - episode_reward: 154.750 [-50.000, 270.000] - loss: 287.110 - mae: 158.841 - mean_q: 171.752\n",
      "\n",
      "Interval 159 (1580000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.0640\n",
      "200 episodes - episode_reward: 153.200 [-40.000, 290.000] - loss: 297.850 - mae: 158.825 - mean_q: 171.889\n",
      "\n",
      "Interval 160 (1590000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.1620\n",
      "200 episodes - episode_reward: 158.100 [-30.000, 260.000] - loss: 306.265 - mae: 163.751 - mean_q: 177.144\n",
      "\n",
      "Interval 161 (1600000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.2980\n",
      "200 episodes - episode_reward: 164.900 [-50.000, 280.000] - loss: 298.655 - mae: 160.597 - mean_q: 173.808\n",
      "\n",
      "Interval 162 (1610000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.4270\n",
      "200 episodes - episode_reward: 171.350 [-50.000, 310.000] - loss: 304.533 - mae: 162.209 - mean_q: 175.664\n",
      "\n",
      "Interval 163 (1620000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.5820\n",
      "200 episodes - episode_reward: 179.100 [-20.000, 270.000] - loss: 308.400 - mae: 163.983 - mean_q: 177.634\n",
      "\n",
      "Interval 164 (1630000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 3.6430\n",
      "200 episodes - episode_reward: 182.150 [0.000, 290.000] - loss: 324.818 - mae: 167.308 - mean_q: 181.684\n",
      "\n",
      "Interval 165 (1640000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.5810\n",
      "200 episodes - episode_reward: 179.050 [-50.000, 310.000] - loss: 329.289 - mae: 168.711 - mean_q: 183.257\n",
      "\n",
      "Interval 166 (1650000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.6510\n",
      "200 episodes - episode_reward: 182.550 [-40.000, 290.000] - loss: 327.483 - mae: 166.985 - mean_q: 181.400\n",
      "\n",
      "Interval 167 (1660000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.5660\n",
      "200 episodes - episode_reward: 178.300 [-50.000, 330.000] - loss: 325.817 - mae: 168.044 - mean_q: 182.564\n",
      "\n",
      "Interval 168 (1670000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 3.3580\n",
      "200 episodes - episode_reward: 167.900 [-50.000, 280.000] - loss: 335.915 - mae: 171.315 - mean_q: 185.625\n",
      "\n",
      "Interval 169 (1680000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.5990\n",
      "200 episodes - episode_reward: 179.950 [-50.000, 310.000] - loss: 334.290 - mae: 169.919 - mean_q: 184.283\n",
      "\n",
      "Interval 170 (1690000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.8240\n",
      "200 episodes - episode_reward: 191.200 [20.000, 280.000] - loss: 336.435 - mae: 169.877 - mean_q: 184.149\n",
      "\n",
      "Interval 171 (1700000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.7200\n",
      "200 episodes - episode_reward: 186.000 [-10.000, 300.000] - loss: 334.557 - mae: 170.188 - mean_q: 184.636\n",
      "\n",
      "Interval 172 (1710000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.7610\n",
      "200 episodes - episode_reward: 188.050 [-10.000, 290.000] - loss: 334.988 - mae: 171.411 - mean_q: 185.543\n",
      "\n",
      "Interval 173 (1720000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.1550\n",
      "200 episodes - episode_reward: 157.750 [-50.000, 290.000] - loss: 333.836 - mae: 169.258 - mean_q: 183.229\n",
      "\n",
      "Interval 174 (1730000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.2110\n",
      "200 episodes - episode_reward: 160.550 [-40.000, 280.000] - loss: 334.869 - mae: 171.192 - mean_q: 184.988\n",
      "\n",
      "Interval 175 (1740000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.5530\n",
      "200 episodes - episode_reward: 177.650 [-20.000, 310.000] - loss: 321.036 - mae: 166.222 - mean_q: 179.794\n",
      "\n",
      "Interval 176 (1750000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.7040\n",
      "200 episodes - episode_reward: 185.200 [30.000, 300.000] - loss: 318.910 - mae: 164.711 - mean_q: 178.281\n",
      "\n",
      "Interval 177 (1760000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 3.6010\n",
      "200 episodes - episode_reward: 180.050 [-40.000, 280.000] - loss: 321.994 - mae: 164.854 - mean_q: 178.486\n",
      "\n",
      "Interval 178 (1770000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 3.6650\n",
      "200 episodes - episode_reward: 183.250 [0.000, 310.000] - loss: 321.563 - mae: 165.895 - mean_q: 179.866\n",
      "\n",
      "Interval 179 (1780000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 3.7040\n",
      "200 episodes - episode_reward: 185.200 [20.000, 280.000] - loss: 327.041 - mae: 169.858 - mean_q: 184.183\n",
      "\n",
      "Interval 180 (1790000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 2.4570\n",
      "200 episodes - episode_reward: 122.850 [-30.000, 250.000] - loss: 369.861 - mae: 179.700 - mean_q: 194.660\n",
      "\n",
      "Interval 181 (1800000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 2.8290\n",
      "200 episodes - episode_reward: 141.450 [-50.000, 260.000] - loss: 334.292 - mae: 168.135 - mean_q: 182.045\n",
      "\n",
      "Interval 182 (1810000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.1380\n",
      "200 episodes - episode_reward: 156.900 [-50.000, 270.000] - loss: 280.610 - mae: 157.596 - mean_q: 170.803\n",
      "\n",
      "Interval 183 (1820000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.0490\n",
      "200 episodes - episode_reward: 152.450 [-40.000, 290.000] - loss: 289.034 - mae: 157.247 - mean_q: 170.187\n",
      "\n",
      "Interval 184 (1830000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.2470\n",
      "200 episodes - episode_reward: 162.350 [-40.000, 290.000] - loss: 269.975 - mae: 152.600 - mean_q: 165.192\n",
      "\n",
      "Interval 185 (1840000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.2810\n",
      "200 episodes - episode_reward: 164.050 [-40.000, 270.000] - loss: 260.818 - mae: 148.215 - mean_q: 160.317\n",
      "\n",
      "Interval 186 (1850000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.3550\n",
      "200 episodes - episode_reward: 167.750 [-40.000, 280.000] - loss: 261.774 - mae: 150.248 - mean_q: 162.667\n",
      "\n",
      "Interval 187 (1860000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.1090\n",
      "200 episodes - episode_reward: 155.450 [-40.000, 240.000] - loss: 274.518 - mae: 153.299 - mean_q: 165.820\n",
      "\n",
      "Interval 188 (1870000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.4090\n",
      "200 episodes - episode_reward: 170.450 [10.000, 280.000] - loss: 265.382 - mae: 151.606 - mean_q: 163.969\n",
      "\n",
      "Interval 189 (1880000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.5360\n",
      "200 episodes - episode_reward: 176.800 [-40.000, 260.000] - loss: 272.974 - mae: 153.373 - mean_q: 165.973\n",
      "\n",
      "Interval 190 (1890000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.4160\n",
      "200 episodes - episode_reward: 170.800 [-10.000, 270.000] - loss: 284.931 - mae: 155.783 - mean_q: 168.698\n",
      "\n",
      "Interval 191 (1900000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.5690\n",
      "200 episodes - episode_reward: 178.450 [20.000, 280.000] - loss: 290.400 - mae: 158.810 - mean_q: 171.876\n",
      "\n",
      "Interval 192 (1910000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.4970\n",
      "200 episodes - episode_reward: 174.850 [-10.000, 270.000] - loss: 295.674 - mae: 160.143 - mean_q: 173.378\n",
      "\n",
      "Interval 193 (1920000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.6440\n",
      "200 episodes - episode_reward: 182.200 [0.000, 300.000] - loss: 313.921 - mae: 165.418 - mean_q: 179.047\n",
      "\n",
      "Interval 194 (1930000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.2970\n",
      "200 episodes - episode_reward: 164.850 [-50.000, 290.000] - loss: 311.915 - mae: 164.858 - mean_q: 178.242\n",
      "\n",
      "Interval 195 (1940000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.6300\n",
      "200 episodes - episode_reward: 181.500 [-40.000, 290.000] - loss: 305.773 - mae: 161.500 - mean_q: 174.557\n",
      "\n",
      "Interval 196 (1950000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.6330\n",
      "200 episodes - episode_reward: 181.650 [-20.000, 340.000] - loss: 293.256 - mae: 159.284 - mean_q: 172.208\n",
      "\n",
      "Interval 197 (1960000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.3540\n",
      "200 episodes - episode_reward: 167.700 [-40.000, 290.000] - loss: 308.312 - mae: 162.858 - mean_q: 176.254\n",
      "\n",
      "Interval 198 (1970000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.7510\n",
      "200 episodes - episode_reward: 187.550 [-40.000, 280.000] - loss: 300.928 - mae: 161.621 - mean_q: 175.002\n",
      "\n",
      "Interval 199 (1980000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.6560\n",
      "200 episodes - episode_reward: 182.800 [-40.000, 290.000] - loss: 317.054 - mae: 164.076 - mean_q: 177.738\n",
      "\n",
      "Interval 200 (1990000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.5260\n",
      "200 episodes - episode_reward: 176.300 [-50.000, 300.000] - loss: 317.365 - mae: 164.157 - mean_q: 177.706\n",
      "\n",
      "Interval 201 (2000000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.7940\n",
      "200 episodes - episode_reward: 189.700 [-50.000, 320.000] - loss: 322.126 - mae: 164.953 - mean_q: 178.802\n",
      "\n",
      "Interval 202 (2010000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.7320\n",
      "200 episodes - episode_reward: 186.600 [-20.000, 290.000] - loss: 313.427 - mae: 163.759 - mean_q: 177.104\n",
      "\n",
      "Interval 203 (2020000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.6760\n",
      "200 episodes - episode_reward: 183.800 [10.000, 290.000] - loss: 313.919 - mae: 163.763 - mean_q: 177.110\n",
      "\n",
      "Interval 204 (2030000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.6780\n",
      "200 episodes - episode_reward: 183.900 [-10.000, 300.000] - loss: 310.972 - mae: 163.581 - mean_q: 176.888\n",
      "\n",
      "Interval 205 (2040000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.8680\n",
      "200 episodes - episode_reward: 193.400 [0.000, 300.000] - loss: 312.203 - mae: 166.322 - mean_q: 179.690\n",
      "\n",
      "Interval 206 (2050000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7510\n",
      "200 episodes - episode_reward: 187.550 [-10.000, 300.000] - loss: 327.677 - mae: 167.494 - mean_q: 180.989\n",
      "\n",
      "Interval 207 (2060000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.0320\n",
      "200 episodes - episode_reward: 151.600 [-50.000, 300.000] - loss: 342.046 - mae: 169.464 - mean_q: 183.113\n",
      "\n",
      "Interval 208 (2070000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.6320\n",
      "200 episodes - episode_reward: 181.600 [-50.000, 290.000] - loss: 329.755 - mae: 169.054 - mean_q: 182.772\n",
      "\n",
      "Interval 209 (2080000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.2360\n",
      "200 episodes - episode_reward: 161.800 [-50.000, 330.000] - loss: 375.685 - mae: 175.056 - mean_q: 189.587\n",
      "\n",
      "Interval 210 (2090000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.8300\n",
      "200 episodes - episode_reward: 191.500 [10.000, 300.000] - loss: 372.198 - mae: 175.293 - mean_q: 189.800\n",
      "\n",
      "Interval 211 (2100000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.0100\n",
      "200 episodes - episode_reward: 200.500 [-50.000, 330.000] - loss: 345.674 - mae: 171.358 - mean_q: 185.490\n",
      "\n",
      "Interval 212 (2110000 steps performed)\n",
      "10000/10000 [==============================] - 50s 5ms/step - reward: 3.8110\n",
      "200 episodes - episode_reward: 190.550 [-50.000, 300.000] - loss: 347.090 - mae: 172.229 - mean_q: 186.578\n",
      "\n",
      "Interval 213 (2120000 steps performed)\n",
      "10000/10000 [==============================] - 50s 5ms/step - reward: 3.8210\n",
      "200 episodes - episode_reward: 191.050 [-10.000, 300.000] - loss: 353.871 - mae: 175.172 - mean_q: 189.723\n",
      "\n",
      "Interval 214 (2130000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.5720\n",
      "200 episodes - episode_reward: 178.600 [0.000, 290.000] - loss: 357.497 - mae: 175.700 - mean_q: 190.226\n",
      "\n",
      "Interval 215 (2140000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.6200\n",
      "200 episodes - episode_reward: 181.000 [-30.000, 290.000] - loss: 349.077 - mae: 173.012 - mean_q: 187.302\n",
      "\n",
      "Interval 216 (2150000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.6810\n",
      "200 episodes - episode_reward: 184.050 [-50.000, 310.000] - loss: 340.653 - mae: 172.635 - mean_q: 186.670\n",
      "\n",
      "Interval 217 (2160000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7060\n",
      "200 episodes - episode_reward: 185.300 [40.000, 310.000] - loss: 337.008 - mae: 169.529 - mean_q: 183.344\n",
      "\n",
      "Interval 218 (2170000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.6350\n",
      "200 episodes - episode_reward: 181.750 [-10.000, 320.000] - loss: 318.576 - mae: 167.592 - mean_q: 180.987\n",
      "\n",
      "Interval 219 (2180000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.8890\n",
      "200 episodes - episode_reward: 194.450 [-20.000, 300.000] - loss: 317.885 - mae: 167.044 - mean_q: 180.289\n",
      "\n",
      "Interval 220 (2190000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.5740\n",
      "200 episodes - episode_reward: 178.700 [-40.000, 300.000] - loss: 313.460 - mae: 163.347 - mean_q: 176.411\n",
      "\n",
      "Interval 221 (2200000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7670\n",
      "200 episodes - episode_reward: 188.350 [-40.000, 320.000] - loss: 315.017 - mae: 164.101 - mean_q: 177.243\n",
      "\n",
      "Interval 222 (2210000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.5440\n",
      "200 episodes - episode_reward: 177.200 [-30.000, 290.000] - loss: 315.431 - mae: 163.005 - mean_q: 175.950\n",
      "\n",
      "Interval 223 (2220000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.8210\n",
      "200 episodes - episode_reward: 191.050 [0.000, 300.000] - loss: 311.043 - mae: 165.067 - mean_q: 178.185\n",
      "\n",
      "Interval 224 (2230000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.5750\n",
      "200 episodes - episode_reward: 178.750 [-50.000, 280.000] - loss: 316.299 - mae: 166.864 - mean_q: 180.131\n",
      "\n",
      "Interval 225 (2240000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 2.9750\n",
      "200 episodes - episode_reward: 148.750 [-50.000, 270.000] - loss: 365.163 - mae: 172.008 - mean_q: 185.704\n",
      "\n",
      "Interval 226 (2250000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.4410\n",
      "200 episodes - episode_reward: 172.050 [-40.000, 290.000] - loss: 367.363 - mae: 173.792 - mean_q: 187.643\n",
      "\n",
      "Interval 227 (2260000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 4.0200\n",
      "200 episodes - episode_reward: 201.000 [0.000, 320.000] - loss: 323.681 - mae: 168.888 - mean_q: 182.293\n",
      "\n",
      "Interval 228 (2270000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7480\n",
      "200 episodes - episode_reward: 187.400 [-30.000, 320.000] - loss: 319.363 - mae: 167.602 - mean_q: 180.773\n",
      "\n",
      "Interval 229 (2280000 steps performed)\n",
      "10000/10000 [==============================] - 50s 5ms/step - reward: 3.8060\n",
      "200 episodes - episode_reward: 190.300 [-50.000, 290.000] - loss: 322.663 - mae: 167.969 - mean_q: 181.355\n",
      "\n",
      "Interval 230 (2290000 steps performed)\n",
      "10000/10000 [==============================] - 50s 5ms/step - reward: 3.8610\n",
      "200 episodes - episode_reward: 193.050 [-40.000, 300.000] - loss: 322.791 - mae: 168.533 - mean_q: 182.077\n",
      "\n",
      "Interval 231 (2300000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 3.7470\n",
      "200 episodes - episode_reward: 187.350 [-40.000, 290.000] - loss: 329.974 - mae: 168.054 - mean_q: 181.907\n",
      "\n",
      "Interval 232 (2310000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.5620\n",
      "200 episodes - episode_reward: 178.100 [-40.000, 290.000] - loss: 324.863 - mae: 168.811 - mean_q: 182.734\n",
      "\n",
      "Interval 233 (2320000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.8950\n",
      "200 episodes - episode_reward: 194.750 [-30.000, 300.000] - loss: 326.935 - mae: 167.625 - mean_q: 181.558\n",
      "\n",
      "Interval 234 (2330000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 4.0330\n",
      "200 episodes - episode_reward: 201.650 [-10.000, 330.000] - loss: 331.277 - mae: 168.428 - mean_q: 182.359\n",
      "\n",
      "Interval 235 (2340000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.8810\n",
      "200 episodes - episode_reward: 194.050 [0.000, 330.000] - loss: 330.378 - mae: 169.006 - mean_q: 182.625\n",
      "\n",
      "Interval 236 (2350000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.4550\n",
      "200 episodes - episode_reward: 172.750 [-20.000, 290.000] - loss: 329.232 - mae: 171.000 - mean_q: 184.589\n",
      "\n",
      "Interval 237 (2360000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 3.5390\n",
      "200 episodes - episode_reward: 176.950 [-50.000, 310.000] - loss: 332.301 - mae: 171.212 - mean_q: 184.913\n",
      "\n",
      "Interval 238 (2370000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 3.7530\n",
      "200 episodes - episode_reward: 187.650 [10.000, 320.000] - loss: 326.170 - mae: 168.268 - mean_q: 181.747\n",
      "\n",
      "Interval 239 (2380000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.6480\n",
      "200 episodes - episode_reward: 182.400 [-20.000, 290.000] - loss: 317.333 - mae: 165.591 - mean_q: 178.678\n",
      "\n",
      "Interval 240 (2390000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 3.8660\n",
      "200 episodes - episode_reward: 193.300 [-20.000, 300.000] - loss: 308.097 - mae: 164.091 - mean_q: 177.040\n",
      "\n",
      "Interval 241 (2400000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 4.1320\n",
      "200 episodes - episode_reward: 206.600 [50.000, 300.000] - loss: 311.116 - mae: 165.569 - mean_q: 178.772\n",
      "\n",
      "Interval 242 (2410000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 3.5100\n",
      "200 episodes - episode_reward: 175.500 [-50.000, 310.000] - loss: 321.298 - mae: 167.775 - mean_q: 180.832\n",
      "\n",
      "Interval 243 (2420000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.9890\n",
      "200 episodes - episode_reward: 199.450 [-50.000, 300.000] - loss: 319.026 - mae: 166.088 - mean_q: 179.186\n",
      "\n",
      "Interval 244 (2430000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 4.0710\n",
      "200 episodes - episode_reward: 203.550 [-20.000, 360.000] - loss: 318.839 - mae: 165.500 - mean_q: 178.839\n",
      "\n",
      "Interval 245 (2440000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 3.6240\n",
      "200 episodes - episode_reward: 181.200 [-20.000, 290.000] - loss: 330.163 - mae: 168.657 - mean_q: 181.943\n",
      "\n",
      "Interval 246 (2450000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.4800\n",
      "200 episodes - episode_reward: 174.000 [-50.000, 300.000] - loss: 313.850 - mae: 165.170 - mean_q: 178.085\n",
      "\n",
      "Interval 247 (2460000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 3.8480\n",
      "200 episodes - episode_reward: 192.400 [-10.000, 330.000] - loss: 312.105 - mae: 163.471 - mean_q: 176.582\n",
      "\n",
      "Interval 248 (2470000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 4.1190\n",
      "200 episodes - episode_reward: 205.950 [-20.000, 320.000] - loss: 324.447 - mae: 166.795 - mean_q: 180.527\n",
      "\n",
      "Interval 249 (2480000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 4.1390\n",
      "200 episodes - episode_reward: 206.950 [-40.000, 310.000] - loss: 340.399 - mae: 171.720 - mean_q: 185.924\n",
      "\n",
      "Interval 250 (2490000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 4.2620\n",
      "200 episodes - episode_reward: 213.100 [-10.000, 340.000] - loss: 343.369 - mae: 172.635 - mean_q: 186.953\n",
      "\n",
      "Interval 251 (2500000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 4.0870\n",
      "200 episodes - episode_reward: 204.350 [-50.000, 320.000] - loss: 352.640 - mae: 174.419 - mean_q: 188.924\n",
      "\n",
      "Interval 252 (2510000 steps performed)\n",
      "10000/10000 [==============================] - 61s 6ms/step - reward: 3.7420\n",
      "200 episodes - episode_reward: 187.100 [-10.000, 310.000] - loss: 363.219 - mae: 176.097 - mean_q: 190.798\n",
      "\n",
      "Interval 253 (2520000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 3.8810\n",
      "200 episodes - episode_reward: 194.050 [-40.000, 290.000] - loss: 362.188 - mae: 176.095 - mean_q: 190.498\n",
      "\n",
      "Interval 254 (2530000 steps performed)\n",
      "10000/10000 [==============================] - 65s 7ms/step - reward: 3.8720\n",
      "200 episodes - episode_reward: 193.600 [-10.000, 320.000] - loss: 349.554 - mae: 172.702 - mean_q: 186.853\n",
      "\n",
      "Interval 255 (2540000 steps performed)\n",
      "10000/10000 [==============================] - 71s 7ms/step - reward: 3.8740\n",
      "200 episodes - episode_reward: 193.700 [-20.000, 320.000] - loss: 342.763 - mae: 171.200 - mean_q: 185.315\n",
      "\n",
      "Interval 256 (2550000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 3.6160\n",
      "200 episodes - episode_reward: 180.800 [-40.000, 280.000] - loss: 335.455 - mae: 169.118 - mean_q: 183.068\n",
      "\n",
      "Interval 257 (2560000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 4.0140\n",
      "200 episodes - episode_reward: 200.700 [30.000, 330.000] - loss: 324.590 - mae: 167.553 - mean_q: 181.301\n",
      "\n",
      "Interval 258 (2570000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 4.0850\n",
      "200 episodes - episode_reward: 204.250 [20.000, 290.000] - loss: 333.104 - mae: 169.301 - mean_q: 183.477\n",
      "\n",
      "Interval 259 (2580000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 4.0520\n",
      "200 episodes - episode_reward: 202.600 [80.000, 290.000] - loss: 348.092 - mae: 173.571 - mean_q: 188.076\n",
      "\n",
      "Interval 260 (2590000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.9410\n",
      "200 episodes - episode_reward: 197.050 [10.000, 290.000] - loss: 346.782 - mae: 173.408 - mean_q: 187.702\n",
      "\n",
      "Interval 261 (2600000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 3.9180\n",
      "200 episodes - episode_reward: 195.900 [-20.000, 310.000] - loss: 349.648 - mae: 174.219 - mean_q: 188.441\n",
      "\n",
      "Interval 262 (2610000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.7520\n",
      "200 episodes - episode_reward: 187.600 [-20.000, 350.000] - loss: 345.133 - mae: 174.286 - mean_q: 188.044\n",
      "\n",
      "Interval 263 (2620000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 4.0160\n",
      "200 episodes - episode_reward: 200.800 [10.000, 300.000] - loss: 339.457 - mae: 172.226 - mean_q: 185.566\n",
      "\n",
      "Interval 264 (2630000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.8260\n",
      "200 episodes - episode_reward: 191.300 [50.000, 280.000] - loss: 341.380 - mae: 171.305 - mean_q: 184.817\n",
      "\n",
      "Interval 265 (2640000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.9080\n",
      "200 episodes - episode_reward: 195.400 [30.000, 310.000] - loss: 338.871 - mae: 170.670 - mean_q: 184.212\n",
      "\n",
      "Interval 266 (2650000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.8140\n",
      "200 episodes - episode_reward: 190.700 [-10.000, 290.000] - loss: 342.784 - mae: 173.306 - mean_q: 186.939\n",
      "\n",
      "Interval 267 (2660000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 4.1790\n",
      "200 episodes - episode_reward: 208.950 [0.000, 340.000] - loss: 338.323 - mae: 172.365 - mean_q: 186.262\n",
      "\n",
      "Interval 268 (2670000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 4.0640\n",
      "200 episodes - episode_reward: 203.200 [-20.000, 310.000] - loss: 343.540 - mae: 171.195 - mean_q: 185.172\n",
      "\n",
      "Interval 269 (2680000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.9800\n",
      "200 episodes - episode_reward: 199.000 [-30.000, 290.000] - loss: 337.340 - mae: 168.869 - mean_q: 182.630\n",
      "\n",
      "Interval 270 (2690000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 4.0280\n",
      "200 episodes - episode_reward: 201.400 [-30.000, 300.000] - loss: 339.902 - mae: 171.039 - mean_q: 185.024\n",
      "\n",
      "Interval 271 (2700000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 4.2120\n",
      "200 episodes - episode_reward: 210.600 [100.000, 300.000] - loss: 342.555 - mae: 171.283 - mean_q: 185.161\n",
      "\n",
      "Interval 272 (2710000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.9420\n",
      "200 episodes - episode_reward: 197.100 [40.000, 320.000] - loss: 349.823 - mae: 172.571 - mean_q: 186.436\n",
      "\n",
      "Interval 273 (2720000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.7680\n",
      "200 episodes - episode_reward: 188.400 [-50.000, 280.000] - loss: 347.959 - mae: 173.770 - mean_q: 187.793\n",
      "\n",
      "Interval 274 (2730000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.9270\n",
      "200 episodes - episode_reward: 196.350 [-30.000, 290.000] - loss: 341.605 - mae: 171.344 - mean_q: 185.083\n",
      "\n",
      "Interval 275 (2740000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.9410\n",
      "200 episodes - episode_reward: 197.050 [-30.000, 290.000] - loss: 334.574 - mae: 170.013 - mean_q: 183.957\n",
      "\n",
      "Interval 276 (2750000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.4170\n",
      "200 episodes - episode_reward: 170.850 [-40.000, 290.000] - loss: 344.139 - mae: 173.976 - mean_q: 188.175\n",
      "\n",
      "Interval 277 (2760000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 4.0540\n",
      "200 episodes - episode_reward: 202.700 [-50.000, 300.000] - loss: 341.781 - mae: 171.913 - mean_q: 186.198\n",
      "\n",
      "Interval 278 (2770000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 4.1370\n",
      "200 episodes - episode_reward: 206.850 [-50.000, 310.000] - loss: 337.179 - mae: 172.359 - mean_q: 186.959\n",
      "\n",
      "Interval 279 (2780000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.8920\n",
      "200 episodes - episode_reward: 194.600 [-10.000, 310.000] - loss: 348.836 - mae: 173.835 - mean_q: 188.250\n",
      "\n",
      "Interval 280 (2790000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 4.1940\n",
      "200 episodes - episode_reward: 209.700 [30.000, 340.000] - loss: 345.014 - mae: 173.232 - mean_q: 187.719\n",
      "\n",
      "Interval 281 (2800000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 4.1420\n",
      "200 episodes - episode_reward: 207.100 [40.000, 330.000] - loss: 346.071 - mae: 174.320 - mean_q: 188.493\n",
      "\n",
      "Interval 282 (2810000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.7720\n",
      "200 episodes - episode_reward: 188.600 [-10.000, 280.000] - loss: 346.204 - mae: 173.353 - mean_q: 187.121\n",
      "\n",
      "Interval 283 (2820000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.9280\n",
      "200 episodes - episode_reward: 196.400 [-40.000, 310.000] - loss: 337.582 - mae: 171.264 - mean_q: 184.672\n",
      "\n",
      "Interval 284 (2830000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.7200\n",
      "200 episodes - episode_reward: 186.000 [-50.000, 310.000] - loss: 345.906 - mae: 173.587 - mean_q: 187.314\n",
      "\n",
      "Interval 285 (2840000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.9000\n",
      "200 episodes - episode_reward: 195.000 [30.000, 300.000] - loss: 336.382 - mae: 171.449 - mean_q: 184.971\n",
      "\n",
      "Interval 286 (2850000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 4.0160\n",
      "200 episodes - episode_reward: 200.800 [-50.000, 320.000] - loss: 337.161 - mae: 170.178 - mean_q: 184.030\n",
      "\n",
      "Interval 287 (2860000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 4.0410\n",
      "200 episodes - episode_reward: 202.050 [0.000, 300.000] - loss: 345.567 - mae: 172.468 - mean_q: 186.516\n",
      "\n",
      "Interval 288 (2870000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 4.1210\n",
      "200 episodes - episode_reward: 206.050 [-10.000, 320.000] - loss: 338.030 - mae: 171.809 - mean_q: 185.510\n",
      "\n",
      "Interval 289 (2880000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.7880\n",
      "200 episodes - episode_reward: 189.400 [-40.000, 320.000] - loss: 344.621 - mae: 170.887 - mean_q: 184.569\n",
      "\n",
      "Interval 290 (2890000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.6500\n",
      "200 episodes - episode_reward: 182.500 [0.000, 320.000] - loss: 359.262 - mae: 176.838 - mean_q: 191.054\n",
      "\n",
      "Interval 291 (2900000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.5900\n",
      "200 episodes - episode_reward: 179.500 [-10.000, 320.000] - loss: 354.325 - mae: 175.063 - mean_q: 188.928\n",
      "\n",
      "Interval 292 (2910000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 3.8570\n",
      "200 episodes - episode_reward: 192.850 [-30.000, 300.000] - loss: 353.855 - mae: 173.985 - mean_q: 187.947\n",
      "\n",
      "Interval 293 (2920000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 4.0680\n",
      "200 episodes - episode_reward: 203.400 [70.000, 310.000] - loss: 339.577 - mae: 171.419 - mean_q: 185.264\n",
      "\n",
      "Interval 294 (2930000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 4.0410\n",
      "200 episodes - episode_reward: 202.050 [-50.000, 310.000] - loss: 337.189 - mae: 173.984 - mean_q: 188.145\n",
      "\n",
      "Interval 295 (2940000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 4.1470\n",
      "200 episodes - episode_reward: 207.350 [-20.000, 310.000] - loss: 343.178 - mae: 173.618 - mean_q: 187.793\n",
      "\n",
      "Interval 296 (2950000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 3.7200\n",
      "200 episodes - episode_reward: 186.000 [-40.000, 290.000] - loss: 351.260 - mae: 173.680 - mean_q: 188.203\n",
      "\n",
      "Interval 297 (2960000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 56s 6ms/step - reward: 4.3890\n",
      "200 episodes - episode_reward: 219.450 [70.000, 300.000] - loss: 353.150 - mae: 176.006 - mean_q: 190.787\n",
      "\n",
      "Interval 298 (2970000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 4.1140\n",
      "200 episodes - episode_reward: 205.700 [60.000, 330.000] - loss: 360.830 - mae: 175.630 - mean_q: 190.333\n",
      "\n",
      "Interval 299 (2980000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 4.1910\n",
      "200 episodes - episode_reward: 209.550 [-20.000, 320.000] - loss: 356.955 - mae: 175.688 - mean_q: 190.101\n",
      "\n",
      "Interval 300 (2990000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 4.1150\n",
      "200 episodes - episode_reward: 205.750 [50.000, 310.000] - loss: 347.215 - mae: 170.406 - mean_q: 184.162\n",
      "\n",
      "Interval 301 (3000000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.7990\n",
      "200 episodes - episode_reward: 189.950 [0.000, 290.000] - loss: 340.222 - mae: 170.077 - mean_q: 183.622\n",
      "\n",
      "Interval 302 (3010000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.8710\n",
      "200 episodes - episode_reward: 193.550 [-30.000, 290.000] - loss: 336.376 - mae: 172.422 - mean_q: 185.980\n",
      "\n",
      "Interval 303 (3020000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.8060\n",
      "200 episodes - episode_reward: 190.300 [-10.000, 330.000] - loss: 330.782 - mae: 170.520 - mean_q: 183.722\n",
      "\n",
      "Interval 304 (3030000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.2940\n",
      "200 episodes - episode_reward: 164.700 [-10.000, 290.000] - loss: 329.333 - mae: 168.126 - mean_q: 181.348\n",
      "\n",
      "Interval 305 (3040000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.3220\n",
      "200 episodes - episode_reward: 166.100 [-40.000, 290.000] - loss: 316.024 - mae: 164.924 - mean_q: 177.773\n",
      "\n",
      "Interval 306 (3050000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.4480\n",
      "200 episodes - episode_reward: 172.400 [-20.000, 280.000] - loss: 298.477 - mae: 161.079 - mean_q: 173.552\n",
      "\n",
      "Interval 307 (3060000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.4480\n",
      "200 episodes - episode_reward: 172.400 [-40.000, 280.000] - loss: 286.466 - mae: 157.170 - mean_q: 169.416\n",
      "\n",
      "Interval 308 (3070000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.4780\n",
      "200 episodes - episode_reward: 123.900 [-30.000, 280.000] - loss: 282.928 - mae: 156.394 - mean_q: 168.486\n",
      "\n",
      "Interval 309 (3080000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.6170\n",
      "200 episodes - episode_reward: 130.850 [-50.000, 250.000] - loss: 290.806 - mae: 156.650 - mean_q: 168.751\n",
      "\n",
      "Interval 310 (3090000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.8050\n",
      "200 episodes - episode_reward: 140.250 [-50.000, 250.000] - loss: 287.354 - mae: 155.727 - mean_q: 167.953\n",
      "\n",
      "Interval 311 (3100000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.9970\n",
      "200 episodes - episode_reward: 149.850 [-50.000, 280.000] - loss: 257.831 - mae: 150.253 - mean_q: 161.861\n",
      "\n",
      "Interval 312 (3110000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.3220\n",
      "200 episodes - episode_reward: 166.100 [20.000, 310.000] - loss: 254.842 - mae: 149.529 - mean_q: 161.200\n",
      "\n",
      "Interval 313 (3120000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.2660\n",
      "200 episodes - episode_reward: 163.300 [-40.000, 280.000] - loss: 252.311 - mae: 147.404 - mean_q: 158.968\n",
      "\n",
      "Interval 314 (3130000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 3.1060\n",
      "200 episodes - episode_reward: 155.300 [-30.000, 290.000] - loss: 250.790 - mae: 147.612 - mean_q: 159.136\n",
      "\n",
      "Interval 315 (3140000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.4030\n",
      "200 episodes - episode_reward: 170.150 [0.000, 280.000] - loss: 256.226 - mae: 148.818 - mean_q: 160.658\n",
      "\n",
      "Interval 316 (3150000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.4710\n",
      "200 episodes - episode_reward: 173.550 [-30.000, 270.000] - loss: 258.217 - mae: 148.586 - mean_q: 160.469\n",
      "\n",
      "Interval 317 (3160000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 3.1790\n",
      "200 episodes - episode_reward: 158.950 [-40.000, 280.000] - loss: 255.425 - mae: 150.009 - mean_q: 161.940\n",
      "\n",
      "Interval 318 (3170000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.1070\n",
      "200 episodes - episode_reward: 155.350 [-30.000, 280.000] - loss: 255.521 - mae: 148.639 - mean_q: 160.563\n",
      "\n",
      "Interval 319 (3180000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.9110\n",
      "200 episodes - episode_reward: 145.550 [-30.000, 270.000] - loss: 250.291 - mae: 146.538 - mean_q: 158.325\n",
      "\n",
      "Interval 320 (3190000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.1900\n",
      "200 episodes - episode_reward: 159.500 [-50.000, 260.000] - loss: 239.938 - mae: 142.994 - mean_q: 154.358\n",
      "\n",
      "Interval 321 (3200000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.3290\n",
      "200 episodes - episode_reward: 166.450 [-10.000, 280.000] - loss: 238.852 - mae: 143.400 - mean_q: 155.069\n",
      "\n",
      "Interval 322 (3210000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.1080\n",
      "200 episodes - episode_reward: 155.400 [-10.000, 300.000] - loss: 241.938 - mae: 144.730 - mean_q: 156.407\n",
      "\n",
      "Interval 323 (3220000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.5450\n",
      "200 episodes - episode_reward: 177.250 [10.000, 270.000] - loss: 239.310 - mae: 143.644 - mean_q: 155.207\n",
      "\n",
      "Interval 324 (3230000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.5260\n",
      "200 episodes - episode_reward: 176.300 [-30.000, 290.000] - loss: 248.914 - mae: 146.141 - mean_q: 158.072\n",
      "\n",
      "Interval 325 (3240000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.0000\n",
      "200 episodes - episode_reward: 150.000 [-40.000, 280.000] - loss: 260.427 - mae: 150.194 - mean_q: 162.085\n",
      "\n",
      "Interval 326 (3250000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.1260\n",
      "200 episodes - episode_reward: 156.300 [-40.000, 270.000] - loss: 253.401 - mae: 148.299 - mean_q: 159.868\n",
      "\n",
      "Interval 327 (3260000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 3.2710\n",
      "200 episodes - episode_reward: 163.550 [-20.000, 290.000] - loss: 253.860 - mae: 147.099 - mean_q: 158.907\n",
      "\n",
      "Interval 328 (3270000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.2750\n",
      "200 episodes - episode_reward: 163.750 [-30.000, 280.000] - loss: 260.693 - mae: 149.848 - mean_q: 161.726\n",
      "\n",
      "Interval 329 (3280000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.5200\n",
      "200 episodes - episode_reward: 176.000 [-30.000, 290.000] - loss: 260.222 - mae: 149.316 - mean_q: 161.042\n",
      "\n",
      "Interval 330 (3290000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.5620\n",
      "200 episodes - episode_reward: 178.100 [0.000, 280.000] - loss: 261.889 - mae: 149.165 - mean_q: 161.271\n",
      "\n",
      "Interval 331 (3300000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.6370\n",
      "200 episodes - episode_reward: 181.850 [30.000, 300.000] - loss: 262.554 - mae: 150.247 - mean_q: 162.420\n",
      "\n",
      "Interval 332 (3310000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.0900\n",
      "200 episodes - episode_reward: 154.500 [-50.000, 310.000] - loss: 268.415 - mae: 152.104 - mean_q: 164.013\n",
      "\n",
      "Interval 333 (3320000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.9420\n",
      "200 episodes - episode_reward: 147.100 [-40.000, 260.000] - loss: 265.380 - mae: 151.979 - mean_q: 164.057\n",
      "\n",
      "Interval 334 (3330000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.4600\n",
      "200 episodes - episode_reward: 173.000 [-20.000, 310.000] - loss: 255.042 - mae: 149.751 - mean_q: 161.548\n",
      "\n",
      "Interval 335 (3340000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.2750\n",
      "200 episodes - episode_reward: 163.750 [-30.000, 250.000] - loss: 251.123 - mae: 145.901 - mean_q: 157.549\n",
      "\n",
      "Interval 336 (3350000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.3190\n",
      "200 episodes - episode_reward: 165.950 [-30.000, 280.000] - loss: 239.893 - mae: 144.508 - mean_q: 155.820\n",
      "\n",
      "Interval 337 (3360000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.4090\n",
      "200 episodes - episode_reward: 170.450 [10.000, 290.000] - loss: 239.589 - mae: 144.670 - mean_q: 155.896\n",
      "\n",
      "Interval 338 (3370000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.3660\n",
      "200 episodes - episode_reward: 168.300 [-40.000, 320.000] - loss: 240.855 - mae: 142.919 - mean_q: 154.086\n",
      "\n",
      "Interval 339 (3380000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.1510\n",
      "200 episodes - episode_reward: 157.550 [-30.000, 290.000] - loss: 255.303 - mae: 148.215 - mean_q: 159.914\n",
      "\n",
      "Interval 340 (3390000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 2.8920\n",
      "200 episodes - episode_reward: 144.600 [-40.000, 280.000] - loss: 262.018 - mae: 150.329 - mean_q: 162.023\n",
      "\n",
      "Interval 341 (3400000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.4090\n",
      "200 episodes - episode_reward: 170.450 [10.000, 280.000] - loss: 250.097 - mae: 147.013 - mean_q: 158.350\n",
      "\n",
      "Interval 342 (3410000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.2270\n",
      "200 episodes - episode_reward: 161.350 [-50.000, 260.000] - loss: 249.065 - mae: 145.453 - mean_q: 156.762\n",
      "\n",
      "Interval 343 (3420000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.3780\n",
      "200 episodes - episode_reward: 168.900 [-30.000, 300.000] - loss: 239.161 - mae: 143.425 - mean_q: 154.594\n",
      "\n",
      "Interval 344 (3430000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 2.8250\n",
      "200 episodes - episode_reward: 141.250 [-30.000, 270.000] - loss: 238.838 - mae: 142.603 - mean_q: 153.728\n",
      "\n",
      "Interval 345 (3440000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.2560\n",
      "200 episodes - episode_reward: 162.800 [-30.000, 310.000] - loss: 238.542 - mae: 142.002 - mean_q: 152.928\n",
      "\n",
      "Interval 346 (3450000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.5240\n",
      "200 episodes - episode_reward: 176.200 [-40.000, 300.000] - loss: 229.486 - mae: 143.168 - mean_q: 154.239\n",
      "\n",
      "Interval 347 (3460000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.7250\n",
      "200 episodes - episode_reward: 186.250 [20.000, 290.000] - loss: 248.579 - mae: 146.327 - mean_q: 157.760\n",
      "\n",
      "Interval 348 (3470000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.5780\n",
      "200 episodes - episode_reward: 178.900 [-50.000, 290.000] - loss: 255.446 - mae: 150.091 - mean_q: 161.640\n",
      "\n",
      "Interval 349 (3480000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.0520\n",
      "200 episodes - episode_reward: 152.600 [-40.000, 260.000] - loss: 258.171 - mae: 148.690 - mean_q: 160.214\n",
      "\n",
      "Interval 350 (3490000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 2.9810\n",
      "200 episodes - episode_reward: 149.050 [0.000, 290.000] - loss: 256.202 - mae: 149.387 - mean_q: 161.017\n",
      "\n",
      "Interval 351 (3500000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 1.4740\n",
      "200 episodes - episode_reward: 73.700 [-50.000, 270.000] - loss: 270.342 - mae: 153.145 - mean_q: 164.890\n",
      "\n",
      "Interval 352 (3510000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 2.5900\n",
      "200 episodes - episode_reward: 129.500 [-40.000, 260.000] - loss: 252.003 - mae: 147.063 - mean_q: 158.496\n",
      "\n",
      "Interval 353 (3520000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.5710\n",
      "200 episodes - episode_reward: 178.550 [-30.000, 280.000] - loss: 240.834 - mae: 143.785 - mean_q: 155.285\n",
      "\n",
      "Interval 354 (3530000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 3.8210\n",
      "200 episodes - episode_reward: 191.050 [-40.000, 290.000] - loss: 254.149 - mae: 146.463 - mean_q: 158.236\n",
      "\n",
      "Interval 355 (3540000 steps performed)\n",
      "10000/10000 [==============================] - 59s 6ms/step - reward: 3.4280\n",
      "200 episodes - episode_reward: 171.400 [-40.000, 310.000] - loss: 262.169 - mae: 151.647 - mean_q: 163.810\n",
      "\n",
      "Interval 356 (3550000 steps performed)\n",
      "10000/10000 [==============================] - 59s 6ms/step - reward: 3.2480\n",
      "200 episodes - episode_reward: 162.400 [-30.000, 260.000] - loss: 269.935 - mae: 151.402 - mean_q: 163.487\n",
      "\n",
      "Interval 357 (3560000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 3.1650\n",
      "200 episodes - episode_reward: 158.250 [-30.000, 280.000] - loss: 270.520 - mae: 152.435 - mean_q: 164.736\n",
      "\n",
      "Interval 358 (3570000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 3.1730\n",
      "200 episodes - episode_reward: 158.650 [-50.000, 280.000] - loss: 265.285 - mae: 152.100 - mean_q: 163.952\n",
      "\n",
      "Interval 359 (3580000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 3.4960\n",
      "200 episodes - episode_reward: 174.800 [-20.000, 270.000] - loss: 251.447 - mae: 147.206 - mean_q: 158.640\n",
      "\n",
      "Interval 360 (3590000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 3.3820\n",
      "200 episodes - episode_reward: 169.100 [-30.000, 270.000] - loss: 253.395 - mae: 148.644 - mean_q: 160.306\n",
      "\n",
      "Interval 361 (3600000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 3.4060\n",
      "200 episodes - episode_reward: 170.300 [-50.000, 270.000] - loss: 251.304 - mae: 148.910 - mean_q: 160.435\n",
      "\n",
      "Interval 362 (3610000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.9650\n",
      "200 episodes - episode_reward: 98.250 [-50.000, 270.000] - loss: 258.914 - mae: 149.539 - mean_q: 160.816\n",
      "\n",
      "Interval 363 (3620000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 2.7020\n",
      "200 episodes - episode_reward: 135.100 [-40.000, 250.000] - loss: 247.072 - mae: 144.480 - mean_q: 155.539\n",
      "\n",
      "Interval 364 (3630000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.3250\n",
      "200 episodes - episode_reward: 166.250 [-20.000, 270.000] - loss: 230.124 - mae: 140.871 - mean_q: 151.766\n",
      "\n",
      "Interval 365 (3640000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.3020\n",
      "200 episodes - episode_reward: 165.100 [-40.000, 280.000] - loss: 229.187 - mae: 141.012 - mean_q: 152.040\n",
      "\n",
      "Interval 366 (3650000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 3.1920\n",
      "200 episodes - episode_reward: 159.600 [-40.000, 280.000] - loss: 235.374 - mae: 141.061 - mean_q: 152.048\n",
      "\n",
      "Interval 367 (3660000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 2.9130\n",
      "200 episodes - episode_reward: 145.650 [-40.000, 290.000] - loss: 222.522 - mae: 137.651 - mean_q: 148.502\n",
      "\n",
      "Interval 368 (3670000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 2.7520\n",
      "200 episodes - episode_reward: 137.600 [-40.000, 250.000] - loss: 234.483 - mae: 142.454 - mean_q: 153.822\n",
      "\n",
      "Interval 369 (3680000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.6780\n",
      "200 episodes - episode_reward: 133.900 [-50.000, 260.000] - loss: 241.131 - mae: 143.146 - mean_q: 154.393\n",
      "\n",
      "Interval 370 (3690000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.5210\n",
      "200 episodes - episode_reward: 176.050 [30.000, 300.000] - loss: 235.308 - mae: 141.806 - mean_q: 153.070\n",
      "\n",
      "Interval 371 (3700000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.1960\n",
      "200 episodes - episode_reward: 159.800 [-50.000, 270.000] - loss: 244.813 - mae: 144.532 - mean_q: 156.216\n",
      "\n",
      "Interval 372 (3710000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.4790\n",
      "200 episodes - episode_reward: 173.950 [20.000, 270.000] - loss: 259.477 - mae: 149.041 - mean_q: 161.428\n",
      "\n",
      "Interval 373 (3720000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.5110\n",
      "200 episodes - episode_reward: 175.550 [50.000, 270.000] - loss: 270.364 - mae: 151.723 - mean_q: 164.141\n",
      "\n",
      "Interval 374 (3730000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 3.1460\n",
      "200 episodes - episode_reward: 157.300 [-20.000, 300.000] - loss: 270.598 - mae: 152.341 - mean_q: 164.732\n",
      "\n",
      "Interval 375 (3740000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.9970\n",
      "200 episodes - episode_reward: 149.850 [-40.000, 260.000] - loss: 258.584 - mae: 149.675 - mean_q: 161.664\n",
      "\n",
      "Interval 376 (3750000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 3.0710\n",
      "200 episodes - episode_reward: 153.550 [10.000, 270.000] - loss: 245.276 - mae: 144.669 - mean_q: 156.321\n",
      "\n",
      "Interval 377 (3760000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 3.2840\n",
      "200 episodes - episode_reward: 164.200 [20.000, 280.000] - loss: 234.576 - mae: 140.353 - mean_q: 151.622\n",
      "\n",
      "Interval 378 (3770000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 3.2710\n",
      "200 episodes - episode_reward: 163.550 [-30.000, 290.000] - loss: 228.602 - mae: 140.684 - mean_q: 151.964\n",
      "\n",
      "Interval 379 (3780000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.1210\n",
      "200 episodes - episode_reward: 156.050 [-40.000, 270.000] - loss: 236.047 - mae: 144.761 - mean_q: 156.149\n",
      "\n",
      "Interval 380 (3790000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 3.5800\n",
      "200 episodes - episode_reward: 179.000 [-40.000, 290.000] - loss: 242.472 - mae: 143.537 - mean_q: 154.723\n",
      "\n",
      "Interval 381 (3800000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.4660\n",
      "200 episodes - episode_reward: 173.300 [-50.000, 300.000] - loss: 244.139 - mae: 147.231 - mean_q: 158.798\n",
      "\n",
      "Interval 382 (3810000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.1910\n",
      "200 episodes - episode_reward: 159.550 [-50.000, 320.000] - loss: 262.310 - mae: 151.326 - mean_q: 163.147\n",
      "\n",
      "Interval 383 (3820000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.4990\n",
      "200 episodes - episode_reward: 174.950 [-40.000, 280.000] - loss: 266.210 - mae: 151.777 - mean_q: 163.520\n",
      "\n",
      "Interval 384 (3830000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 3.5040\n",
      "200 episodes - episode_reward: 175.200 [-30.000, 290.000] - loss: 260.155 - mae: 149.754 - mean_q: 161.113\n",
      "\n",
      "Interval 385 (3840000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 3.1800\n",
      "200 episodes - episode_reward: 159.000 [-10.000, 270.000] - loss: 260.036 - mae: 149.039 - mean_q: 160.559\n",
      "\n",
      "Interval 386 (3850000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.9900\n",
      "200 episodes - episode_reward: 149.500 [-30.000, 300.000] - loss: 256.127 - mae: 149.455 - mean_q: 160.773\n",
      "\n",
      "Interval 387 (3860000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.3550\n",
      "200 episodes - episode_reward: 167.750 [-50.000, 270.000] - loss: 236.183 - mae: 143.326 - mean_q: 154.301\n",
      "\n",
      "Interval 388 (3870000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.4440\n",
      "200 episodes - episode_reward: 172.200 [-30.000, 310.000] - loss: 232.652 - mae: 141.534 - mean_q: 152.594\n",
      "\n",
      "Interval 389 (3880000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.0530\n",
      "200 episodes - episode_reward: 152.650 [-20.000, 260.000] - loss: 235.416 - mae: 141.499 - mean_q: 152.678\n",
      "\n",
      "Interval 390 (3890000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.2030\n",
      "200 episodes - episode_reward: 160.150 [-30.000, 270.000] - loss: 235.308 - mae: 141.170 - mean_q: 152.565\n",
      "\n",
      "Interval 391 (3900000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.7210\n",
      "200 episodes - episode_reward: 186.050 [20.000, 310.000] - loss: 235.790 - mae: 141.039 - mean_q: 152.567\n",
      "\n",
      "Interval 392 (3910000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 3.3410\n",
      "200 episodes - episode_reward: 167.050 [-10.000, 300.000] - loss: 255.092 - mae: 149.429 - mean_q: 161.480\n",
      "\n",
      "Interval 393 (3920000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 3.3810\n",
      "200 episodes - episode_reward: 169.050 [-40.000, 260.000] - loss: 255.731 - mae: 149.353 - mean_q: 160.930\n",
      "\n",
      "Interval 394 (3930000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 3.5040\n",
      "200 episodes - episode_reward: 175.200 [-50.000, 270.000] - loss: 250.246 - mae: 147.509 - mean_q: 159.097\n",
      "\n",
      "Interval 395 (3940000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.0370\n",
      "200 episodes - episode_reward: 151.850 [-50.000, 260.000] - loss: 262.454 - mae: 149.596 - mean_q: 161.476\n",
      "\n",
      "Interval 396 (3950000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.4890\n",
      "200 episodes - episode_reward: 174.450 [-50.000, 290.000] - loss: 258.727 - mae: 147.173 - mean_q: 158.853\n",
      "\n",
      "Interval 397 (3960000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.6660\n",
      "200 episodes - episode_reward: 183.300 [-50.000, 300.000] - loss: 259.481 - mae: 149.484 - mean_q: 161.518\n",
      "\n",
      "Interval 398 (3970000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.4320\n",
      "200 episodes - episode_reward: 171.600 [-20.000, 280.000] - loss: 268.967 - mae: 151.920 - mean_q: 163.840\n",
      "\n",
      "Interval 399 (3980000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 3.1050\n",
      "200 episodes - episode_reward: 155.250 [-40.000, 270.000] - loss: 267.654 - mae: 152.576 - mean_q: 164.413\n",
      "\n",
      "Interval 400 (3990000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 3.1770\n",
      "200 episodes - episode_reward: 158.850 [-40.000, 270.000] - loss: 266.629 - mae: 150.505 - mean_q: 162.290\n",
      "\n",
      "Interval 401 (4000000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 3.6260\n",
      "200 episodes - episode_reward: 181.300 [30.000, 280.000] - loss: 257.718 - mae: 146.971 - mean_q: 158.624\n",
      "\n",
      "Interval 402 (4010000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 3.5890\n",
      "200 episodes - episode_reward: 179.450 [-40.000, 270.000] - loss: 253.705 - mae: 148.279 - mean_q: 159.886\n",
      "\n",
      "Interval 403 (4020000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.6850\n",
      "200 episodes - episode_reward: 184.250 [0.000, 290.000] - loss: 263.538 - mae: 149.771 - mean_q: 161.691\n",
      "\n",
      "Interval 404 (4030000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 3.3330\n",
      "200 episodes - episode_reward: 166.650 [0.000, 280.000] - loss: 259.551 - mae: 149.707 - mean_q: 161.328\n",
      "\n",
      "Interval 405 (4040000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.4380\n",
      "200 episodes - episode_reward: 171.900 [-40.000, 260.000] - loss: 255.481 - mae: 147.755 - mean_q: 159.072\n",
      "\n",
      "Interval 406 (4050000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.1470\n",
      "200 episodes - episode_reward: 157.350 [0.000, 290.000] - loss: 249.704 - mae: 147.115 - mean_q: 158.410\n",
      "\n",
      "Interval 407 (4060000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.0560\n",
      "200 episodes - episode_reward: 152.800 [-40.000, 260.000] - loss: 248.158 - mae: 146.171 - mean_q: 157.195\n",
      "\n",
      "Interval 408 (4070000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.9210\n",
      "200 episodes - episode_reward: 146.050 [-30.000, 310.000] - loss: 240.470 - mae: 143.821 - mean_q: 154.787\n",
      "\n",
      "Interval 409 (4080000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.2810\n",
      "200 episodes - episode_reward: 164.050 [-10.000, 310.000] - loss: 243.218 - mae: 143.398 - mean_q: 154.464\n",
      "\n",
      "Interval 410 (4090000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.5750\n",
      "200 episodes - episode_reward: 178.750 [-40.000, 300.000] - loss: 246.541 - mae: 144.980 - mean_q: 156.342\n",
      "\n",
      "Interval 411 (4100000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.5910\n",
      "200 episodes - episode_reward: 179.550 [-40.000, 280.000] - loss: 248.551 - mae: 144.811 - mean_q: 156.369\n",
      "\n",
      "Interval 412 (4110000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.6660\n",
      "200 episodes - episode_reward: 183.300 [20.000, 300.000] - loss: 260.625 - mae: 147.761 - mean_q: 159.828\n",
      "\n",
      "Interval 413 (4120000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.5100\n",
      "200 episodes - episode_reward: 175.500 [-30.000, 260.000] - loss: 268.639 - mae: 151.724 - mean_q: 163.828\n",
      "\n",
      "Interval 414 (4130000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.5260\n",
      "200 episodes - episode_reward: 126.300 [-50.000, 290.000] - loss: 287.521 - mae: 158.202 - mean_q: 170.685\n",
      "\n",
      "Interval 415 (4140000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.2730\n",
      "200 episodes - episode_reward: 113.650 [-50.000, 260.000] - loss: 296.671 - mae: 159.857 - mean_q: 171.984\n",
      "\n",
      "Interval 416 (4150000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.2660\n",
      "200 episodes - episode_reward: 163.300 [-10.000, 290.000] - loss: 256.607 - mae: 148.783 - mean_q: 160.309\n",
      "\n",
      "Interval 417 (4160000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.4170\n",
      "200 episodes - episode_reward: 170.850 [-50.000, 270.000] - loss: 244.057 - mae: 144.045 - mean_q: 155.363\n",
      "\n",
      "Interval 418 (4170000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.6450\n",
      "200 episodes - episode_reward: 182.250 [-20.000, 310.000] - loss: 243.167 - mae: 143.217 - mean_q: 154.618\n",
      "\n",
      "Interval 419 (4180000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.1440\n",
      "200 episodes - episode_reward: 157.200 [-50.000, 310.000] - loss: 242.852 - mae: 143.172 - mean_q: 154.526\n",
      "\n",
      "Interval 420 (4190000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.0030\n",
      "200 episodes - episode_reward: 150.150 [-40.000, 260.000] - loss: 249.600 - mae: 146.645 - mean_q: 158.326\n",
      "\n",
      "Interval 421 (4200000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.3050\n",
      "200 episodes - episode_reward: 165.250 [-30.000, 280.000] - loss: 254.211 - mae: 146.853 - mean_q: 158.505\n",
      "\n",
      "Interval 422 (4210000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.3590\n",
      "200 episodes - episode_reward: 167.950 [-10.000, 270.000] - loss: 278.927 - mae: 155.540 - mean_q: 167.794\n",
      "\n",
      "Interval 423 (4220000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.3620\n",
      "200 episodes - episode_reward: 168.100 [-40.000, 290.000] - loss: 262.292 - mae: 151.652 - mean_q: 163.419\n",
      "\n",
      "Interval 424 (4230000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 3.4730\n",
      "200 episodes - episode_reward: 173.650 [0.000, 290.000] - loss: 255.420 - mae: 147.968 - mean_q: 159.467\n",
      "\n",
      "Interval 425 (4240000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 3.0940\n",
      "200 episodes - episode_reward: 154.700 [-10.000, 250.000] - loss: 248.753 - mae: 146.414 - mean_q: 157.893\n",
      "\n",
      "Interval 426 (4250000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.5090\n",
      "200 episodes - episode_reward: 125.450 [-50.000, 250.000] - loss: 241.152 - mae: 143.397 - mean_q: 154.442\n",
      "\n",
      "Interval 427 (4260000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.7110\n",
      "200 episodes - episode_reward: 135.550 [-20.000, 220.000] - loss: 219.788 - mae: 136.810 - mean_q: 147.336\n",
      "\n",
      "Interval 428 (4270000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.4630\n",
      "200 episodes - episode_reward: 123.150 [-50.000, 230.000] - loss: 207.206 - mae: 132.073 - mean_q: 142.529\n",
      "\n",
      "Interval 429 (4280000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.6760\n",
      "200 episodes - episode_reward: 133.800 [-50.000, 260.000] - loss: 190.165 - mae: 127.138 - mean_q: 136.947\n",
      "\n",
      "Interval 430 (4290000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.7560\n",
      "200 episodes - episode_reward: 137.800 [-40.000, 240.000] - loss: 180.061 - mae: 123.577 - mean_q: 133.267\n",
      "\n",
      "Interval 431 (4300000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.9110\n",
      "200 episodes - episode_reward: 145.550 [-20.000, 250.000] - loss: 179.330 - mae: 123.895 - mean_q: 133.631\n",
      "\n",
      "Interval 432 (4310000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.9680\n",
      "200 episodes - episode_reward: 148.400 [-10.000, 300.000] - loss: 191.893 - mae: 127.903 - mean_q: 137.786\n",
      "\n",
      "Interval 433 (4320000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.0070\n",
      "200 episodes - episode_reward: 150.350 [-30.000, 260.000] - loss: 194.134 - mae: 130.089 - mean_q: 140.004\n",
      "\n",
      "Interval 434 (4330000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.9820\n",
      "200 episodes - episode_reward: 149.100 [-40.000, 280.000] - loss: 199.571 - mae: 132.286 - mean_q: 142.216\n",
      "\n",
      "Interval 435 (4340000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.8240\n",
      "200 episodes - episode_reward: 141.200 [-30.000, 240.000] - loss: 197.335 - mae: 130.963 - mean_q: 140.919\n",
      "\n",
      "Interval 436 (4350000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.9850\n",
      "200 episodes - episode_reward: 149.250 [-20.000, 260.000] - loss: 202.236 - mae: 131.467 - mean_q: 141.653\n",
      "\n",
      "Interval 437 (4360000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.1240\n",
      "200 episodes - episode_reward: 156.200 [-30.000, 260.000] - loss: 208.484 - mae: 131.790 - mean_q: 142.059\n",
      "\n",
      "Interval 438 (4370000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.9320\n",
      "200 episodes - episode_reward: 146.600 [-50.000, 250.000] - loss: 208.860 - mae: 133.961 - mean_q: 144.333\n",
      "\n",
      "Interval 439 (4380000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.0020\n",
      "200 episodes - episode_reward: 150.100 [-40.000, 260.000] - loss: 208.333 - mae: 135.334 - mean_q: 145.828\n",
      "\n",
      "Interval 440 (4390000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.9830\n",
      "200 episodes - episode_reward: 149.150 [-40.000, 260.000] - loss: 204.126 - mae: 131.130 - mean_q: 141.256\n",
      "\n",
      "Interval 441 (4400000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.9350\n",
      "200 episodes - episode_reward: 146.750 [-30.000, 260.000] - loss: 197.478 - mae: 129.641 - mean_q: 139.623\n",
      "\n",
      "Interval 442 (4410000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.0340\n",
      "200 episodes - episode_reward: 151.700 [-20.000, 230.000] - loss: 194.633 - mae: 130.115 - mean_q: 140.119\n",
      "\n",
      "Interval 443 (4420000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 3.2870\n",
      "200 episodes - episode_reward: 164.350 [10.000, 270.000] - loss: 205.516 - mae: 132.989 - mean_q: 143.489\n",
      "\n",
      "Interval 444 (4430000 steps performed)\n",
      "10000/10000 [==============================] - 58s 6ms/step - reward: 3.3250\n",
      "200 episodes - episode_reward: 166.250 [-50.000, 250.000] - loss: 202.630 - mae: 132.947 - mean_q: 143.202\n",
      "\n",
      "Interval 445 (4440000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 56s 6ms/step - reward: 3.1480\n",
      "200 episodes - episode_reward: 157.400 [-40.000, 270.000] - loss: 216.796 - mae: 136.889 - mean_q: 147.518\n",
      "\n",
      "Interval 446 (4450000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.0090\n",
      "200 episodes - episode_reward: 150.450 [-20.000, 270.000] - loss: 226.058 - mae: 139.280 - mean_q: 149.820\n",
      "\n",
      "Interval 447 (4460000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.0900\n",
      "200 episodes - episode_reward: 154.500 [-50.000, 250.000] - loss: 218.298 - mae: 136.522 - mean_q: 147.089\n",
      "\n",
      "Interval 448 (4470000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.2430\n",
      "200 episodes - episode_reward: 162.150 [-40.000, 280.000] - loss: 214.885 - mae: 136.448 - mean_q: 147.161\n",
      "\n",
      "Interval 449 (4480000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.6720\n",
      "200 episodes - episode_reward: 183.600 [40.000, 320.000] - loss: 223.076 - mae: 137.252 - mean_q: 148.116\n",
      "\n",
      "Interval 450 (4490000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.3960\n",
      "200 episodes - episode_reward: 169.800 [-40.000, 290.000] - loss: 230.061 - mae: 140.169 - mean_q: 151.126\n",
      "\n",
      "Interval 451 (4500000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.4570\n",
      "200 episodes - episode_reward: 172.850 [-50.000, 260.000] - loss: 232.696 - mae: 142.926 - mean_q: 154.317\n",
      "\n",
      "Interval 452 (4510000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.3810\n",
      "200 episodes - episode_reward: 169.050 [0.000, 310.000] - loss: 239.985 - mae: 143.181 - mean_q: 154.529\n",
      "\n",
      "Interval 453 (4520000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.4960\n",
      "200 episodes - episode_reward: 174.800 [-30.000, 290.000] - loss: 237.387 - mae: 141.780 - mean_q: 152.928\n",
      "\n",
      "Interval 454 (4530000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.4260\n",
      "200 episodes - episode_reward: 171.300 [0.000, 290.000] - loss: 242.039 - mae: 145.311 - mean_q: 156.651\n",
      "\n",
      "Interval 455 (4540000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.1740\n",
      "200 episodes - episode_reward: 158.700 [-50.000, 310.000] - loss: 242.254 - mae: 145.957 - mean_q: 157.302\n",
      "\n",
      "Interval 456 (4550000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.7390\n",
      "200 episodes - episode_reward: 186.950 [-40.000, 280.000] - loss: 239.707 - mae: 143.561 - mean_q: 154.674\n",
      "\n",
      "Interval 457 (4560000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.4150\n",
      "200 episodes - episode_reward: 170.750 [-20.000, 270.000] - loss: 256.213 - mae: 147.813 - mean_q: 159.307\n",
      "\n",
      "Interval 458 (4570000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.0170\n",
      "200 episodes - episode_reward: 150.850 [-20.000, 280.000] - loss: 254.717 - mae: 148.525 - mean_q: 160.001\n",
      "\n",
      "Interval 459 (4580000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 3.2970\n",
      "200 episodes - episode_reward: 164.850 [-50.000, 270.000] - loss: 253.112 - mae: 146.622 - mean_q: 157.890\n",
      "\n",
      "Interval 460 (4590000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 3.1190\n",
      "200 episodes - episode_reward: 155.950 [-40.000, 280.000] - loss: 243.521 - mae: 143.771 - mean_q: 154.952\n",
      "\n",
      "Interval 461 (4600000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 2.7140\n",
      "200 episodes - episode_reward: 135.700 [-40.000, 240.000] - loss: 239.270 - mae: 143.201 - mean_q: 154.361\n",
      "\n",
      "Interval 462 (4610000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 3.1420\n",
      "200 episodes - episode_reward: 157.100 [-50.000, 290.000] - loss: 227.353 - mae: 140.083 - mean_q: 151.113\n",
      "\n",
      "Interval 463 (4620000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.5080\n",
      "200 episodes - episode_reward: 175.400 [40.000, 280.000] - loss: 225.127 - mae: 139.325 - mean_q: 150.103\n",
      "\n",
      "Interval 464 (4630000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 3.5760\n",
      "200 episodes - episode_reward: 178.800 [-30.000, 290.000] - loss: 231.656 - mae: 140.458 - mean_q: 151.486\n",
      "\n",
      "Interval 465 (4640000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 3.4300\n",
      "200 episodes - episode_reward: 171.500 [-20.000, 270.000] - loss: 241.366 - mae: 143.764 - mean_q: 155.099\n",
      "\n",
      "Interval 466 (4650000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.5040\n",
      "200 episodes - episode_reward: 175.200 [0.000, 280.000] - loss: 253.853 - mae: 148.112 - mean_q: 159.895\n",
      "\n",
      "Interval 467 (4660000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 3.5420\n",
      "200 episodes - episode_reward: 177.100 [-30.000, 290.000] - loss: 264.383 - mae: 151.030 - mean_q: 163.136\n",
      "\n",
      "Interval 468 (4670000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 3.4020\n",
      "200 episodes - episode_reward: 170.100 [-20.000, 280.000] - loss: 261.923 - mae: 148.968 - mean_q: 160.804\n",
      "\n",
      "Interval 469 (4680000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.9140\n",
      "200 episodes - episode_reward: 145.700 [-50.000, 260.000] - loss: 259.090 - mae: 147.854 - mean_q: 159.474\n",
      "\n",
      "Interval 470 (4690000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.0980\n",
      "200 episodes - episode_reward: 154.900 [-50.000, 280.000] - loss: 242.626 - mae: 145.450 - mean_q: 157.018\n",
      "\n",
      "Interval 471 (4700000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.1380\n",
      "200 episodes - episode_reward: 156.900 [-30.000, 280.000] - loss: 237.235 - mae: 143.067 - mean_q: 154.369\n",
      "\n",
      "Interval 472 (4710000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.3020\n",
      "200 episodes - episode_reward: 165.100 [-30.000, 270.000] - loss: 225.931 - mae: 137.881 - mean_q: 148.849\n",
      "\n",
      "Interval 473 (4720000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.3460\n",
      "200 episodes - episode_reward: 167.300 [-10.000, 330.000] - loss: 221.192 - mae: 137.339 - mean_q: 148.397\n",
      "\n",
      "Interval 474 (4730000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.4250\n",
      "200 episodes - episode_reward: 171.250 [-10.000, 260.000] - loss: 214.979 - mae: 134.726 - mean_q: 145.506\n",
      "\n",
      "Interval 475 (4740000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.0640\n",
      "200 episodes - episode_reward: 103.200 [-50.000, 270.000] - loss: 240.649 - mae: 143.614 - mean_q: 155.092\n",
      "\n",
      "Interval 476 (4750000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.1660\n",
      "200 episodes - episode_reward: 158.300 [-30.000, 280.000] - loss: 235.051 - mae: 142.133 - mean_q: 153.339\n",
      "\n",
      "Interval 477 (4760000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.2700\n",
      "200 episodes - episode_reward: 163.500 [-40.000, 270.000] - loss: 232.681 - mae: 140.542 - mean_q: 151.677\n",
      "\n",
      "Interval 478 (4770000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.3840\n",
      "200 episodes - episode_reward: 169.200 [-30.000, 280.000] - loss: 236.701 - mae: 141.378 - mean_q: 152.759\n",
      "\n",
      "Interval 479 (4780000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 3.0750\n",
      "200 episodes - episode_reward: 153.750 [-50.000, 280.000] - loss: 231.363 - mae: 142.027 - mean_q: 153.250\n",
      "\n",
      "Interval 480 (4790000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.5860\n",
      "200 episodes - episode_reward: 79.300 [-40.000, 240.000] - loss: 232.896 - mae: 142.522 - mean_q: 153.016\n",
      "\n",
      "Interval 481 (4800000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.0390\n",
      "200 episodes - episode_reward: 101.950 [-50.000, 210.000] - loss: 210.010 - mae: 134.356 - mean_q: 144.285\n",
      "\n",
      "Interval 482 (4810000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.6510\n",
      "200 episodes - episode_reward: 82.550 [-50.000, 220.000] - loss: 188.313 - mae: 125.904 - mean_q: 135.319\n",
      "\n",
      "Interval 483 (4820000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.8110\n",
      "200 episodes - episode_reward: 90.550 [-50.000, 210.000] - loss: 165.403 - mae: 116.776 - mean_q: 125.572\n",
      "\n",
      "Interval 484 (4830000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.7860\n",
      "200 episodes - episode_reward: 89.300 [-50.000, 220.000] - loss: 136.148 - mae: 105.969 - mean_q: 113.965\n",
      "\n",
      "Interval 485 (4840000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.9290\n",
      "200 episodes - episode_reward: 96.450 [-50.000, 240.000] - loss: 121.800 - mae: 99.634 - mean_q: 107.228\n",
      "\n",
      "Interval 486 (4850000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.0430\n",
      "200 episodes - episode_reward: 102.150 [-40.000, 220.000] - loss: 112.350 - mae: 96.859 - mean_q: 104.128\n",
      "\n",
      "Interval 487 (4860000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.9220\n",
      "200 episodes - episode_reward: 96.100 [-40.000, 220.000] - loss: 116.674 - mae: 98.929 - mean_q: 106.319\n",
      "\n",
      "Interval 488 (4870000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.1230\n",
      "200 episodes - episode_reward: 106.150 [-50.000, 230.000] - loss: 113.613 - mae: 97.801 - mean_q: 105.223\n",
      "\n",
      "Interval 489 (4880000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.7110\n",
      "200 episodes - episode_reward: 85.550 [-40.000, 220.000] - loss: 120.229 - mae: 100.244 - mean_q: 107.965\n",
      "\n",
      "Interval 490 (4890000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 2.3380\n",
      "200 episodes - episode_reward: 116.900 [-30.000, 260.000] - loss: 113.316 - mae: 97.143 - mean_q: 104.657\n",
      "\n",
      "Interval 491 (4900000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 2.3090\n",
      "200 episodes - episode_reward: 115.450 [-40.000, 230.000] - loss: 111.377 - mae: 97.060 - mean_q: 104.571\n",
      "\n",
      "Interval 492 (4910000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 2.3120\n",
      "200 episodes - episode_reward: 115.600 [-20.000, 220.000] - loss: 119.606 - mae: 100.377 - mean_q: 108.121\n",
      "\n",
      "Interval 493 (4920000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 2.2190\n",
      "200 episodes - episode_reward: 110.950 [-50.000, 230.000] - loss: 125.471 - mae: 101.719 - mean_q: 109.536\n",
      "\n",
      "Interval 494 (4930000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 2.1590\n",
      "200 episodes - episode_reward: 107.950 [-50.000, 200.000] - loss: 125.109 - mae: 102.008 - mean_q: 109.796\n",
      "\n",
      "Interval 495 (4940000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 2.1500\n",
      "200 episodes - episode_reward: 107.500 [-40.000, 200.000] - loss: 131.373 - mae: 104.609 - mean_q: 112.565\n",
      "\n",
      "Interval 496 (4950000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 2.3120\n",
      "200 episodes - episode_reward: 115.600 [-50.000, 270.000] - loss: 126.536 - mae: 103.748 - mean_q: 111.495\n",
      "\n",
      "Interval 497 (4960000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.3730\n",
      "200 episodes - episode_reward: 118.650 [-10.000, 220.000] - loss: 128.980 - mae: 103.003 - mean_q: 110.789\n",
      "\n",
      "Interval 498 (4970000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.3790\n",
      "200 episodes - episode_reward: 118.950 [-50.000, 230.000] - loss: 126.384 - mae: 103.809 - mean_q: 111.836\n",
      "\n",
      "Interval 499 (4980000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.2740\n",
      "200 episodes - episode_reward: 113.700 [-40.000, 190.000] - loss: 131.135 - mae: 104.514 - mean_q: 112.627\n",
      "\n",
      "Interval 500 (4990000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 2.2210\n",
      "done, took 27046.497 seconds\n"
     ]
    }
   ],
   "source": [
    "history = dqn.fit(env, nb_steps=5*MILLION, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d17fd876",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = history.history\n",
    "data['episode_reward'] = [float(v) for v in data['episode_reward']]\n",
    "data['nb_episode_steps'] = [int(v) for v in data['nb_episode_steps']]\n",
    "data['nb_steps'] = [int(v) for v in data['nb_steps']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0de94a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('agents/{}'.format(name))  # If the directory does not exist we cannot write the file\n",
    "with open(get_training_path(name), 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d1d24c",
   "metadata": {},
   "source": [
    "Save agent to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1759baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights(get_agent_path(name), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910b175d",
   "metadata": {},
   "source": [
    "## 4. Reloading Agent from memory and test ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7eb14792",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridworldMultiAgentv1(seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28c3a145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 25)                425       \n",
      "=================================================================\n",
      "Total params: 1,241\n",
      "Trainable params: 1,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n\n",
    "model = build_model(states, actions, [32, 16], ['relu', 'relu'])\n",
    "print(model.summary())\n",
    "dqn = build_agent(model, actions, 0.01, EpsGreedyQPolicy(eps=0), 50000)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Load weights\n",
    "dqn.load_weights(get_agent_path(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc62e1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_episodes = 1*THOUSAND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7973b84f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = dqn.test(env, nb_episodes=nb_episodes, visualize=False, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72835449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_path(name):\n",
    "    return 'agents/{}/{}_test_{}episodes.txt'.format(name, name, nb_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0ea72fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.array(scores.history['episode_reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a897feb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(get_test_path(name), rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a0a234",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
