{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4221e1e3",
   "metadata": {},
   "source": [
    "# Training v1.0 with a Deep Q Network (DQN) #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d628c23d",
   "metadata": {},
   "source": [
    "Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b9fe8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "rc('text', usetex=True)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "761fd1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f09c161",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory  # For experience replay!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c9f747a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_environment_ncml import *\n",
    "from learning import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a0674c",
   "metadata": {},
   "source": [
    "Useful numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cf11480",
   "metadata": {},
   "outputs": [],
   "source": [
    "MILLION = 1000000\n",
    "HTHOUSAND = 100000\n",
    "THOUSAND = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfb0b18",
   "metadata": {},
   "source": [
    "## 1. Create environment ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b9a3f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridworldMultiAgentv1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "708f7943",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eca1527b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 25)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states, actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0343c79a",
   "metadata": {},
   "source": [
    "## 2. Create a Deep Learning Model with Keras ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "941ff796",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(states, actions, [32, 16], ['relu', 'relu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f136bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 8)                 0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                288       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 25)                425       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,241\n",
      "Trainable params: 1,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0b28d2",
   "metadata": {},
   "source": [
    "## 3. Build Agent with Keras-RL ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a418e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/koji/data/NCMMLProject/.venv/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "2023-09-17 14:51:02.879517: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "2023-09-17 14:51:02.881417: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "dqn = build_agent(model, actions, 0.01, EpsGreedyQPolicy(), 5000)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "# dqn.compile(Adam(lr=1e-2), metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "674f77a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'dqn1_5b5_3216_adam_lr0.001_tmu0.01_ml50K_ns5M_eps0.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "073ef900",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000.0 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "    1/10000 [..............................] - ETA: 3:48 - reward: -1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/koji/data/NCMMLProject/.venv/lib/python3.10/site-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 22s 2ms/step - reward: -0.2430\n",
      "200 episodes - episode_reward: -12.150 [-50.000, 120.000] - loss: 7.399 - mae: 15.169 - mean_q: 18.102\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 22s 2ms/step - reward: 0.0400\n",
      "200 episodes - episode_reward: 2.000 [-50.000, 110.000] - loss: 27.060 - mae: 41.553 - mean_q: 46.567\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 22s 2ms/step - reward: 0.1370\n",
      "200 episodes - episode_reward: 6.850 [-50.000, 110.000] - loss: 27.073 - mae: 43.093 - mean_q: 47.957\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 22s 2ms/step - reward: 0.1660\n",
      "200 episodes - episode_reward: 8.300 [-50.000, 160.000] - loss: 33.115 - mae: 49.227 - mean_q: 54.349\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "   71/10000 [..............................] - ETA: 21s - reward: 1.8169"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Expected tensor of type int64 but got type float\n\t [[{{node training/Adam/add}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mdqn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mMILLION\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/data/NCMMLProject/.venv/lib/python3.10/site-packages/rl/core.py:194\u001b[0m, in \u001b[0;36mAgent.fit\u001b[0;34m(self, env, nb_steps, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, start_step_policy, log_interval, nb_max_episode_steps)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nb_max_episode_steps \u001b[38;5;129;01mand\u001b[39;00m episode_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m nb_max_episode_steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# Force a terminal state.\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterminal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m    197\u001b[0m step_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m'\u001b[39m: action,\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobservation\u001b[39m\u001b[38;5;124m'\u001b[39m: observation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo\u001b[39m\u001b[38;5;124m'\u001b[39m: accumulated_info,\n\u001b[1;32m    204\u001b[0m }\n",
      "File \u001b[0;32m~/data/NCMMLProject/.venv/lib/python3.10/site-packages/rl/agents/dqn.py:322\u001b[0m, in \u001b[0;36mDQNAgent.backward\u001b[0;34m(self, reward, terminal)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# Finally, perform a single update on the entire batch. We use a dummy target since\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m# the actual loss is computed in a Lambda layer that needs more complex input. However,\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# it is still useful to know the actual target to compute metrics properly.\u001b[39;00m\n\u001b[1;32m    321\u001b[0m ins \u001b[38;5;241m=\u001b[39m [state0_batch] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39minput) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlist\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m state0_batch\n\u001b[0;32m--> 322\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mins\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdummy_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m metrics \u001b[38;5;241m=\u001b[39m [metric \u001b[38;5;28;01mfor\u001b[39;00m idx, metric \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(metrics) \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)]  \u001b[38;5;66;03m# throw away individual losses\u001b[39;00m\n\u001b[1;32m    324\u001b[0m metrics \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mmetrics\n",
      "File \u001b[0;32m~/data/NCMMLProject/.venv/lib/python3.10/site-packages/keras/engine/training_v1.py:1075\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1073\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_sample_weight_modes(sample_weights\u001b[38;5;241m=\u001b[39msample_weights)\n\u001b[1;32m   1074\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_train_function()\n\u001b[0;32m-> 1075\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mins\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reset_metrics:\n\u001b[1;32m   1078\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_metrics()\n",
      "File \u001b[0;32m~/data/NCMMLProject/.venv/lib/python3.10/site-packages/keras/backend.py:4284\u001b[0m, in \u001b[0;36mGraphExecutionFunction.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   4278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callable_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m feed_arrays \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feed_arrays \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   4279\u001b[0m     symbol_vals \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol_vals \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   4280\u001b[0m     feed_symbols \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feed_symbols \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfetches \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetches \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   4281\u001b[0m     session \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session):\n\u001b[1;32m   4282\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_callable(feed_arrays, feed_symbols, symbol_vals, session)\n\u001b[0;32m-> 4284\u001b[0m fetched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marray_vals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4285\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4286\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_fetch_callbacks(fetched[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetches):])\n\u001b[1;32m   4287\u001b[0m output_structure \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mpack_sequence_as(\n\u001b[1;32m   4288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_structure,\n\u001b[1;32m   4289\u001b[0m     fetched[:\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs)],\n\u001b[1;32m   4290\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/data/NCMMLProject/.venv/lib/python3.10/site-packages/tensorflow/python/client/session.py:1480\u001b[0m, in \u001b[0;36mBaseSession._Callable.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1479\u001b[0m   run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1480\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRunCallable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1481\u001b[0m \u001b[43m                                         \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1482\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1483\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[1;32m   1484\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Expected tensor of type int64 but got type float\n\t [[{{node training/Adam/add}}]]"
     ]
    }
   ],
   "source": [
    "history = dqn.fit(env, nb_steps=0.05*MILLION, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17fd876",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = history.history\n",
    "data['episode_reward'] = [float(v) for v in data['episode_reward']]\n",
    "data['nb_episode_steps'] = [int(v) for v in data['nb_episode_steps']]\n",
    "data['nb_steps'] = [int(v) for v in data['nb_steps']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0de94a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('agents/{}'.format(name))  # If the directory does not exist we cannot write the file\n",
    "with open(get_training_path(name), 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d1d24c",
   "metadata": {},
   "source": [
    "Save agent to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1759baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights(get_agent_path(name), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910b175d",
   "metadata": {},
   "source": [
    "## 4. Reloading Agent from memory and test ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb14792",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridworldMultiAgentv1(seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c3a145",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n\n",
    "model = build_model(states, actions, [32, 16], ['relu', 'relu'])\n",
    "print(model.summary())\n",
    "dqn = build_agent(model, actions, 0.01, EpsGreedyQPolicy(eps=0), 50000)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Load weights\n",
    "dqn.load_weights(get_agent_path(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc62e1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_episodes = 10*THOUSAND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7973b84f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = dqn.test(env, nb_episodes=nb_episodes, visualize=False, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ea72fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.array(scores.history['episode_reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a897feb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(get_test_path(name, nb_episodes), rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a0a234",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
